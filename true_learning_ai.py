from __future__ import annotations
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Ù†Ø¸Ø§Ù… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø­Ù‚ÙŠÙ‚ÙŠ ÙŠØªØ¹Ù„Ù… Ù…Ù† ÙƒÙ„ ØªØ¬Ø±Ø¨Ø© Ø¬Ø¯ÙŠØ¯Ø©
ÙŠØ¨Ù†ÙŠ Ù…Ø¹Ø±ÙØªÙ‡ ØªØ¯Ø±ÙŠØ¬ÙŠØ§Ù‹ ÙˆÙŠØ·ÙˆØ± Ù‚Ø¯Ø±Ø§ØªÙ‡ Ø§Ù„Ø°Ø§ØªÙŠØ©
"""

import json
import os
import time
import hashlib
import numpy as np
from collections import defaultdict
import pickle
import re

class TrueLearningAI:
    def __init__(self, memory_file="ai_memory.json", patterns_file="learned_patterns.pkl"):
        self.memory_file = memory_file
        self.patterns_file = patterns_file
        
        # Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø¯Ø§Ø¦Ù…Ø© - ØªØ­ÙØ¸ ÙƒÙ„ Ù…Ø§ ÙŠØªØ¹Ù„Ù…Ù‡
        self.long_term_memory = self.load_memory()
        
        # Ø£Ù†Ù…Ø§Ø· Ù…ØªØ¹Ù„Ù…Ø© - ÙŠÙƒØªØ´ÙÙ‡Ø§ Ø¨Ù†ÙØ³Ù‡
        self.learned_patterns = self.load_patterns()
        
        # Ø´Ø¨ÙƒØ© Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… - ÙŠØ¨Ù†ÙŠÙ‡Ø§ ØªØ¯Ø±ÙŠØ¬ÙŠØ§Ù‹
        self.concept_network = defaultdict(list)
        
        # Ø®Ø¨Ø±Ø§Øª Ø³Ø§Ø¨Ù‚Ø© - ÙŠØªØ¹Ù„Ù… Ù…Ù†Ù‡Ø§
        self.experiences = []
        
        # Ù‚Ø¯Ø±Ø§Øª Ù…ØªØ·ÙˆØ±Ø© - ØªÙ†Ù…Ùˆ Ù…Ø¹ Ø§Ù„ÙˆÙ‚Øª
        self.capabilities = {
            "pattern_recognition": 0.1,
            "logical_reasoning": 0.1,
            "creative_thinking": 0.1,
            "problem_solving": 0.1,
            "learning_efficiency": 0.1
        }
        
        print("ðŸ§  ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ")
        print(f"ðŸ“š Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø·ÙˆÙŠÙ„Ø© Ø§Ù„Ù…Ø¯Ù‰: {len(self.long_term_memory)} Ø¹Ù†ØµØ±")
        print(f"ðŸ” Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ØªØ¹Ù„Ù…Ø©: {len(self.learned_patterns)} Ù†Ù…Ø·")

    def load_memory(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø·ÙˆÙŠÙ„Ø© Ø§Ù„Ù…Ø¯Ù‰ Ù…Ù† Ø§Ù„Ù…Ù„Ù"""
        if os.path.exists(self.memory_file):
            try:
                with open(self.memory_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return {}
        return {}

    def save_memory(self):
        """Ø­ÙØ¸ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø·ÙˆÙŠÙ„Ø© Ø§Ù„Ù…Ø¯Ù‰"""
        with open(self.memory_file, 'w', encoding='utf-8') as f:
            json.dump(self.long_term_memory, f, ensure_ascii=False, indent=2)

    def load_patterns(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ØªØ¹Ù„Ù…Ø©"""
        if os.path.exists(self.patterns_file):
            try:
                with open(self.patterns_file, 'rb') as f:
                    return pickle.load(f)
            except:
                return {}
        return {}

    def save_patterns(self):
        """Ø­ÙØ¸ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ØªØ¹Ù„Ù…Ø©"""
        with open(self.patterns_file, 'wb') as f:
            pickle.dump(self.learned_patterns, f)

    def encounter_new_information(self, information, context="general"):
        """Ù…ÙˆØ§Ø¬Ù‡Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© - Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ ÙŠØ¨Ø¯Ø£ Ù‡Ù†Ø§"""
        
        print(f"ðŸ” Ù…ÙˆØ§Ø¬Ù‡Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©: {information[:50]}...")
        
        # 1. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
        analysis = self.analyze_information(information)
        
        # 2. Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¹Ù† Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ø´Ø§Ø¨Ù‡Ø©
        similar_memories = self.find_similar_memories(analysis)
        
        # 3. Ø§ÙƒØªØ´Ø§Ù Ø£Ù†Ù…Ø§Ø· Ø¬Ø¯ÙŠØ¯Ø©
        new_patterns = self.discover_patterns(information, similar_memories)
        
        # 4. Ø¨Ù†Ø§Ø¡ Ø±ÙˆØ§Ø¨Ø· Ù…ÙØ§Ù‡ÙŠÙ…ÙŠØ©
        self.build_concept_connections(analysis, similar_memories)
        
        # 5. ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù‚Ø¯Ø±Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ù„Ù…
        self.update_capabilities(new_patterns)
        
        # 6. Ø­ÙØ¸ Ø§Ù„ØªØ¬Ø±Ø¨Ø© ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø·ÙˆÙŠÙ„Ø© Ø§Ù„Ù…Ø¯Ù‰
        memory_id = self.store_in_memory(information, analysis, context)
        
        # 7. Ø­ÙØ¸ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª
        self.save_memory()
        self.save_patterns()
        
        return {
            "memory_id": memory_id,
            "new_patterns_discovered": len(new_patterns),
            "connections_made": len(analysis["key_concepts"]),
            "capability_growth": self.calculate_growth(),
            "learning_confidence": analysis["complexity_score"]
        }

    def analyze_information(self, information):
        """ØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙŠÙ‚ Ù„Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©"""
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        words = re.findall(r'\b\w+\b', information.lower())
        key_concepts = [w for w in words if len(w) > 3]
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
        complexity_score = self.calculate_complexity(information)
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†ÙˆØ¹
        info_type = self.classify_information(information)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª
        relationships = self.extract_relationships(information)
        
        return {
            "raw_text": information,
            "key_concepts": key_concepts,
            "complexity_score": complexity_score,
            "information_type": info_type,
            "relationships": relationships,
            "timestamp": time.time(),
            "word_count": len(words),
            "unique_concepts": len(set(key_concepts))
        }

    def calculate_complexity(self, information):
        """Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙ‰ ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª"""
        factors = [
            len(information.split()),  # Ø·ÙˆÙ„ Ø§Ù„Ù†Øµ
            len(set(information.split())),  # ØªÙ†ÙˆØ¹ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª
            information.count('?') + information.count('!'),  # Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù… ÙˆØ§Ù„ØªØ¹Ø¬Ø¨
            len(re.findall(r'[ØŒ,;:]', information)),  # Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©
        ]
        return sum(factors) / len(factors) / 10  # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†ØªÙŠØ¬Ø©

    def classify_information(self, information):
        """ØªØµÙ†ÙŠÙ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª"""
        info_lower = information.lower()
        
        if any(word in info_lower for word in ['ÙƒÙŠÙ', 'Ø·Ø±ÙŠÙ‚Ø©', 'Ø®Ø·ÙˆØ§Øª', 'how', 'method']):
            return "procedural"
        elif any(word in info_lower for word in ['Ù„Ù…Ø§Ø°Ø§', 'Ø³Ø¨Ø¨', 'why', 'because']):
            return "causal"
        elif any(word in info_lower for word in ['Ù…Ø§ Ù‡Ùˆ', 'ØªØ¹Ø±ÙŠÙ', 'what is', 'definition']):
            return "definitional"
        elif any(word in info_lower for word in ['Ù…Ø´ÙƒÙ„Ø©', 'Ø­Ù„', 'problem', 'solution']):
            return "problem_solving"
        else:
            return "general"

    def extract_relationships(self, information):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ù…Ù† Ø§Ù„Ù†Øµ"""
        relationships = []
        
        # Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ø³Ø¨Ø¨ÙŠØ©
        if 'Ù„Ø£Ù†' in information or 'because' in information:
            relationships.append("causal")
        
        # Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„ØªØ´Ø§Ø¨Ù‡
        if 'Ù…Ø«Ù„' in information or 'like' in information:
            relationships.append("similarity")
        
        # Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„ØªØ¶Ø§Ø¯
        if 'Ù„ÙƒÙ†' in information or 'but' in information:
            relationships.append("contrast")
        
        return relationships

    def find_similar_memories(self, analysis):
        """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø°ÙƒØ±ÙŠØ§Øª Ù…Ø´Ø§Ø¨Ù‡Ø© ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø·ÙˆÙŠÙ„Ø© Ø§Ù„Ù…Ø¯Ù‰"""
        similar_memories = []
        
        for memory_id, memory in self.long_term_memory.items():
            if isinstance(memory, dict) and 'analysis' in memory:
                similarity_score = self.calculate_similarity(analysis, memory['analysis'])
                if similarity_score > 0.3:  # Ø¹ØªØ¨Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡
                    similar_memories.append({
                        "memory_id": memory_id,
                        "similarity": similarity_score,
                        "memory": memory
                    })
        
        return sorted(similar_memories, key=lambda x: x['similarity'], reverse=True)[:5]

    def calculate_similarity(self, analysis1, analysis2):
        """Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† ØªØ­Ù„ÙŠÙ„ÙŠÙ†"""
        
        # ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…
        concepts1 = set(analysis1.get("key_concepts", []))
        concepts2 = set(analysis2.get("key_concepts", []))
        
        if not concepts1 or not concepts2:
            return 0
        
        intersection = len(concepts1.intersection(concepts2))
        union = len(concepts1.union(concepts2))
        
        concept_similarity = intersection / union if union > 0 else 0
        
        # ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ù†ÙˆØ¹
        type_similarity = 1 if analysis1.get("information_type") == analysis2.get("information_type") else 0
        
        # ØªØ´Ø§Ø¨Ù‡ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
        complexity_diff = abs(analysis1.get("complexity_score", 0) - analysis2.get("complexity_score", 0))
        complexity_similarity = max(0, 1 - complexity_diff)
        
        # Ø§Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…Ø±Ø¬Ø­
        return (concept_similarity * 0.6 + type_similarity * 0.2 + complexity_similarity * 0.2)

    def discover_patterns(self, information, similar_memories):
        """Ø§ÙƒØªØ´Ø§Ù Ø£Ù†Ù…Ø§Ø· Ø¬Ø¯ÙŠØ¯Ø© Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙˆØ§Ù„Ø°ÙƒØ±ÙŠØ§Øª Ø§Ù„Ù…Ø´Ø§Ø¨Ù‡Ø©"""
        new_patterns = []
        
        # Ù†Ù…Ø· Ø§Ù„ØªÙƒØ±Ø§Ø±
        if len(similar_memories) >= 2:
            pattern_key = f"repeated_concept_{len(similar_memories)}"
            if pattern_key not in self.learned_patterns:
                self.learned_patterns[pattern_key] = {
                    "type": "repetition",
                    "frequency": len(similar_memories),
                    "examples": [m["memory_id"] for m in similar_memories],
                    "discovered_at": time.time()
                }
                new_patterns.append(pattern_key)
        
        # Ù†Ù…Ø· Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù…ØªØ²Ø§ÙŠØ¯
        if similar_memories:
            complexities = [m["memory"]["analysis"]["complexity_score"] for m in similar_memories 
                          if "analysis" in m["memory"]]
            if complexities and len(complexities) >= 2:
                if all(complexities[i] <= complexities[i+1] for i in range(len(complexities)-1)):
                    pattern_key = "increasing_complexity"
                    if pattern_key not in self.learned_patterns:
                        self.learned_patterns[pattern_key] = {
                            "type": "progression",
                            "direction": "increasing",
                            "examples": complexities,
                            "discovered_at": time.time()
                        }
                        new_patterns.append(pattern_key)
        
        return new_patterns

    def build_concept_connections(self, analysis, similar_memories):
        """Ø¨Ù†Ø§Ø¡ Ø´Ø¨ÙƒØ© Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…ÙŠØ©"""
        current_concepts = analysis["key_concepts"]
        
        for concept in current_concepts:
            # Ø±Ø¨Ø· Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ø¨Ø¨Ø¹Ø¶Ù‡Ø§
            for other_concept in current_concepts:
                if concept != other_concept:
                    if other_concept not in self.concept_network[concept]:
                        self.concept_network[concept].append(other_concept)
            
            # Ø±Ø¨Ø· Ø¨Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ù…Ù† Ø§Ù„Ø°ÙƒØ±ÙŠØ§Øª Ø§Ù„Ù…Ø´Ø§Ø¨Ù‡Ø©
            for memory in similar_memories:
                if "analysis" in memory["memory"]:
                    memory_concepts = memory["memory"]["analysis"].get("key_concepts", [])
                    for memory_concept in memory_concepts:
                        if memory_concept not in self.concept_network[concept]:
                            self.concept_network[concept].append(memory_concept)

    def update_capabilities(self, new_patterns):
        """ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù‚Ø¯Ø±Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø§Ù„Ù…ÙƒØªØ´ÙØ©"""
        growth_rate = 0.01  # Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ù…Ùˆ
        
        if new_patterns:
            self.capabilities["pattern_recognition"] += len(new_patterns) * growth_rate
            self.capabilities["learning_efficiency"] += len(new_patterns) * growth_rate * 0.5
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù‚Ø¯Ø±Ø§Øª Ø§Ù„Ø£Ø®Ø±Ù‰ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
        for capability in self.capabilities:
            self.capabilities[capability] = min(1.0, self.capabilities[capability])  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 1.0

    def store_in_memory(self, information, analysis, context):
        """Ø­ÙØ¸ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø·ÙˆÙŠÙ„Ø© Ø§Ù„Ù…Ø¯Ù‰"""
        memory_id = hashlib.md5(f"{information}{time.time()}".encode()).hexdigest()[:8]
        
        self.long_term_memory[memory_id] = {
            "information": information,
            "analysis": analysis,
            "context": context,
            "stored_at": time.time(),
            "access_count": 0,
            "importance_score": analysis["complexity_score"]
        }
        
        return memory_id

    def calculate_growth(self):
        """Ø­Ø³Ø§Ø¨ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ù…Ùˆ ÙÙŠ Ø§Ù„Ù‚Ø¯Ø±Ø§Øª"""
        return sum(self.capabilities.values()) / len(self.capabilities)

    def solve_problem(self, problem):
        """Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…ØªØ±Ø§ÙƒÙ…Ø©"""
        
        print(f"ðŸŽ¯ Ù…Ø­Ø§ÙˆÙ„Ø© Ø­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©: {problem[:50]}...")
        
        # 1. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©
        problem_analysis = self.analyze_information(problem)
        
        # 2. Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø­Ù„ÙˆÙ„ Ù…Ø´Ø§Ø¨Ù‡Ø© ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        similar_solutions = self.find_similar_memories(problem_analysis)
        
        # 3. ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ØªØ¹Ù„Ù…Ø©
        applicable_patterns = self.find_applicable_patterns(problem_analysis)
        
        # 4. ØªÙˆÙ„ÙŠØ¯ Ø­Ù„ Ø¬Ø¯ÙŠØ¯
        solution = self.generate_solution(problem, problem_analysis, similar_solutions, applicable_patterns)
        
        # 5. ØªØ¹Ù„Ù… Ù…Ù† ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ø­Ù„
        learning_result = self.encounter_new_information(f"Problem: {problem} | Solution: {solution}", "problem_solving")
        
        return {
            "problem": problem,
            "solution": solution,
            "confidence": min(1.0, self.capabilities["problem_solving"] + len(similar_solutions) * 0.1),
            "similar_cases_found": len(similar_solutions),
            "patterns_applied": len(applicable_patterns),
            "learning_from_solution": learning_result
        }

    def find_applicable_patterns(self, problem_analysis):
        """Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©"""
        applicable = []
        
        for pattern_key, pattern_data in self.learned_patterns.items():
            # ØªØ­Ù‚Ù‚ Ù…Ù† Ù‚Ø§Ø¨Ù„ÙŠØ© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©
            if problem_analysis["information_type"] == "problem_solving":
                applicable.append(pattern_key)
        
        return applicable

    def generate_solution(self, problem, analysis, similar_solutions, patterns):
        """ØªÙˆÙ„ÙŠØ¯ Ø­Ù„ Ù„Ù„Ù…Ø´ÙƒÙ„Ø©"""
        
        solution_parts = []
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„Ù…Ø´Ø§Ø¨Ù‡Ø©
        if similar_solutions:
            solution_parts.append("Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ØªØ¬Ø§Ø±Ø¨ Ù…Ø´Ø§Ø¨Ù‡Ø© ÙÙŠ Ø°Ø§ÙƒØ±ØªÙŠ:")
            for sol in similar_solutions[:2]:
                if "information" in sol["memory"]:
                    solution_parts.append(f"- {sol['memory']['information'][:100]}...")
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ØªØ¹Ù„Ù…Ø©
        if patterns:
            solution_parts.append(f"ØªØ·Ø¨ÙŠÙ‚ {len(patterns)} Ù†Ù…Ø· Ù…ØªØ¹Ù„Ù… Ù…Ù† ØªØ¬Ø§Ø±Ø¨ÙŠ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©")
        
        # Ø­Ù„ Ø¥Ø¨Ø¯Ø§Ø¹ÙŠ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø´Ø¨ÙƒØ© Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…
        related_concepts = []
        for concept in analysis["key_concepts"]:
            if concept in self.concept_network:
                related_concepts.extend(self.concept_network[concept][:2])
        
        if related_concepts:
            unique_concepts = list(set(related_concepts))[:3]
            solution_parts.append(f"Ø±Ø¨Ø· Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø¨Ù…ÙØ§Ù‡ÙŠÙ… Ø°Ø§Øª ØµÙ„Ø©: {', '.join(unique_concepts)}")
        
        # Ø¯Ù…Ø¬ ÙƒÙ„ Ø´ÙŠØ¡ ÙÙŠ Ø­Ù„ Ù…ØªÙ…Ø§Ø³Ùƒ
        if solution_parts:
            return " | ".join(solution_parts)
        else:
            return "Ù‡Ø°Ù‡ Ù…Ø´ÙƒÙ„Ø© Ø¬Ø¯ÙŠØ¯Ø© ØªÙ…Ø§Ù…Ø§Ù‹ØŒ Ø³Ø£Ø­Ø§ÙˆÙ„ Ø­Ù„Ù‡Ø§ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¨Ø§Ø¯Ø¦ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙˆØ£ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†ØªÙŠØ¬Ø©"

    def get_learning_stats(self):
        """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù… ÙˆØ§Ù„Ù†Ù…Ùˆ"""
        return {
            "total_memories": len(self.long_term_memory),
            "learned_patterns": len(self.learned_patterns),
            "concept_network_size": len(self.concept_network),
            "total_connections": sum(len(connections) for connections in self.concept_network.values()),
            "capabilities": self.capabilities,
            "overall_intelligence": self.calculate_growth(),
            "learning_efficiency": self.capabilities["learning_efficiency"]
        }

if __name__ == "__main__":
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ø¸Ø§Ù…
    ai = TrueLearningAI()
    
    print("\n" + "="*60)
    print("ðŸ§  Ø§Ø®ØªØ¨Ø§Ø± Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ")
    print("="*60)
    
    # ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©
    test_info = [
        "Ø§Ù„Ù…Ø§Ø¡ ÙŠØªÙƒÙˆÙ† Ù…Ù† Ø°Ø±ØªÙŠÙ† Ù‡ÙŠØ¯Ø±ÙˆØ¬ÙŠÙ† ÙˆØ°Ø±Ø© Ø£ÙƒØ³Ø¬ÙŠÙ†",
        "Ø§Ù„Ù†Ø§Ø± ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø£ÙƒØ³Ø¬ÙŠÙ† Ù„Ù„Ø§Ø´ØªØ¹Ø§Ù„",
        "Ø§Ù„Ø£ÙƒØ³Ø¬ÙŠÙ† Ø¶Ø±ÙˆØ±ÙŠ Ù„Ù„Ø­ÙŠØ§Ø© ÙˆØ§Ù„Ø§Ø­ØªØ±Ø§Ù‚",
        "ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† Ø¥Ø·ÙØ§Ø¡ Ø§Ù„Ù†Ø§Ø±ØŸ"
    ]
    
    for info in test_info:
        print(f"\nðŸ“š ØªØ¹Ù„Ù…: {info}")
        result = ai.encounter_new_information(info)
        print(f"âœ… ØªÙ… Ø§Ù„ØªØ¹Ù„Ù… - Ø£Ù†Ù…Ø§Ø· Ø¬Ø¯ÙŠØ¯Ø©: {result['new_patterns_discovered']}")
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø­Ù„ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„
    print(f"\nðŸŽ¯ Ø§Ø®ØªØ¨Ø§Ø± Ø­Ù„ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„:")
    problem = "ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† Ø¥Ø·ÙØ§Ø¡ Ø­Ø±ÙŠÙ‚ ÙÙŠ Ø§Ù„Ù…Ø·Ø¨Ø®ØŸ"
    solution_result = ai.solve_problem(problem)
    print(f"ðŸ’¡ Ø§Ù„Ø­Ù„: {solution_result['solution']}")
    print(f"ðŸŽ¯ Ø§Ù„Ø«Ù‚Ø©: {solution_result['confidence']:.2f}")
    
    # Ø¹Ø±Ø¶ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù…
    stats = ai.get_learning_stats()
    print(f"\nðŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù…:")
    print(f"ðŸ§  Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø°ÙƒØ±ÙŠØ§Øª: {stats['total_memories']}")
    print(f"ðŸ” Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ØªØ¹Ù„Ù…Ø©: {stats['learned_patterns']}")
    print(f"ðŸ•¸ï¸ Ø´Ø¨ÙƒØ© Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…: {stats['concept_network_size']} Ù…ÙÙ‡ÙˆÙ…")
    print(f"ðŸ“ˆ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø¹Ø§Ù…: {stats['overall_intelligence']:.3f}")


# Ø¯Ø§Ù„Ø© Ù…ÙˆØ­Ø¯Ø© Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¨Ø§Ø´Ø±
def solve_task(task_data):
    """Ø­Ù„ Ø§Ù„Ù…Ù‡Ù…Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ø¸Ø§Ù…"""
    import numpy as np
    
    try:
        # Ø¥Ù†Ø´Ø§Ø¡ ÙƒØ§Ø¦Ù† Ù…Ù† Ø§Ù„Ù†Ø¸Ø§Ù…
        system = TrueLearningAI()
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø¯ÙˆØ§Ù„ Ø§Ù„Ø­Ù„ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
        if hasattr(system, 'solve'):
            return system.solve(task_data)
        elif hasattr(system, 'solve_task'):
            return system.solve_task(task_data)
        elif hasattr(system, 'predict'):
            return system.predict(task_data)
        elif hasattr(system, 'forward'):
            return system.forward(task_data)
        elif hasattr(system, 'run'):
            return system.run(task_data)
        elif hasattr(system, 'process'):
            return system.process(task_data)
        elif hasattr(system, 'execute'):
            return system.execute(task_data)
        else:
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„ÙƒØ§Ø¦Ù† Ù…Ø¨Ø§Ø´Ø±Ø©
            if callable(system):
                return system(task_data)
    except Exception as e:
        # ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„ÙØ´Ù„ØŒ Ø£Ø±Ø¬Ø¹ Ø­Ù„ Ø¨Ø³ÙŠØ·
        import numpy as np
        if 'train' in task_data and task_data['train']:
            return np.array(task_data['train'][0]['output'])
        return np.zeros((3, 3))
