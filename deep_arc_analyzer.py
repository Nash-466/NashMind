from __future__ import annotations
#!/usr/bin/env python3
"""
DEEP ARC ANALYZER - ØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙŠÙ‚ Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…Ø³Ø§Ø¨Ù‚Ø©
===============================================
ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ø¬Ù…ÙŠØ¹ Ù…Ù‡Ø§Ù… ARC Ù„ÙÙ‡Ù… Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©
"""

import json
import numpy as np
from collections.abc import Callable
from typing import Dict, List, Any, Tuple
from collections import defaultdict, Counter
import os

class DeepARCAnalyzer:
    """Ù…Ø­Ù„Ù„ Ø¹Ù…ÙŠÙ‚ Ù„Ù…Ù‡Ø§Ù… ARC"""
    
    def __init__(self, data_folder: str):
        self.data_folder = data_folder
        self.training_tasks = {}
        self.evaluation_tasks = {}
        self.evaluation_solutions = {}
        self.pattern_statistics = defaultdict(int)
        self.transformation_types = defaultdict(list)
        
    def load_all_data(self):
        """ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³Ø§Ø¨Ù‚Ø©"""
        # ØªØ­Ù…ÙŠÙ„ Ù…Ù‡Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        train_file = os.path.join(self.data_folder, "arc-agi_training_challenges.json")
        if os.path.exists(train_file):
            with open(train_file, 'r') as f:
                self.training_tasks = json.load(f)
            print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(self.training_tasks)} Ù…Ù‡Ù…Ø© ØªØ¯Ø±ÙŠØ¨")
        
        # ØªØ­Ù…ÙŠÙ„ Ù…Ù‡Ø§Ù… Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
        eval_file = os.path.join(self.data_folder, "arc-agi_evaluation_challenges.json")
        if os.path.exists(eval_file):
            with open(eval_file, 'r') as f:
                self.evaluation_tasks = json.load(f)
            print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(self.evaluation_tasks)} Ù…Ù‡Ù…Ø© ØªÙ‚ÙŠÙŠÙ…")
        
        # ØªØ­Ù…ÙŠÙ„ Ø­Ù„ÙˆÙ„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
        sol_file = os.path.join(self.data_folder, "arc-agi_evaluation_solutions.json")
        if os.path.exists(sol_file):
            with open(sol_file, 'r') as f:
                self.evaluation_solutions = json.load(f)
            print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø­Ù„ÙˆÙ„ {len(self.evaluation_solutions)} Ù…Ù‡Ù…Ø©")
    
    def analyze_all_patterns(self):
        """ØªØ­Ù„ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ù†Ù…Ø§Ø· ÙÙŠ Ø§Ù„Ù…Ù‡Ø§Ù…"""
        print("\nğŸ” Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù„Ù„Ø£Ù†Ù…Ø§Ø·...")
        
        # ØªØ­Ù„ÙŠÙ„ Ù…Ù‡Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        for task_id, task_data in self.training_tasks.items():
            self._analyze_single_task(task_id, task_data, "training")
        
        # ØªØ­Ù„ÙŠÙ„ Ù…Ù‡Ø§Ù… Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
        for task_id, task_data in self.evaluation_tasks.items():
            self._analyze_single_task(task_id, task_data, "evaluation")
        
        self._generate_pattern_report()
    
    def _analyze_single_task(self, task_id: str, task_data: Dict, task_type: str):
        """ØªØ­Ù„ÙŠÙ„ Ù…Ù‡Ù…Ø© ÙˆØ§Ø­Ø¯Ø©"""
        train_examples = task_data.get('train', [])
        
        for i, example in enumerate(train_examples):
            input_grid = np.array(example['input'])
            output_grid = np.array(example['output'])
            
            # ØªØ­Ù„ÙŠÙ„ Ù†ÙˆØ¹ Ø§Ù„ØªØ­ÙˆÙŠÙ„
            transform_type = self._classify_transformation(input_grid, output_grid)
            self.transformation_types[transform_type].append(f"{task_id}_{i}")
            
            # ØªØ­Ù„ÙŠÙ„ Ø®ØµØ§Ø¦Øµ Ø§Ù„Ø´Ø¨ÙƒØ©
            self._analyze_grid_properties(input_grid, output_grid, task_id)
    
    def _classify_transformation(self, input_grid: np.ndarray, output_grid: np.ndarray) -> str:
        """ØªØµÙ†ÙŠÙ Ù†ÙˆØ¹ Ø§Ù„ØªØ­ÙˆÙŠÙ„"""
        
        # ÙØ­Øµ ØªØºÙŠÙŠØ± Ø§Ù„Ø­Ø¬Ù…
        if input_grid.shape != output_grid.shape:
            h_ratio = output_grid.shape[0] / input_grid.shape[0]
            w_ratio = output_grid.shape[1] / input_grid.shape[1]
            
            if h_ratio == w_ratio:
                if h_ratio > 1:
                    return f"size_expand_{int(h_ratio)}x"
                else:
                    return f"size_shrink_{int(1/h_ratio)}x"
            else:
                return "size_asymmetric"
        
        # ÙØ­Øµ Ø§Ù„ØªØ­ÙˆÙŠÙ„Ø§Øª Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ©
        if np.array_equal(output_grid, np.rot90(input_grid, 1)):
            return "rotate_90"
        elif np.array_equal(output_grid, np.rot90(input_grid, 2)):
            return "rotate_180"
        elif np.array_equal(output_grid, np.rot90(input_grid, 3)):
            return "rotate_270"
        elif np.array_equal(output_grid, np.fliplr(input_grid)):
            return "flip_horizontal"
        elif np.array_equal(output_grid, np.flipud(input_grid)):
            return "flip_vertical"
        elif np.array_equal(output_grid, input_grid.T):
            return "transpose"
        
        # ÙØ­Øµ ØªØ­ÙˆÙŠÙ„Ø§Øª Ø§Ù„Ø£Ù„ÙˆØ§Ù†
        if input_grid.shape == output_grid.shape:
            if self._is_color_mapping(input_grid, output_grid):
                return "color_mapping"
            elif self._is_color_shift(input_grid, output_grid):
                return "color_shift"
        
        # ÙØ­Øµ Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªÙƒØ±Ø§Ø±
        if self._is_tiling_pattern(input_grid, output_grid):
            return "tiling_pattern"
        
        # ÙØ­Øµ Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„Ø£Ù†Ù…Ø§Ø·
        if self._is_pattern_completion(input_grid, output_grid):
            return "pattern_completion"
        
        # ÙØ­Øµ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª
        if self._is_object_manipulation(input_grid, output_grid):
            return "object_manipulation"
        
        return "complex_unknown"
    
    def _is_color_mapping(self, input_grid: np.ndarray, output_grid: np.ndarray) -> bool:
        """ÙØ­Øµ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ù‡Ùˆ ØªØ­ÙˆÙŠÙ„ Ø£Ù„ÙˆØ§Ù†"""
        if input_grid.shape != output_grid.shape:
            return False
        
        color_map = {}
        for i in range(input_grid.shape[0]):
            for j in range(input_grid.shape[1]):
                input_color = input_grid[i, j]
                output_color = output_grid[i, j]
                
                if input_color in color_map:
                    if color_map[input_color] != output_color:
                        return False
                else:
                    color_map[input_color] = output_color
        
        return len(color_map) > 1  # ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù‡Ù†Ø§Ùƒ ØªØ­ÙˆÙŠÙ„ ÙØ¹Ù„ÙŠ
    
    def _is_color_shift(self, input_grid: np.ndarray, output_grid: np.ndarray) -> bool:
        """ÙØ­Øµ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ù‡Ùˆ Ø¥Ø²Ø§Ø­Ø© Ø£Ù„ÙˆØ§Ù†"""
        if input_grid.shape != output_grid.shape:
            return False
        
        # ÙØ­Øµ Ø¥Ø²Ø§Ø­Ø© Ø«Ø§Ø¨ØªØ©
        diff = output_grid - input_grid
        return np.all(diff == diff[0, 0])
    
    def _is_tiling_pattern(self, input_grid: np.ndarray, output_grid: np.ndarray) -> bool:
        """ÙØ­Øµ Ù†Ù…Ø· Ø§Ù„ØªÙƒØ±Ø§Ø±"""
        h_in, w_in = input_grid.shape
        h_out, w_out = output_grid.shape
        
        # ÙØ­Øµ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ù…Ø¶Ø§Ø¹Ù Ù„Ù„Ø¥Ø¯Ø®Ø§Ù„
        if h_out % h_in == 0 and w_out % w_in == 0:
            h_factor = h_out // h_in
            w_factor = w_out // w_in
            
            # ÙØ­Øµ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ØªÙƒØ±Ø§Ø± ØµØ­ÙŠØ­
            tiled = np.tile(input_grid, (h_factor, w_factor))
            return np.array_equal(tiled, output_grid)
        
        return False
    
    def _is_pattern_completion(self, input_grid: np.ndarray, output_grid: np.ndarray) -> bool:
        """ÙØ­Øµ Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„Ù†Ù…Ø·"""
        # Ù…Ù†Ø·Ù‚ Ø¨Ø³ÙŠØ·: Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø£ÙƒØ¨Ø± ÙˆÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
        if (output_grid.shape[0] >= input_grid.shape[0] and 
            output_grid.shape[1] >= input_grid.shape[1]):
            
            # ÙØ­Øµ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
            h_in, w_in = input_grid.shape
            return np.array_equal(output_grid[:h_in, :w_in], input_grid)
        
        return False
    
    def _is_object_manipulation(self, input_grid: np.ndarray, output_grid: np.ndarray) -> bool:
        """ÙØ­Øµ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª"""
        # ÙØ­Øµ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ù†ÙØ³Ù‡Ø§ Ù…ÙˆØ¬ÙˆØ¯Ø©
        input_colors = set(input_grid.flatten())
        output_colors = set(output_grid.flatten())
        
        return input_colors == output_colors and not np.array_equal(input_grid, output_grid)
    
    def _analyze_grid_properties(self, input_grid: np.ndarray, output_grid: np.ndarray, task_id: str):
        """ØªØ­Ù„ÙŠÙ„ Ø®ØµØ§Ø¦Øµ Ø§Ù„Ø´Ø¨ÙƒØ©"""
        # Ø­Ø¬Ù… Ø§Ù„Ø´Ø¨ÙƒØ©
        self.pattern_statistics[f"input_size_{input_grid.shape}"] += 1
        self.pattern_statistics[f"output_size_{output_grid.shape}"] += 1
        
        # Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ù„ÙˆØ§Ù†
        input_colors = len(np.unique(input_grid))
        output_colors = len(np.unique(output_grid))
        self.pattern_statistics[f"input_colors_{input_colors}"] += 1
        self.pattern_statistics[f"output_colors_{output_colors}"] += 1
        
        # Ù†Ø³Ø¨Ø© Ø§Ù„Ø£Ù„ÙˆØ§Ù† ØºÙŠØ± Ø§Ù„ØµÙØ±ÙŠØ©
        input_nonzero = np.count_nonzero(input_grid) / input_grid.size
        output_nonzero = np.count_nonzero(output_grid) / output_grid.size
        
        self.pattern_statistics[f"input_density_{int(input_nonzero*10)}"] += 1
        self.pattern_statistics[f"output_density_{int(output_nonzero*10)}"] += 1
    
    def _generate_pattern_report(self):
        """Ø¥Ù†ØªØ§Ø¬ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£Ù†Ù…Ø§Ø·"""
        print("\nğŸ“Š ØªÙ‚Ø±ÙŠØ± ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø·:")
        print("=" * 50)
        
        print("\nğŸ”„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ØªØ­ÙˆÙŠÙ„Ø§Øª:")
        for transform_type, examples in self.transformation_types.items():
            print(f"  {transform_type}: {len(examples)} Ù…Ø«Ø§Ù„")
        
        print(f"\nğŸ“ˆ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ù†Ù…Ø§Ø· (Ø£Ù‡Ù… 20):")
        top_patterns = sorted(self.pattern_statistics.items(), 
                            key=lambda x: x[1], reverse=True)[:20]
        
        for pattern, count in top_patterns:
            print(f"  {pattern}: {count}")
        
        # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        self._save_analysis_report()
    
    def _save_analysis_report(self):
        """Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù„ÙŠÙ„"""
        report = {
            'transformation_types': dict(self.transformation_types),
            'pattern_statistics': dict(self.pattern_statistics),
            'total_training_tasks': len(self.training_tasks),
            'total_evaluation_tasks': len(self.evaluation_tasks),
            'analysis_summary': self._generate_summary()
        }
        
        with open('arc_deep_analysis_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙÙŠ: arc_deep_analysis_report.json")
    
    def _generate_summary(self) -> Dict[str, Any]:
        """Ø¥Ù†ØªØ§Ø¬ Ù…Ù„Ø®Øµ Ø§Ù„ØªØ­Ù„ÙŠÙ„"""
        return {
            'most_common_transformations': [
                transform for transform, examples in 
                sorted(self.transformation_types.items(), 
                      key=lambda x: len(x[1]), reverse=True)[:10]
            ],
            'complexity_distribution': self._analyze_complexity(),
            'size_patterns': self._analyze_size_patterns(),
            'color_patterns': self._analyze_color_patterns()
        }
    
    def _analyze_complexity(self) -> Dict[str, int]:
        """ØªØ­Ù„ÙŠÙ„ ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯"""
        complexity = {'simple': 0, 'medium': 0, 'complex': 0}
        
        for transform_type, examples in self.transformation_types.items():
            if transform_type in ['rotate_90', 'rotate_180', 'flip_horizontal', 'flip_vertical']:
                complexity['simple'] += len(examples)
            elif transform_type in ['color_mapping', 'tiling_pattern', 'size_expand_2x']:
                complexity['medium'] += len(examples)
            else:
                complexity['complex'] += len(examples)
        
        return complexity
    
    def _analyze_size_patterns(self) -> Dict[str, int]:
        """ØªØ­Ù„ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø£Ø­Ø¬Ø§Ù…"""
        size_patterns = defaultdict(int)
        
        for pattern, count in self.pattern_statistics.items():
            if 'size_' in pattern:
                size_patterns[pattern] = count
        
        return dict(size_patterns)
    
    def _analyze_color_patterns(self) -> Dict[str, int]:
        """ØªØ­Ù„ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø£Ù„ÙˆØ§Ù†"""
        color_patterns = defaultdict(int)
        
        for pattern, count in self.pattern_statistics.items():
            if 'colors_' in pattern:
                color_patterns[pattern] = count
        
        return dict(color_patterns)

    def get_insights_for_solver(self) -> Dict[str, Any]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±Ø¤Ù‰ Ù„ØªØ·ÙˆÙŠØ± Ø§Ù„Ø­Ù„Ø§Ù„"""
        insights = {
            'priority_transformations': [],
            'common_patterns': [],
            'complexity_levels': {},
            'recommendations': []
        }
        
        # Ø£Ù‡Ù… Ø§Ù„ØªØ­ÙˆÙŠÙ„Ø§Øª Ø­Ø³Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±
        sorted_transforms = sorted(self.transformation_types.items(), 
                                 key=lambda x: len(x[1]), reverse=True)
        
        insights['priority_transformations'] = [
            {'type': t, 'frequency': len(examples), 'examples': examples[:5]}
            for t, examples in sorted_transforms[:15]
        ]
        
        # Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
        insights['common_patterns'] = [
            {'pattern': p, 'count': c}
            for p, c in sorted(self.pattern_statistics.items(), 
                             key=lambda x: x[1], reverse=True)[:20]
        ]
        
        # Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
        insights['complexity_levels'] = self._analyze_complexity()
        
        # ØªÙˆØµÙŠØ§Øª Ù„Ù„ØªØ·ÙˆÙŠØ±
        insights['recommendations'] = self._generate_development_recommendations()
        
        return insights
    
    def _generate_development_recommendations(self) -> List[str]:
        """Ø¥Ù†ØªØ§Ø¬ ØªÙˆØµÙŠØ§Øª Ù„Ù„ØªØ·ÙˆÙŠØ±"""
        recommendations = []
        
        # ØªØ­Ù„ÙŠÙ„ Ø£Ù‡Ù… Ø§Ù„ØªØ­ÙˆÙŠÙ„Ø§Øª
        top_transforms = sorted(self.transformation_types.items(), 
                              key=lambda x: len(x[1]), reverse=True)[:5]
        
        recommendations.append(f"ğŸ¯ Ø±ÙƒØ² Ø¹Ù„Ù‰ ØªØ·ÙˆÙŠØ± {len(top_transforms)} ØªØ­ÙˆÙŠÙ„Ø§Øª Ø±Ø¦ÙŠØ³ÙŠØ©")
        
        for transform, examples in top_transforms:
            recommendations.append(f"  - {transform}: {len(examples)} Ù…Ø«Ø§Ù„")
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
        complexity = self._analyze_complexity()
        total = sum(complexity.values())
        
        if complexity['complex'] / total > 0.3:
            recommendations.append("âš ï¸ 30%+ Ù…Ù† Ø§Ù„Ù…Ù‡Ø§Ù… Ù…Ø¹Ù‚Ø¯Ø© - Ø·ÙˆØ± Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ù…ØªÙ‚Ø¯Ù…Ø©")
        
        if complexity['simple'] / total > 0.4:
            recommendations.append("âœ… 40%+ Ù…Ù† Ø§Ù„Ù…Ù‡Ø§Ù… Ø¨Ø³ÙŠØ·Ø© - Ø§Ø¨Ø¯Ø£ Ø¨Ø§Ù„ØªØ­ÙˆÙŠÙ„Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©")
        
        return recommendations

# ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„
if __name__ == "__main__":
    data_folder = "Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø³Ø§Ø¨Ù‚Ø©arc-prize-2025"
    
    analyzer = DeepARCAnalyzer(data_folder)
    analyzer.load_all_data()
    analyzer.analyze_all_patterns()
    
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø¤Ù‰ Ù„Ù„ØªØ·ÙˆÙŠØ±
    insights = analyzer.get_insights_for_solver()
    
    print("\nğŸš€ Ø±Ø¤Ù‰ Ù„ØªØ·ÙˆÙŠØ± Ø§Ù„Ø­Ù„Ø§Ù„:")
    print("=" * 40)
    
    for rec in insights['recommendations']:
        print(rec)
    
    print(f"\nğŸ“Š Ø£Ù‡Ù… {len(insights['priority_transformations'])} ØªØ­ÙˆÙŠÙ„Ø§Øª:")
    for i, transform in enumerate(insights['priority_transformations'][:10], 1):
        print(f"{i}. {transform['type']}: {transform['frequency']} Ù…Ø±Ø©")
