from __future__ import annotations
#!/usr/bin/env python3
"""
COMPREHENSIVE TEST FRAMEWORK - Ø¥Ø·Ø§Ø± Ø§Ø®ØªØ¨Ø§Ø± Ø´Ø§Ù…Ù„
=============================================
Ù†Ø¸Ø§Ù… Ø§Ø®ØªØ¨Ø§Ø± Ù…ØªÙƒØ§Ù…Ù„ Ù„Ù‚ÙŠØ§Ø³ Ø£Ø¯Ø§Ø¡ Ø­Ù„Ø§Ù„Ø§Øª ARC Ø¨Ø¯Ù‚Ø©
"""

import os
import json
import numpy as np
import time
from collections.abc import Callable
from typing import Dict, List, Any, Tuple
from collections import defaultdict
import traceback

class ComprehensiveTestFramework:
    """Ø¥Ø·Ø§Ø± Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø´Ø§Ù…Ù„"""
    
    def __init__(self):
        self.test_results = {}
        self.performance_metrics = {}
        self.error_log = []
        
    def run_full_evaluation(self, solver_class, max_tasks: int = 50):
        """ØªØ´ØºÙŠÙ„ ØªÙ‚ÙŠÙŠÙ… Ø´Ø§Ù…Ù„ Ù„Ù„Ø­Ù„Ø§Ù„"""
        
        print(f"ğŸ§ª Ø¨Ø¯Ø¡ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù„Ø­Ù„Ø§Ù„...")
        print(f"ğŸ“Š Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…Ø³ØªÙ‡Ø¯ÙØ©: {max_tasks}")
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù‡Ø§Ù…
        tasks = self._load_tasks()
        if not tasks:
            print("âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù‡Ø§Ù…")
            return
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ù…Ù‡Ø§Ù…
        task_ids = list(tasks.keys())[:max_tasks]
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø­Ù„Ø§Ù„
        try:
            solver = solver_class()
            print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø­Ù„Ø§Ù„: {solver_class.__name__}")
        except Exception as e:
            print(f"âŒ ÙØ´Ù„ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø­Ù„Ø§Ù„: {e}")
            return
        
        # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
        results = {
            'total_tasks': len(task_ids),
            'successful_tasks': 0,
            'failed_tasks': 0,
            'error_tasks': 0,
            'average_time': 0,
            'pattern_success_rate': defaultdict(int),
            'pattern_total_count': defaultdict(int),
            'detailed_results': []
        }
        
        total_time = 0
        
        for i, task_id in enumerate(task_ids):
            print(f"\nğŸ¯ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ù‡Ù…Ø© {i+1}/{len(task_ids)}: {task_id}")
            
            task = tasks[task_id]
            start_time = time.time()
            
            try:
                # Ù…Ø­Ø§ÙˆÙ„Ø© Ø­Ù„ Ø§Ù„Ù…Ù‡Ù…Ø©
                solutions = solver.solve_task(task)
                execution_time = time.time() - start_time
                total_time += execution_time
                
                # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†ØªÙŠØ¬Ø©
                success = self._evaluate_solution(solutions, task)
                
                if success:
                    results['successful_tasks'] += 1
                    print(f"  âœ… Ù†Ø¬Ø­ ({execution_time:.2f}s)")
                else:
                    results['failed_tasks'] += 1
                    print(f"  âŒ ÙØ´Ù„ ({execution_time:.2f}s)")
                
                # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ©
                task_result = {
                    'task_id': task_id,
                    'success': success,
                    'execution_time': execution_time,
                    'solution_count': len(solutions) if solutions else 0
                }
                results['detailed_results'].append(task_result)
                
            except Exception as e:
                results['error_tasks'] += 1
                execution_time = time.time() - start_time
                total_time += execution_time
                
                print(f"  ğŸ’¥ Ø®Ø·Ø£ ({execution_time:.2f}s): {str(e)[:50]}...")
                
                self.error_log.append({
                    'task_id': task_id,
                    'error': str(e),
                    'traceback': traceback.format_exc()
                })
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        results['average_time'] = total_time / len(task_ids)
        results['success_rate'] = (results['successful_tasks'] / results['total_tasks']) * 100
        
        # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        self._print_final_results(results)
        
        # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        self._save_results(results, solver_class.__name__)
        
        return results
    
    def _load_tasks(self) -> Dict:
        """ØªØ­Ù…ÙŠÙ„ Ù…Ù‡Ø§Ù… ARC"""
        
        data_paths = [
            'Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø³Ø§Ø¨Ù‚Ø©arc-prize-2025/arc-agi_training_challenges.json',
            'data/arc-tasks/arc-agi_training_challenges.json',
            'arc-agi_training_challenges.json'
        ]
        
        for path in data_paths:
            if os.path.exists(path):
                try:
                    with open(path, 'r') as f:
                        tasks = json.load(f)
                    print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(tasks)} Ù…Ù‡Ù…Ø© Ù…Ù†: {path}")
                    return tasks
                except Exception as e:
                    print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ {path}: {e}")
        
        print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„ÙØ§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        return {}
    
    def _evaluate_solution(self, solutions: List[np.ndarray], task: Dict) -> bool:
        """ØªÙ‚ÙŠÙŠÙ… ØµØ­Ø© Ø§Ù„Ø­Ù„"""
        
        if not solutions:
            return False
        
        # ÙØ­Øµ Ø¹Ø¯Ø¯ Ø§Ù„Ø­Ù„ÙˆÙ„
        if len(solutions) != len(task['test']):
            return False
        
        # ÙØ­Øµ ÙƒÙ„ Ø­Ù„
        for i, solution in enumerate(solutions):
            if solution is None or solution.size == 0:
                return False
            
            # ÙØ­Øµ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠØ©
            if solution.shape[0] < 1 or solution.shape[1] < 1:
                return False
            
            # ÙØ­Øµ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ù…Ø¹Ù‚ÙˆÙ„Ø©
            if solution.shape[0] > 30 or solution.shape[1] > 30:
                return False
        
        return True
    
    def _print_final_results(self, results: Dict):
        """Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©"""
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ù„Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø´Ø§Ù…Ù„")
        print(f"{'='*60}")
        
        print(f"ğŸ“ˆ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù‡Ø§Ù…: {results['total_tasks']}")
        print(f"âœ… Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù†Ø§Ø¬Ø­Ø©: {results['successful_tasks']}")
        print(f"âŒ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„ÙØ§Ø´Ù„Ø©: {results['failed_tasks']}")
        print(f"ğŸ’¥ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…Ø¹Ø·Ù„Ø©: {results['error_tasks']}")
        
        print(f"\nğŸ¯ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­: {results['success_rate']:.1f}%")
        print(f"â±ï¸ Ù…ØªÙˆØ³Ø· ÙˆÙ‚Øª Ø§Ù„ØªÙ†ÙÙŠØ°: {results['average_time']:.2f} Ø«Ø§Ù†ÙŠØ©")
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡
        if results['success_rate'] >= 80:
            print("ğŸ‰ Ø£Ø¯Ø§Ø¡ Ù…Ù…ØªØ§Ø²!")
        elif results['success_rate'] >= 60:
            print("ğŸ‘ Ø£Ø¯Ø§Ø¡ Ø¬ÙŠØ¯")
        elif results['success_rate'] >= 40:
            print("âš ï¸ Ø£Ø¯Ø§Ø¡ Ù…ØªÙˆØ³Ø· - ÙŠØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ†")
        else:
            print("ğŸš¨ Ø£Ø¯Ø§Ø¡ Ø¶Ø¹ÙŠÙ - ÙŠØ­ØªØ§Ø¬ Ø¥Ø¹Ø§Ø¯Ø© ØªØµÙ…ÙŠÙ…")
    
    def _save_results(self, results: Dict, solver_name: str):
        """Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬"""
        
        timestamp = int(time.time())
        filename = f"test_results_{solver_name}_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False, default=str)
            
            print(f"\nğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ: {filename}")
            
        except Exception as e:
            print(f"âŒ ÙØ´Ù„ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬: {e}")
    
    def compare_solvers(self, solver_classes: List, max_tasks: int = 20):
        """Ù…Ù‚Ø§Ø±Ù†Ø© Ø¹Ø¯Ø© Ø­Ù„Ø§Ù„Ø§Øª"""
        
        print(f"ğŸ”¬ Ù…Ù‚Ø§Ø±Ù†Ø© {len(solver_classes)} Ø­Ù„Ø§Ù„...")
        
        comparison_results = {}
        
        for solver_class in solver_classes:
            print(f"\n{'='*50}")
            print(f"Ø§Ø®ØªØ¨Ø§Ø±: {solver_class.__name__}")
            print(f"{'='*50}")
            
            results = self.run_full_evaluation(solver_class, max_tasks)
            comparison_results[solver_class.__name__] = results
        
        # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
        self._print_comparison(comparison_results)
        
        return comparison_results
    
    def _print_comparison(self, comparison_results: Dict):
        """Ø·Ø¨Ø§Ø¹Ø© Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø­Ù„Ø§Ù„Ø§Øª"""
        
        print(f"\n{'='*80}")
        print(f"ğŸ“Š Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø­Ù„Ø§Ù„Ø§Øª")
        print(f"{'='*80}")
        
        print(f"{'Ø§Ù„Ø­Ù„Ø§Ù„':<30} {'Ø§Ù„Ù†Ø¬Ø§Ø­%':<10} {'Ø§Ù„ÙˆÙ‚Øª(s)':<12} {'Ø§Ù„Ø£Ø®Ø·Ø§Ø¡':<8}")
        print(f"{'-'*70}")
        
        for solver_name, results in comparison_results.items():
            success_rate = results['success_rate']
            avg_time = results['average_time']
            errors = results['error_tasks']
            
            print(f"{solver_name:<30} {success_rate:<10.1f} {avg_time:<12.2f} {errors:<8}")
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£ÙØ¶Ù„
        best_solver = max(comparison_results.keys(), 
                         key=lambda x: comparison_results[x]['success_rate'])
        
        print(f"\nğŸ† Ø£ÙØ¶Ù„ Ø­Ù„Ø§Ù„: {best_solver}")
        print(f"   Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­: {comparison_results[best_solver]['success_rate']:.1f}%")

def test_current_systems():
    """Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©"""
    
    print("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©...")
    
    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø£Ù†Ø¸Ù…Ø©
    systems_to_test = []
    
    try:
        from arc_clean_integrated_system import ARCCleanIntegratedSystem
        systems_to_test.append(ARCCleanIntegratedSystem)
        print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„")
    except Exception as e:
        print(f"âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„: {e}")
    
    try:
        from arc_ultimate_perfect_system import ARCUltimatePerfectSolver
        systems_to_test.append(ARCUltimatePerfectSolver)
        print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø«Ø§Ù„ÙŠ")
    except Exception as e:
        print(f"âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø«Ø§Ù„ÙŠ: {e}")
    
    if not systems_to_test:
        print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£Ù†Ø¸Ù…Ø© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±")
        return
    
    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
    framework = ComprehensiveTestFramework()
    
    if len(systems_to_test) == 1:
        framework.run_full_evaluation(systems_to_test[0], max_tasks=10)
    else:
        framework.compare_solvers(systems_to_test, max_tasks=10)

if __name__ == "__main__":
    test_current_systems()
