from __future__ import annotations
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ” ØªØ­Ù„ÙŠÙ„ Ø¨Ø³ÙŠØ· Ù„Ù„Ø£Ù†Ù…Ø§Ø· ÙÙŠ Ù…Ù‡Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ARC
==========================================

ØªØ­Ù„ÙŠÙ„ Ù…Ø¨Ø³Ø· Ø¨Ø¯ÙˆÙ† matplotlib Ù„ÙÙ‡Ù… Ø®ØµØ§Ø¦Øµ Ù…Ù‡Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨.
"""

import json
import numpy as np
from collections import Counter, defaultdict
from collections.abc import Callable
from typing import Dict, List, Any, Tuple
from pathlib import Path
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)


def load_training_data() -> Dict[str, Any]:
    """ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
    
    challenges_file = Path("arc-agi_training_challenges.json")
    
    if not challenges_file.exists():
        raise FileNotFoundError(f"Training challenges file not found: {challenges_file}")
    
    with open(challenges_file, 'r') as f:
        challenges = json.load(f)
    
    logging.info(f"ğŸ“š Loaded {len(challenges)} training challenges")
    return challenges


def analyze_grid_properties(grid: List[List[int]]) -> Dict[str, Any]:
    """ØªØ­Ù„ÙŠÙ„ Ø®ØµØ§Ø¦Øµ Ø§Ù„Ø´Ø¨ÙƒØ©"""
    
    grid_np = np.array(grid)
    height, width = grid_np.shape
    
    # Basic properties
    properties = {
        'height': height,
        'width': width,
        'area': height * width,
        'aspect_ratio': width / height,
        'unique_colors': len(np.unique(grid_np)),
        'color_distribution': Counter(grid_np.flatten()),
        'is_square': height == width,
        'max_dimension': max(height, width),
        'min_dimension': min(height, width)
    }
    
    # Pattern analysis
    properties['has_symmetry_h'] = np.array_equal(grid_np, np.fliplr(grid_np))
    properties['has_symmetry_v'] = np.array_equal(grid_np, np.flipud(grid_np))
    properties['has_symmetry_d1'] = np.array_equal(grid_np, grid_np.T)
    
    # Color analysis
    background_color = Counter(grid_np.flatten()).most_common(1)[0][0]
    properties['background_color'] = background_color
    properties['background_ratio'] = properties['color_distribution'][background_color] / properties['area']
    
    # Connected components (simple version)
    non_background = grid_np != background_color
    properties['non_background_cells'] = np.sum(non_background)
    properties['density'] = properties['non_background_cells'] / properties['area']
    
    return properties


def analyze_transformation_patterns(train_pairs: List[Dict]) -> Dict[str, Any]:
    """ØªØ­Ù„ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªØ­ÙˆÙŠÙ„"""
    
    patterns = {
        'size_changes': [],
        'color_changes': [],
        'shape_preservation': [],
        'scaling_factors': []
    }
    
    for pair in train_pairs:
        input_grid = np.array(pair['input'])
        output_grid = np.array(pair['output'])
        
        input_props = analyze_grid_properties(pair['input'])
        output_props = analyze_grid_properties(pair['output'])
        
        # Size changes
        size_change = {
            'input_size': input_props['area'],
            'output_size': output_props['area'],
            'size_ratio': output_props['area'] / input_props['area'],
            'width_change': output_props['width'] - input_props['width'],
            'height_change': output_props['height'] - input_props['height']
        }
        patterns['size_changes'].append(size_change)
        
        # Scaling factors
        width_scale = output_props['width'] / input_props['width']
        height_scale = output_props['height'] / input_props['height']
        patterns['scaling_factors'].append({
            'width_scale': width_scale,
            'height_scale': height_scale,
            'uniform_scale': abs(width_scale - height_scale) < 0.1
        })
        
        # Color changes
        input_colors = set(input_props['color_distribution'].keys())
        output_colors = set(output_props['color_distribution'].keys())
        
        color_change = {
            'input_colors': len(input_colors),
            'output_colors': len(output_colors),
            'new_colors': len(output_colors - input_colors),
            'removed_colors': len(input_colors - output_colors),
            'preserved_colors': len(input_colors & output_colors)
        }
        patterns['color_changes'].append(color_change)
        
        # Shape preservation
        if input_grid.shape == output_grid.shape:
            patterns['shape_preservation'].append(True)
        else:
            patterns['shape_preservation'].append(False)
    
    return patterns


def analyze_problem_complexity(challenge_data: Dict) -> Dict[str, Any]:
    """ØªØ­Ù„ÙŠÙ„ ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù…Ø³Ø£Ù„Ø©"""
    
    train_pairs = challenge_data.get('train', [])
    test_pairs = challenge_data.get('test', [])
    
    complexity = {
        'num_train_examples': len(train_pairs),
        'num_test_examples': len(test_pairs),
        'avg_input_size': 0,
        'avg_output_size': 0,
        'max_colors_used': 0,
        'size_consistency': 0,
        'transformation_type': 'unknown'
    }
    
    if not train_pairs:
        return complexity
    
    # Calculate averages
    input_sizes = []
    output_sizes = []
    colors_used = set()
    
    for pair in train_pairs:
        input_props = analyze_grid_properties(pair['input'])
        output_props = analyze_grid_properties(pair['output'])
        
        input_sizes.append(input_props['area'])
        output_sizes.append(output_props['area'])
        colors_used.update(input_props['color_distribution'].keys())
        colors_used.update(output_props['color_distribution'].keys())
    
    complexity['avg_input_size'] = np.mean(input_sizes)
    complexity['avg_output_size'] = np.mean(output_sizes)
    complexity['max_colors_used'] = len(colors_used)
    
    # Analyze transformation patterns
    transformation_patterns = analyze_transformation_patterns(train_pairs)
    
    # Calculate consistency score based on size changes
    size_ratios = [sc['size_ratio'] for sc in transformation_patterns['size_changes']]
    if size_ratios:
        complexity['size_consistency'] = 1.0 - np.std(size_ratios)
    
    # Determine transformation type
    scaling_factors = transformation_patterns['scaling_factors']
    if scaling_factors:
        uniform_scales = [sf['uniform_scale'] for sf in scaling_factors]
        if all(uniform_scales):
            complexity['transformation_type'] = 'uniform_scaling'
        elif any(sf['width_scale'] != 1 or sf['height_scale'] != 1 for sf in scaling_factors):
            complexity['transformation_type'] = 'scaling'
        elif all(transformation_patterns['shape_preservation']):
            complexity['transformation_type'] = 'shape_preserving'
        else:
            complexity['transformation_type'] = 'shape_changing'
    
    return complexity


def generate_statistics(challenges: Dict[str, Any]) -> Dict[str, Any]:
    """Ø¥Ù†ØªØ§Ø¬ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø´Ø§Ù…Ù„Ø©"""
    
    stats = {
        'total_problems': len(challenges),
        'grid_sizes': [],
        'color_usage': Counter(),
        'complexity_distribution': [],
        'transformation_types': defaultdict(int),
        'size_patterns': [],
        'symmetry_patterns': defaultdict(int),
        'scaling_patterns': defaultdict(int)
    }
    
    for problem_id, challenge_data in challenges.items():
        
        # Analyze complexity
        complexity = analyze_problem_complexity(challenge_data)
        stats['complexity_distribution'].append(complexity)
        stats['transformation_types'][complexity['transformation_type']] += 1
        
        # Analyze all grids in the problem
        all_grids = []
        
        # Training examples
        for pair in challenge_data.get('train', []):
            all_grids.extend([pair['input'], pair['output']])
        
        # Test examples
        for pair in challenge_data.get('test', []):
            all_grids.append(pair['input'])
        
        # Analyze each grid
        for grid in all_grids:
            props = analyze_grid_properties(grid)
            
            stats['grid_sizes'].append((props['height'], props['width']))
            stats['color_usage'].update(props['color_distribution'])
            
            # Symmetry patterns
            if props['has_symmetry_h']:
                stats['symmetry_patterns']['horizontal'] += 1
            if props['has_symmetry_v']:
                stats['symmetry_patterns']['vertical'] += 1
            if props['has_symmetry_d1']:
                stats['symmetry_patterns']['diagonal'] += 1
        
        # Analyze scaling patterns
        train_pairs = challenge_data.get('train', [])
        if train_pairs:
            transformation_patterns = analyze_transformation_patterns(train_pairs)
            for sf in transformation_patterns['scaling_factors']:
                if sf['uniform_scale']:
                    scale_factor = sf['width_scale']
                    if scale_factor == 2:
                        stats['scaling_patterns']['2x'] += 1
                    elif scale_factor == 3:
                        stats['scaling_patterns']['3x'] += 1
                    elif scale_factor == 0.5:
                        stats['scaling_patterns']['0.5x'] += 1
                    elif scale_factor == 1:
                        stats['scaling_patterns']['1x'] += 1
                    else:
                        stats['scaling_patterns']['other'] += 1
    
    return stats


def print_summary(stats: Dict[str, Any]):
    """Ø·Ø¨Ø§Ø¹Ø© Ù…Ù„Ø®Øµ Ø§Ù„ØªØ­Ù„ÙŠÙ„"""
    
    print("\n" + "="*80)
    print("ğŸ“Š Ù…Ù„Ø®Øµ ØªØ­Ù„ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ù…Ù‡Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨")
    print("="*80)
    
    print(f"ğŸ“š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù‡Ø§Ù…: {stats['total_problems']}")
    print(f"ğŸ¨ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©: {len(stats['color_usage'])}")
    print(f"ğŸ“ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ù…Ø­Ù„Ù„Ø©: {len(stats['grid_sizes'])}")
    print()
    
    # Grid size analysis
    sizes = stats['grid_sizes']
    heights = [s[0] for s in sizes]
    widths = [s[1] for s in sizes]
    
    print("ğŸ“ ØªØ­Ù„ÙŠÙ„ Ø£Ø­Ø¬Ø§Ù… Ø§Ù„Ø´Ø¨ÙƒØ§Øª:")
    print(f"  - Ù…ØªÙˆØ³Ø· Ø§Ù„Ø§Ø±ØªÙØ§Ø¹: {np.mean(heights):.1f}")
    print(f"  - Ù…ØªÙˆØ³Ø· Ø§Ù„Ø¹Ø±Ø¶: {np.mean(widths):.1f}")
    print(f"  - Ø£ÙƒØ¨Ø± Ø´Ø¨ÙƒØ©: {max(heights)}Ã—{max(widths)}")
    print(f"  - Ø£ØµØºØ± Ø´Ø¨ÙƒØ©: {min(heights)}Ã—{min(widths)}")
    print()
    
    # Color analysis
    most_common_colors = stats['color_usage'].most_common(5)
    print("ğŸ¨ Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ø§Ù„Ø£ÙƒØ«Ø± Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Ù‹:")
    for color, count in most_common_colors:
        print(f"  - Ø§Ù„Ù„ÙˆÙ† {color}: {count} Ù…Ø±Ø©")
    print()
    
    # Transformation types
    print("ğŸ”„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ØªØ­ÙˆÙŠÙ„Ø§Øª:")
    for trans_type, count in stats['transformation_types'].items():
        percentage = (count / stats['total_problems']) * 100
        print(f"  - {trans_type}: {count} Ù…Ù‡Ù…Ø© ({percentage:.1f}%)")
    print()
    
    # Scaling patterns
    if stats['scaling_patterns']:
        print("ğŸ“ˆ Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªØ­Ø¬ÙŠÙ…:")
        for pattern, count in stats['scaling_patterns'].items():
            print(f"  - {pattern}: {count} Ù…Ø±Ø©")
        print()
    
    # Symmetry analysis
    if stats['symmetry_patterns']:
        print("ğŸ”„ Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªÙ…Ø§Ø«Ù„:")
        for pattern, count in stats['symmetry_patterns'].items():
            print(f"  - {pattern}: {count} Ø´Ø¨ÙƒØ©")
    
    print("\n" + "="*80)


def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    
    print("ğŸ” Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ù…Ù‡Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨...")
    
    try:
        # Load training data
        challenges = load_training_data()
        
        # Generate statistics
        print("ğŸ“Š Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª...")
        stats = generate_statistics(challenges)
        
        # Print summary
        print_summary(stats)
        
        # Save statistics
        with open('simple_training_patterns_stats.json', 'w', encoding='utf-8') as f:
            # Convert Counter objects and numpy types to regular dicts for JSON serialization
            json_stats = stats.copy()
            json_stats['color_usage'] = {str(k): int(v) for k, v in stats['color_usage'].items()}
            json_stats['symmetry_patterns'] = {str(k): int(v) for k, v in stats['symmetry_patterns'].items()}
            json_stats['transformation_types'] = {str(k): int(v) for k, v in stats['transformation_types'].items()}
            json_stats['scaling_patterns'] = {str(k): int(v) for k, v in stats['scaling_patterns'].items()}

            # Convert grid sizes to regular lists
            json_stats['grid_sizes'] = [(int(h), int(w)) for h, w in stats['grid_sizes']]

            json.dump(json_stats, f, indent=2, ensure_ascii=False)
        
        print("ğŸ’¾ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù…Ø­ÙÙˆØ¸Ø© ÙÙŠ: simple_training_patterns_stats.json")
        print("âœ… Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­!")
        
    except Exception as e:
        logging.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {e}")
        raise


if __name__ == "__main__":
    main()
