from __future__ import annotations
"""
COMPLETE TEST - ALL 1000 TASKS WITH REAL ACCURACY
==================================================
Ø§Ø®ØªØ¨Ø§Ø± Ø´Ø§Ù…Ù„ Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù€ 1000 Ù…Ù‡Ù…Ø©
"""

import json
import numpy as np
import time
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

import logging
logging.basicConfig(level=logging.ERROR)
for logger in logging.root.manager.loggerDict:
    logging.getLogger(logger).setLevel(logging.ERROR)

def calculate_similarity(pred, actual):
    """Ø­Ø³Ø§Ø¨ Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ·Ø§Ø¨Ù‚"""
    try:
        if pred is None or actual is None:
            return 0.0
        
        pred_arr = np.array(pred)
        actual_arr = np.array(actual)
        
        if pred_arr.shape != actual_arr.shape:
            return 0.0
        
        matches = np.sum(pred_arr == actual_arr)
        total = actual_arr.size
        
        return (matches / total) * 100
    except:
        return 0.0

def test_all_1000_tasks():
    print("="*80)
    print("ğŸ¯ TESTING ALL 1000 ARC TASKS - REAL ACCURACY")
    print("="*80)
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù‡Ø§Ù… ÙˆØ§Ù„Ø­Ù„ÙˆÙ„
    print("\nLoading tasks and solutions...")
    try:
        with open('arc-agi_training_challenges.json', 'r') as f:
            challenges = json.load(f)
        with open('arc-agi_training_solutions.json', 'r') as f:
            solutions = json.load(f)
        print(f"âœ“ Loaded {len(challenges)} tasks with solutions")
    except Exception as e:
        print(f"âŒ Error: {e}")
        return
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£Ù†Ø¸Ù…Ø©
    print("\nLoading systems...")
    systems = {}
    
    try:
        from perfect_arc_system_v2 import PerfectARCSystem
        systems['Perfect_V2'] = PerfectARCSystem()
        print("âœ“ Perfect V2 loaded")
    except:
        pass
    
    try:
        from ultimate_arc_solver import UltimateARCSolver
        systems['Ultimate'] = UltimateARCSolver()
        print("âœ“ Ultimate loaded")
    except:
        pass
    
    try:
        from interactive_arc_system_v2 import InteractiveARCSystem
        systems['Interactive_V2'] = InteractiveARCSystem()
        print("âœ“ Interactive V2 loaded")
    except:
        pass
    
    # Ultra V2 ÙŠØ­ØªØ§Ø¬ Ø¥ØµÙ„Ø§Ø­ØŒ Ù†ØªØ®Ø·Ø§Ù‡ Ø§Ù„Ø¢Ù†
    # try:
    #     from ultra_advanced_arc_system_v2 import UltraAdvancedARCSystem
    #     systems['Ultra_V2'] = UltraAdvancedARCSystem()
    #     print("âœ“ Ultra V2 loaded")
    # except:
    #     pass
    
    print(f"\nTotal systems loaded: {len(systems)}")
    
    if not systems:
        print("âŒ No systems loaded!")
        return
    
    # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù„ÙƒÙ„ Ù†Ø¸Ø§Ù…
    results = {name: {
        'perfect': 0,
        'high_partial': 0,  # >80%
        'medium_partial': 0,  # 50-80%
        'low_partial': 0,  # <50%
        'wrong': 0,
        'errors': 0,
        'total_similarity': 0,
        'task_times': [],
        'similarities': []
    } for name in systems.keys()}
    
    print("\n" + "="*80)
    print("TESTING IN PROGRESS...")
    print("="*80)
    
    total_tasks = len(challenges)
    start_time = time.time()
    
    for idx, (task_id, task) in enumerate(challenges.items()):
        # ØªÙ‚Ø¯Ù… ÙƒÙ„ 100 Ù…Ù‡Ù…Ø©
        if idx % 100 == 0:
            elapsed = time.time() - start_time
            print(f"\nProgress: {idx}/{total_tasks} tasks ({idx/total_tasks*100:.1f}%)")
            if idx > 0:
                eta = (elapsed / idx) * (total_tasks - idx)
                print(f"Time elapsed: {elapsed:.1f}s | ETA: {eta:.1f}s")
        
        # Ø§Ù„Ø­Ù„ Ø§Ù„ØµØ­ÙŠØ­
        correct_solution = solutions[task_id][0]  # Ø£ÙˆÙ„ Ø­Ù„
        
        # Ø§Ø®ØªØ¨Ø§Ø± ÙƒÙ„ Ù†Ø¸Ø§Ù…
        for system_name, system in systems.items():
            try:
                task_start = time.time()
                
                # Ø­Ù„ Ø§Ù„Ù…Ù‡Ù…Ø©
                if hasattr(system, 'solve'):
                    result = system.solve(task)
                elif hasattr(system, 'process_task'):
                    result = system.process_task(task)
                else:
                    result = None
                
                task_time = time.time() - task_start
                results[system_name]['task_times'].append(task_time)
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø®Ø±Ø¬
                if result is not None:
                    if isinstance(result, dict) and 'output' in result:
                        predicted = result['output']
                    else:
                        predicted = result
                    
                    # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
                    similarity = calculate_similarity(predicted, correct_solution)
                    results[system_name]['similarities'].append(similarity)
                    results[system_name]['total_similarity'] += similarity
                    
                    # ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØªÙŠØ¬Ø©
                    if similarity == 100:
                        results[system_name]['perfect'] += 1
                    elif similarity >= 80:
                        results[system_name]['high_partial'] += 1
                    elif similarity >= 50:
                        results[system_name]['medium_partial'] += 1
                    elif similarity > 0:
                        results[system_name]['low_partial'] += 1
                    else:
                        results[system_name]['wrong'] += 1
                else:
                    results[system_name]['wrong'] += 1
                    results[system_name]['similarities'].append(0)
                    
            except Exception as e:
                results[system_name]['errors'] += 1
                results[system_name]['similarities'].append(0)
    
    total_time = time.time() - start_time
    
    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    print("\n" + "="*80)
    print("ğŸ“Š FINAL RESULTS - 1000 TASKS")
    print("="*80)
    print(f"\nâ±ï¸ Total test time: {total_time:.1f} seconds")
    
    # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£Ù†Ø¸Ù…Ø©
    system_scores = []
    for system_name, stats in results.items():
        avg_similarity = stats['total_similarity'] / total_tasks
        avg_time = np.mean(stats['task_times']) if stats['task_times'] else 0
        
        # Ù†Ø³Ø¨ Ø§Ù„Ù†Ø¬Ø§Ø­
        perfect_rate = (stats['perfect'] / total_tasks) * 100
        partial_rate = ((stats['high_partial'] + stats['medium_partial'] + stats['low_partial']) / total_tasks) * 100
        failure_rate = ((stats['wrong'] + stats['errors']) / total_tasks) * 100
        
        system_scores.append({
            'name': system_name,
            'avg_similarity': avg_similarity,
            'perfect': stats['perfect'],
            'perfect_rate': perfect_rate,
            'high_partial': stats['high_partial'],
            'medium_partial': stats['medium_partial'],
            'low_partial': stats['low_partial'],
            'partial_rate': partial_rate,
            'wrong': stats['wrong'],
            'errors': stats['errors'],
            'failure_rate': failure_rate,
            'avg_time': avg_time
        })
    
    # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ù…ØªÙˆØ³Ø· Ø§Ù„ØªØ´Ø§Ø¨Ù‡
    system_scores.sort(key=lambda x: x['avg_similarity'], reverse=True)
    
    print("\nğŸ† SYSTEM RANKING BY REAL ACCURACY:")
    print("="*80)
    
    for rank, score in enumerate(system_scores, 1):
        print(f"\n{rank}. {score['name']}")
        print(f"   ğŸ“Š Average Similarity: {score['avg_similarity']:.1f}%")
        print(f"   âœ… Perfect Solutions: {score['perfect']}/1000 ({score['perfect_rate']:.1f}%)")
        print(f"   âš ï¸  Partial Solutions:")
        print(f"      â€¢ High (>80%): {score['high_partial']} tasks")
        print(f"      â€¢ Medium (50-80%): {score['medium_partial']} tasks")
        print(f"      â€¢ Low (<50%): {score['low_partial']} tasks")
        print(f"      â€¢ Total Partial: {score['partial_rate']:.1f}%")
        print(f"   âŒ Failed: {score['wrong']} tasks")
        if score['errors'] > 0:
            print(f"   ğŸ’¥ Errors: {score['errors']} tasks")
        print(f"   â±ï¸  Avg Time: {score['avg_time']*1000:.1f}ms per task")
    
    # Ù…Ù„Ø®Øµ Ø¹Ø§Ù…
    print("\n" + "="*80)
    print("ğŸ’¡ OVERALL SUMMARY")
    print("="*80)
    
    if system_scores:
        best = system_scores[0]
        print(f"\nğŸ¥‡ BEST SYSTEM: {best['name']}")
        print(f"   â€¢ Average Accuracy: {best['avg_similarity']:.1f}%")
        print(f"   â€¢ Perfect Solutions: {best['perfect']}/1000")
        print(f"   â€¢ Tasks with >50% accuracy: {best['perfect'] + best['high_partial'] + best['medium_partial']}/1000")
        
        # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡
        if best['avg_similarity'] >= 70:
            print("\nâœ¨ EXCELLENT! Systems show strong understanding of patterns")
        elif best['avg_similarity'] >= 50:
            print("\nğŸ“ˆ GOOD! Systems understand many patterns but need refinement")
        elif best['avg_similarity'] >= 30:
            print("\nâš ï¸ MODERATE! Systems capture some patterns but need significant improvement")
        else:
            print("\nâŒ POOR! Systems need major redesign")
    
    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    with open('final_1000_tasks_results.json', 'w') as f:
        json.dump({
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'total_tasks': total_tasks,
            'test_duration': total_time,
            'systems': system_scores
        }, f, indent=2)
    
    print("\nğŸ“ Results saved to final_1000_tasks_results.json")
    print("="*80)
    
    return results

if __name__ == "__main__":
    results = test_all_1000_tasks()
