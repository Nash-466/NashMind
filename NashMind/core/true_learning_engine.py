#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ - Ù…ÙƒÙˆÙ† Ø¬Ø¯ÙŠØ¯ ÙÙŠ NashMind
ÙŠØ¶ÙŠÙ Ù‚Ø¯Ø±Ø§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ ÙˆØ§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø¯Ø§Ø¦Ù…Ø© Ù„Ù„Ù†Ø¸Ø§Ù…
"""

import json
import os
import time
import hashlib
import numpy as np
from collections import defaultdict
import pickle
import re

def convert_numpy_to_python(obj):
    """ØªØ­ÙˆÙŠÙ„ NumPy arrays Ùˆ int64 Ø¥Ù„Ù‰ Ø£Ù†ÙˆØ§Ø¹ Python Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©"""
    if isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, dict):
        return {str(k): convert_numpy_to_python(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_to_python(item) for item in obj]
    else:
        return obj

class TrueLearningEngine:
    """
    Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ - ÙŠØªØ¹Ù„Ù… Ù…Ù† ÙƒÙ„ ØªØ¬Ø±Ø¨Ø© ÙˆÙŠØ­ÙØ¸Ù‡Ø§ Ø¯Ø§Ø¦Ù…Ø§Ù‹
    Ù…Ø¯Ù…Ø¬ ÙÙŠ Ù†Ø¸Ø§Ù… NashMind ACES
    """
    
    def __init__(self, memory_file="nashmind_memory.json", patterns_file="nashmind_patterns.pkl"):
        self.memory_file = memory_file
        self.patterns_file = patterns_file
        
        # Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø¯Ø§Ø¦Ù…Ø© - ØªØ­ÙØ¸ ÙƒÙ„ Ù…Ø§ ÙŠØªØ¹Ù„Ù…Ù‡ NashMind
        self.long_term_memory = self.load_memory()
        
        # Ø£Ù†Ù…Ø§Ø· Ù…ØªØ¹Ù„Ù…Ø© - ÙŠÙƒØªØ´ÙÙ‡Ø§ Ø¨Ù†ÙØ³Ù‡
        self.learned_patterns = self.load_patterns()
        
        # Ø´Ø¨ÙƒØ© Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… - ÙŠØ¨Ù†ÙŠÙ‡Ø§ ØªØ¯Ø±ÙŠØ¬ÙŠØ§Ù‹
        self.concept_network = defaultdict(list)
        
        # Ø®Ø¨Ø±Ø§Øª Ø³Ø§Ø¨Ù‚Ø© - ÙŠØªØ¹Ù„Ù… Ù…Ù†Ù‡Ø§
        self.experiences = []
        
        # Ù‚Ø¯Ø±Ø§Øª Ø§Ù„ØªØ¹Ù„Ù… - ØªÙ†Ù…Ùˆ Ù…Ø¹ Ø§Ù„ÙˆÙ‚Øª
        self.learning_capabilities = {
            "pattern_recognition": 0.1,
            "logical_reasoning": 0.1,
            "creative_thinking": 0.1,
            "problem_solving": 0.1,
            "learning_efficiency": 0.1,
            "memory_consolidation": 0.1,
            "concept_formation": 0.1
        }
        
        print("ğŸ§  ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ ÙÙŠ NashMind")
        print(f"ğŸ“š Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø·ÙˆÙŠÙ„Ø© Ø§Ù„Ù…Ø¯Ù‰: {len(self.long_term_memory)} Ø¹Ù†ØµØ±")
        print(f"ğŸ” Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ØªØ¹Ù„Ù…Ø©: {len(self.learned_patterns)} Ù†Ù…Ø·")

    def load_memory(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø·ÙˆÙŠÙ„Ø© Ø§Ù„Ù…Ø¯Ù‰ Ù…Ù† Ø§Ù„Ù…Ù„Ù"""
        if os.path.exists(self.memory_file):
            try:
                with open(self.memory_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return {}
        return {}

    def save_memory(self):
        """Ø­ÙØ¸ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø·ÙˆÙŠÙ„Ø© Ø§Ù„Ù…Ø¯Ù‰"""
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ø¨Ù„ Ø§Ù„Ø­ÙØ¸
        converted_memory = convert_numpy_to_python(self.long_term_memory)
        with open(self.memory_file, 'w', encoding='utf-8') as f:
            json.dump(converted_memory, f, ensure_ascii=False, indent=2)

    def load_patterns(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ØªØ¹Ù„Ù…Ø©"""
        if os.path.exists(self.patterns_file):
            try:
                with open(self.patterns_file, 'rb') as f:
                    return pickle.load(f)
            except:
                return {}
        return {}

    def save_patterns(self):
        """Ø­ÙØ¸ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ØªØ¹Ù„Ù…Ø©"""
        with open(self.patterns_file, 'wb') as f:
            pickle.dump(self.learned_patterns, f)

    def learn_from_experience(self, experience_data, context="general"):
        """Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† ØªØ¬Ø±Ø¨Ø© Ø¬Ø¯ÙŠØ¯Ø© - Ø§Ù„Ù‚Ù„Ø¨ Ø§Ù„Ù†Ø§Ø¨Ø¶ Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ"""
        
        print(f"ğŸ” ØªØ¹Ù„Ù… Ù…Ù† ØªØ¬Ø±Ø¨Ø© Ø¬Ø¯ÙŠØ¯Ø©: {str(experience_data)[:50]}...")
        
        # 1. ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
        analysis = self.analyze_experience(experience_data)
        
        # 2. Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¹Ù† ØªØ¬Ø§Ø±Ø¨ Ù…Ø´Ø§Ø¨Ù‡Ø©
        similar_experiences = self.find_similar_experiences(analysis)
        
        # 3. Ø§ÙƒØªØ´Ø§Ù Ø£Ù†Ù…Ø§Ø· Ø¬Ø¯ÙŠØ¯Ø©
        new_patterns = self.discover_patterns(experience_data, similar_experiences)
        
        # 4. Ø¨Ù†Ø§Ø¡ Ø±ÙˆØ§Ø¨Ø· Ù…ÙØ§Ù‡ÙŠÙ…ÙŠØ©
        self.build_concept_connections(analysis, similar_experiences)
        
        # 5. ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù‚Ø¯Ø±Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ù„Ù…
        self.update_learning_capabilities(new_patterns)
        
        # 6. Ø­ÙØ¸ Ø§Ù„ØªØ¬Ø±Ø¨Ø© ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø·ÙˆÙŠÙ„Ø© Ø§Ù„Ù…Ø¯Ù‰
        memory_id = self.store_experience_in_memory(experience_data, analysis, context)
        
        # 7. Ø­ÙØ¸ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª
        self.save_memory()
        self.save_patterns()
        
        return {
            "memory_id": memory_id,
            "patterns_discovered": len(new_patterns),
            "concepts_connected": len(self.concept_network),
            "learning_growth": self.calculate_learning_growth()
        }

    def analyze_experience(self, experience_data):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©"""
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
        if isinstance(experience_data, str):
            text = experience_data
        elif isinstance(experience_data, dict):
            text = str(experience_data)
        else:
            text = str(experience_data)
        
        keywords = re.findall(r'\b\w+\b', text.lower())
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
        complexity = len(keywords) / 10
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†ÙˆØ¹
        experience_type = self.classify_experience_type(text)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…
        concepts = self.extract_concepts(keywords)
        
        return {
            "keywords": keywords,
            "complexity": complexity,
            "type": experience_type,
            "concepts": concepts,
            "timestamp": time.time(),
            "hash": hashlib.md5(text.encode()).hexdigest()
        }

    def classify_experience_type(self, text):
        """ØªØµÙ†ÙŠÙ Ù†ÙˆØ¹ Ø§Ù„ØªØ¬Ø±Ø¨Ø©"""
        
        if any(word in text.lower() for word in ['Ù…Ø´ÙƒÙ„Ø©', 'Ø­Ù„', 'problem', 'solve']):
            return "problem_solving"
        elif any(word in text.lower() for word in ['ØªØ¹Ù„Ù…', 'Ù…Ø¹Ø±ÙØ©', 'learn', 'knowledge']):
            return "learning"
        elif any(word in text.lower() for word in ['Ø¥Ø¨Ø¯Ø§Ø¹', 'ÙÙƒØ±Ø©', 'creative', 'idea']):
            return "creative"
        elif any(word in text.lower() for word in ['ØªØ­Ù„ÙŠÙ„', 'ÙÙ‡Ù…', 'analyze', 'understand']):
            return "analytical"
        else:
            return "general"

    def extract_concepts(self, keywords):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©"""
        
        # Ù…ÙØ§Ù‡ÙŠÙ… Ø£Ø³Ø§Ø³ÙŠØ©
        concept_categories = {
            "mathematical": ["Ø±Ù‚Ù…", "Ø­Ø³Ø§Ø¨", "Ù…Ø¹Ø§Ø¯Ù„Ø©", "number", "math", "equation"],
            "logical": ["Ù…Ù†Ø·Ù‚", "Ø§Ø³ØªÙ†ØªØ§Ø¬", "logic", "reasoning", "inference"],
            "spatial": ["Ù…ÙƒØ§Ù†", "Ø´ÙƒÙ„", "Ù…ÙˆÙ‚Ø¹", "space", "shape", "position"],
            "temporal": ["ÙˆÙ‚Øª", "Ø²Ù…Ù†", "time", "temporal", "sequence"],
            "causal": ["Ø³Ø¨Ø¨", "Ù†ØªÙŠØ¬Ø©", "cause", "effect", "result"]
        }
        
        concepts = []
        for category, category_words in concept_categories.items():
            if any(word in keywords for word in category_words):
                concepts.append(category)
        
        return concepts

    def find_similar_experiences(self, analysis):
        """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ØªØ¬Ø§Ø±Ø¨ Ù…Ø´Ø§Ø¨Ù‡Ø© ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
        
        similar = []
        current_keywords = set(analysis["keywords"])
        
        for memory_id, memory_data in self.long_term_memory.items():
            if "analysis" in memory_data:
                memory_keywords = set(memory_data["analysis"].get("keywords", []))
                
                # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
                intersection = len(current_keywords.intersection(memory_keywords))
                union = len(current_keywords.union(memory_keywords))
                
                if union > 0:
                    similarity = intersection / union
                    if similarity > 0.3:  # Ø¹ØªØ¨Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡
                        similar.append({
                            "memory_id": memory_id,
                            "similarity": similarity,
                            "data": memory_data
                        })
        
        return sorted(similar, key=lambda x: x["similarity"], reverse=True)[:5]

    def discover_patterns(self, experience_data, similar_experiences):
        """Ø§ÙƒØªØ´Ø§Ù Ø£Ù†Ù…Ø§Ø· Ø¬Ø¯ÙŠØ¯Ø©"""
        
        new_patterns = []
        
        # Ù†Ù…Ø· Ø§Ù„ØªÙƒØ±Ø§Ø±
        if len(similar_experiences) >= 2:
            pattern_id = f"repetition_{len(self.learned_patterns)}"
            pattern = {
                "type": "repetition",
                "description": "Ù†Ù…Ø· ØªÙƒØ±Ø§Ø± ÙÙŠ Ø§Ù„ØªØ¬Ø§Ø±Ø¨ Ø§Ù„Ù…Ø´Ø§Ø¨Ù‡Ø©",
                "frequency": len(similar_experiences),
                "confidence": min(0.9, len(similar_experiences) * 0.2)
            }
            self.learned_patterns[pattern_id] = pattern
            new_patterns.append(pattern_id)
        
        # Ù†Ù…Ø· Ø§Ù„ØªØ·ÙˆØ±
        if similar_experiences:
            latest_similar = max(similar_experiences, 
                               key=lambda x: x["data"].get("analysis", {}).get("timestamp", 0))
            
            if latest_similar["data"].get("analysis", {}).get("complexity", 0) < \
               self.analyze_experience(experience_data)["complexity"]:
                pattern_id = f"evolution_{len(self.learned_patterns)}"
                pattern = {
                    "type": "evolution",
                    "description": "Ù†Ù…Ø· ØªØ·ÙˆØ± ÙÙŠ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯",
                    "growth_rate": 0.1,
                    "confidence": 0.7
                }
                self.learned_patterns[pattern_id] = pattern
                new_patterns.append(pattern_id)
        
        return new_patterns

    def build_concept_connections(self, analysis, similar_experiences):
        """Ø¨Ù†Ø§Ø¡ Ø±ÙˆØ§Ø¨Ø· Ù…ÙØ§Ù‡ÙŠÙ…ÙŠØ©"""
        
        current_concepts = analysis["concepts"]
        
        for concept in current_concepts:
            # Ø±Ø¨Ø· Ø¨Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… ÙÙŠ Ø§Ù„ØªØ¬Ø§Ø±Ø¨ Ø§Ù„Ù…Ø´Ø§Ø¨Ù‡Ø©
            for similar in similar_experiences:
                similar_concepts = similar["data"].get("analysis", {}).get("concepts", [])
                for similar_concept in similar_concepts:
                    if similar_concept not in self.concept_network[concept]:
                        self.concept_network[concept].append(similar_concept)
                        print(f"ğŸ”— Ø±Ø¨Ø· Ù…ÙØ§Ù‡ÙŠÙ…ÙŠ Ø¬Ø¯ÙŠØ¯: {concept} â†” {similar_concept}")

    def update_learning_capabilities(self, new_patterns):
        """ØªØ­Ø¯ÙŠØ« Ù‚Ø¯Ø±Ø§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©"""
        
        growth_factor = len(new_patterns) * 0.01
        
        for capability in self.learning_capabilities:
            self.learning_capabilities[capability] += growth_factor
            # Ø­Ø¯ Ø£Ù‚ØµÙ‰ Ù„Ù„Ù†Ù…Ùˆ
            self.learning_capabilities[capability] = min(1.0, self.learning_capabilities[capability])

    def store_experience_in_memory(self, experience_data, analysis, context):
        """Ø­ÙØ¸ Ø§Ù„ØªØ¬Ø±Ø¨Ø© ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø·ÙˆÙŠÙ„Ø© Ø§Ù„Ù…Ø¯Ù‰"""
        
        memory_id = f"EXP_{len(self.long_term_memory)}_{analysis['hash'][:8]}"
        
        memory_entry = {
            "experience_data": experience_data,
            "analysis": analysis,
            "context": context,
            "stored_at": time.time(),
            "access_count": 0,
            "importance": self.calculate_importance(analysis)
        }
        
        self.long_term_memory[memory_id] = memory_entry
        self.experiences.append(memory_id)
        
        print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªØ¬Ø±Ø¨Ø©: {memory_id}")
        
        return memory_id

    def calculate_importance(self, analysis):
        """Ø­Ø³Ø§Ø¨ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„ØªØ¬Ø±Ø¨Ø©"""
        
        importance = 0.5  # Ø£Ù‡Ù…ÙŠØ© Ø£Ø³Ø§Ø³ÙŠØ©
        
        # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø£Ù‡Ù…ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
        importance += analysis["complexity"] * 0.1
        
        # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø£Ù‡Ù…ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…
        importance += len(analysis["concepts"]) * 0.05
        
        # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø£Ù‡Ù…ÙŠØ© Ù„Ù„Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ù‡Ù…Ø©
        if analysis["type"] in ["problem_solving", "creative"]:
            importance += 0.2
        
        return min(1.0, importance)

    def calculate_learning_growth(self):
        """Ø­Ø³Ø§Ø¨ Ù†Ù…Ùˆ Ø§Ù„ØªØ¹Ù„Ù…"""
        
        total_capability = sum(self.learning_capabilities.values())
        return total_capability / len(self.learning_capabilities)

    def get_learning_stats(self):
        """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù…"""
        
        return {
            "total_experiences": len(self.experiences),
            "total_patterns": len(self.learned_patterns),
            "concept_connections": sum(len(connections) for connections in self.concept_network.values()),
            "learning_capabilities": self.learning_capabilities,
            "overall_learning_level": self.calculate_learning_growth()
        }

    def recall_similar_experiences(self, query):
        """Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ ØªØ¬Ø§Ø±Ø¨ Ù…Ø´Ø§Ø¨Ù‡Ø© Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…"""
        
        query_analysis = self.analyze_experience(query)
        similar = self.find_similar_experiences(query_analysis)
        
        return [exp["data"] for exp in similar]

    def apply_learned_patterns(self, new_situation):
        """ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ØªØ¹Ù„Ù…Ø© Ø¹Ù„Ù‰ Ù…ÙˆÙ‚Ù Ø¬Ø¯ÙŠØ¯"""
        
        applicable_patterns = []
        
        for pattern_id, pattern in self.learned_patterns.items():
            if pattern["confidence"] > 0.5:
                applicable_patterns.append({
                    "pattern_id": pattern_id,
                    "pattern": pattern,
                    "applicability": pattern["confidence"]
                })
        
        return sorted(applicable_patterns, key=lambda x: x["applicability"], reverse=True)
