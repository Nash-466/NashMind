from __future__ import annotations
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¯ Ù†Ø¸Ø§Ù… ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¯Ù‚Ø© - ÙŠØ±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù‚Ø±ÙŠØ¨Ø© Ù…Ù† Ø§Ù„Ù†Ø¬Ø§Ø­
"""

import json
import numpy as np
import time
from collections.abc import Callable
from typing import Dict, List, Any

class PrecisionImprovementSystem:
    """Ù†Ø¸Ø§Ù… ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¯Ù‚Ø©"""
    
    def __init__(self):
        self.high_similarity_tasks = []  # Ø§Ù„Ù…Ù‡Ø§Ù… Ø°Ø§Øª Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¹Ø§Ù„ÙŠ
        self.solved_tasks = set()
        self.iteration = 0
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        self.challenges, self.solutions = self._load_data()
        
        print("ğŸ¯ Ù†Ø¸Ø§Ù… ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¯Ù‚Ø© Ø¬Ø§Ù‡Ø²")
        print(f"ğŸ“Š {len(self.challenges)} Ù…Ù‡Ù…Ø© Ù…ØªØ§Ø­Ø©")
    
    def _load_data(self):
        """ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
        try:
            with open('arc-agi_training_challenges.json', 'r') as f:
                challenges = json.load(f)
            with open('arc-agi_training_solutions.json', 'r') as f:
                solutions = json.load(f)
            return challenges, solutions
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            return {}, {}
    
    def start_precision_improvement(self, target_tasks: int = 100):
        """Ø¨Ø¯Ø¡ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¯Ù‚Ø©"""
        
        print("ğŸš€ Ø¨Ø¯Ø¡ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¯Ù‚Ø©...")
        print("="*50)
        
        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù‚Ø±ÙŠØ¨Ø© Ù…Ù† Ø§Ù„Ù†Ø¬Ø§Ø­
        print("\nğŸ“Š Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù‚Ø±ÙŠØ¨Ø© Ù…Ù† Ø§Ù„Ù†Ø¬Ø§Ø­")
        self._discover_high_similarity_tasks(target_tasks)
        
        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ„ Ù„Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù‚Ø±ÙŠØ¨Ø©
        print("\nğŸ” Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ„ Ù„Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù‚Ø±ÙŠØ¨Ø©")
        detailed_analysis = self._detailed_analysis()
        
        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: ØªØ·ÙˆÙŠØ± ØªØ­Ø³ÙŠÙ†Ø§Øª Ù…Ø³ØªÙ‡Ø¯ÙØ©
        print("\nğŸ”§ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: ØªØ·ÙˆÙŠØ± ØªØ­Ø³ÙŠÙ†Ø§Øª Ù…Ø³ØªÙ‡Ø¯ÙØ©")
        improvements = self._develop_targeted_improvements(detailed_analysis)
        
        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 4: ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª ÙˆØ§Ø®ØªØ¨Ø§Ø±
        print("\nğŸ¯ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 4: ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª ÙˆØ§Ø®ØªØ¨Ø§Ø±")
        final_results = self._apply_and_test_improvements(improvements)
        
        return final_results
    
    def _discover_high_similarity_tasks(self, target_tasks: int):
        """Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ù…Ù‡Ø§Ù… Ø°Ø§Øª Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¹Ø§Ù„ÙŠ"""
        
        print(f"ğŸ” ÙØ­Øµ {target_tasks} Ù…Ù‡Ù…Ø© Ù„Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù‚Ø±ÙŠØ¨Ø©...")
        
        task_ids = list(self.challenges.keys())[:target_tasks]
        high_similarity_threshold = 0.75  # 75% ØªØ´Ø§Ø¨Ù‡ Ø£Ùˆ Ø£ÙƒØ«Ø±
        
        for i, task_id in enumerate(task_ids):
            try:
                result = self._solve_task(task_id)
                
                if result['similarity'] >= high_similarity_threshold:
                    self.high_similarity_tasks.append(result)
                    status = f"ğŸ¯ {result['similarity']:.2f}"
                else:
                    status = f"ğŸ“Š {result['similarity']:.2f}"
                
                print(f"   {i+1:3d}. {task_id[:8]}: {status}")
                
                # Ø·Ø¨Ø§Ø¹Ø© ØªÙ‚Ø¯Ù… ÙƒÙ„ 20 Ù…Ù‡Ù…Ø©
                if (i + 1) % 20 == 0:
                    current_high = len(self.high_similarity_tasks)
                    print(f"       ğŸ“ˆ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù‚Ø±ÙŠØ¨Ø© Ø­ØªÙ‰ Ø§Ù„Ø¢Ù†: {current_high}")
                
            except Exception as e:
                print(f"   {i+1:3d}. {task_id[:8]}: âŒ {str(e)[:20]}...")
        
        print(f"\nğŸ¯ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(self.high_similarity_tasks)} Ù…Ù‡Ù…Ø© Ù‚Ø±ÙŠØ¨Ø© Ù…Ù† Ø§Ù„Ù†Ø¬Ø§Ø­")
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
        self.high_similarity_tasks.sort(key=lambda x: x['similarity'], reverse=True)
        
        # Ø·Ø¨Ø§Ø¹Ø© Ø£ÙØ¶Ù„ 10 Ù…Ù‡Ø§Ù…
        print("\nğŸ† Ø£ÙØ¶Ù„ 10 Ù…Ù‡Ø§Ù… Ù‚Ø±ÙŠØ¨Ø© Ù…Ù† Ø§Ù„Ù†Ø¬Ø§Ø­:")
        for i, task in enumerate(self.high_similarity_tasks[:10]):
            print(f"   {i+1:2d}. {task['task_id'][:8]}: {task['similarity']:.3f}")
    
    def _solve_task(self, task_id: str):
        """Ø­Ù„ Ù…Ù‡Ù…Ø© ÙˆØ§Ø­Ø¯Ø©"""
        
        challenge = self.challenges[task_id]
        solution = self.solutions[task_id]
        
        test_case = challenge['test'][0]
        input_grid = np.array(test_case['input'])
        expected_output = np.array(solution[0])
        
        # Ø­Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… EfficientZero
        from efficient_zero_engine import EfficientZeroEngine
        ez = EfficientZeroEngine()
        
        result = ez.solve_arc_problem(input_grid, max_steps=7)
        
        if result.get('success', True):
            output_grid = np.array(result.get('solution_grid', input_grid))
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
            if output_grid.shape == expected_output.shape:
                similarity = np.sum(output_grid == expected_output) / output_grid.size
            else:
                similarity = 0.0
            
            return {
                'task_id': task_id,
                'input_grid': input_grid,
                'expected_output': expected_output,
                'actual_output': output_grid,
                'similarity': similarity,
                'confidence': result.get('confidence', 0),
                'solved_correctly': similarity >= 0.99
            }
        else:
            raise Exception(result.get('error', 'ÙØ´Ù„ ÙÙŠ Ø§Ù„Ø­Ù„'))
    
    def _detailed_analysis(self):
        """ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ„ Ù„Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù‚Ø±ÙŠØ¨Ø©"""
        
        print(f"ğŸ”¬ ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ„ Ù„Ù€ {len(self.high_similarity_tasks)} Ù…Ù‡Ù…Ø©...")
        
        analysis_results = []
        
        for i, task_result in enumerate(self.high_similarity_tasks[:20]):  # Ø£ÙØ¶Ù„ 20 Ù…Ù‡Ù…Ø©
            try:
                analysis = self._analyze_single_task(task_result)
                analysis_results.append(analysis)
                
                print(f"   {i+1:2d}. {task_result['task_id'][:8]}: {analysis['error_type']}")
                
            except Exception as e:
                print(f"   {i+1:2d}. {task_result['task_id'][:8]}: âŒ ØªØ­Ù„ÙŠÙ„ ÙØ§Ø´Ù„")
        
        # ØªØ¬Ù…ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
        error_types = {}
        for analysis in analysis_results:
            error_type = analysis['error_type']
            error_types[error_type] = error_types.get(error_type, 0) + 1
        
        print(f"\nğŸ“Š Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©:")
        for error_type, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True):
            print(f"   {error_type}: {count} Ù…Ù‡Ù…Ø©")
        
        return analysis_results
    
    def _analyze_single_task(self, task_result):
        """ØªØ­Ù„ÙŠÙ„ Ù…Ù‡Ù…Ø© ÙˆØ§Ø­Ø¯Ø© Ø¨Ø§Ù„ØªÙØµÙŠÙ„"""
        
        input_grid = task_result['input_grid']
        expected = task_result['expected_output']
        actual = task_result['actual_output']
        
        analysis = {
            'task_id': task_result['task_id'],
            'similarity': task_result['similarity']
        }
        
        # ØªØ­Ù„ÙŠÙ„ Ù†ÙˆØ¹ Ø§Ù„Ø®Ø·Ø£
        if actual.shape != expected.shape:
            analysis['error_type'] = 'size_mismatch'
            analysis['expected_shape'] = expected.shape
            analysis['actual_shape'] = actual.shape
        else:
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø®ØªÙ„Ø§ÙØ§Øª
            diff_mask = (actual != expected)
            diff_count = np.sum(diff_mask)
            total_pixels = actual.size
            
            if diff_count <= 3:
                analysis['error_type'] = 'few_pixel_errors'
                analysis['error_pixels'] = diff_count
            elif diff_count <= total_pixels * 0.1:
                analysis['error_type'] = 'minor_pattern_error'
                analysis['error_percentage'] = (diff_count / total_pixels) * 100
            elif diff_count <= total_pixels * 0.3:
                analysis['error_type'] = 'partial_pattern_error'
                analysis['error_percentage'] = (diff_count / total_pixels) * 100
            else:
                analysis['error_type'] = 'major_pattern_error'
                analysis['error_percentage'] = (diff_count / total_pixels) * 100
            
            # ØªØ­Ù„ÙŠÙ„ Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
            if actual.shape == expected.shape:
                error_positions = np.where(diff_mask)
                analysis['error_positions'] = list(zip(error_positions[0], error_positions[1]))
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù„ÙˆØ§Ù†
        expected_colors = set(expected.flatten())
        actual_colors = set(actual.flatten())
        
        analysis['color_analysis'] = {
            'expected_colors': expected_colors,
            'actual_colors': actual_colors,
            'missing_colors': expected_colors - actual_colors,
            'extra_colors': actual_colors - expected_colors
        }
        
        return analysis
    
    def _develop_targeted_improvements(self, detailed_analysis):
        """ØªØ·ÙˆÙŠØ± ØªØ­Ø³ÙŠÙ†Ø§Øª Ù…Ø³ØªÙ‡Ø¯ÙØ©"""
        
        print("ğŸ› ï¸ ØªØ·ÙˆÙŠØ± ØªØ­Ø³ÙŠÙ†Ø§Øª Ù…Ø³ØªÙ‡Ø¯ÙØ©...")
        
        improvements = []
        
        # ØªØ­Ù„ÙŠÙ„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
        error_counts = {}
        for analysis in detailed_analysis:
            error_type = analysis['error_type']
            error_counts[error_type] = error_counts.get(error_type, 0) + 1
        
        # ØªØ·ÙˆÙŠØ± ØªØ­Ø³ÙŠÙ†Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
        for error_type, count in error_counts.items():
            if error_type == 'few_pixel_errors':
                improvements.append({
                    'type': 'pixel_correction',
                    'priority': count,
                    'description': 'ØªØµØ­ÙŠØ­ Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø¨ÙƒØ³Ù„ Ø§Ù„Ù‚Ù„ÙŠÙ„Ø©'
                })
            
            elif error_type == 'size_mismatch':
                improvements.append({
                    'type': 'size_correction',
                    'priority': count,
                    'description': 'ØªØµØ­ÙŠØ­ Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø­Ø¬Ù…'
                })
            
            elif error_type == 'minor_pattern_error':
                improvements.append({
                    'type': 'pattern_refinement',
                    'priority': count,
                    'description': 'ØªØ­Ø³ÙŠÙ† Ø¯Ù‚Ø© Ø§Ù„Ø£Ù†Ù…Ø§Ø·'
                })
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©
        improvements.sort(key=lambda x: x['priority'], reverse=True)
        
        print(f"ğŸ“‹ ØªÙ… ØªØ·ÙˆÙŠØ± {len(improvements)} ØªØ­Ø³ÙŠÙ†:")
        for i, improvement in enumerate(improvements):
            print(f"   {i+1}. {improvement['description']} (Ø£ÙˆÙ„ÙˆÙŠØ©: {improvement['priority']})")
        
        return improvements
    
    def _apply_and_test_improvements(self, improvements):
        """ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª ÙˆØ§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†ØªØ§Ø¦Ø¬"""
        
        print("ğŸ¯ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª...")
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª (Ù…Ø­Ø§ÙƒØ§Ø©)
        for improvement in improvements[:3]:  # Ø£Ù‡Ù… 3 ØªØ­Ø³ÙŠÙ†Ø§Øª
            print(f"   âœ… ØªØ·Ø¨ÙŠÙ‚: {improvement['description']}")
            # Ù‡Ù†Ø§ Ø³ÙŠØªÙ… ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙØ¹Ù„ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙˆØ¯
        
        # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù‚Ø±ÙŠØ¨Ø©
        print("\nğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª...")
        
        success_count = 0
        test_tasks = self.high_similarity_tasks[:10]  # Ø£ÙØ¶Ù„ 10 Ù…Ù‡Ø§Ù…
        
        for i, task_result in enumerate(test_tasks):
            try:
                # Ø¥Ø¹Ø§Ø¯Ø© Ø­Ù„ Ø§Ù„Ù…Ù‡Ù…Ø© Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª
                new_result = self._solve_task(task_result['task_id'])
                
                old_similarity = task_result['similarity']
                new_similarity = new_result['similarity']
                
                if new_result['solved_correctly']:
                    success_count += 1
                    status = "âœ… Ø­ÙÙ„Øª!"
                    self.solved_tasks.add(task_result['task_id'])
                elif new_similarity > old_similarity:
                    status = f"ğŸ“ˆ ØªØ­Ø³Ù†: {old_similarity:.3f} â†’ {new_similarity:.3f}"
                elif new_similarity == old_similarity:
                    status = f"ğŸ“Š Ø«Ø§Ø¨Øª: {new_similarity:.3f}"
                else:
                    status = f"ğŸ“‰ ØªØ±Ø§Ø¬Ø¹: {old_similarity:.3f} â†’ {new_similarity:.3f}"
                
                print(f"   {i+1:2d}. {task_result['task_id'][:8]}: {status}")
                
            except Exception as e:
                print(f"   {i+1:2d}. {task_result['task_id'][:8]}: âŒ Ø®Ø·Ø£")
        
        success_rate = success_count / len(test_tasks)
        
        print(f"\nğŸ‰ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:")
        print(f"âœ… Ù…Ù‡Ø§Ù… Ù…Ø­Ù„ÙˆÙ„Ø©: {success_count}/{len(test_tasks)} ({success_rate:.1%})")
        print(f"ğŸ¯ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…Ø­Ù„ÙˆÙ„Ø©: {len(self.solved_tasks)}")
        
        return {
            'success_count': success_count,
            'total_tested': len(test_tasks),
            'success_rate': success_rate,
            'solved_tasks': list(self.solved_tasks)
        }

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    
    system = PrecisionImprovementSystem()
    results = system.start_precision_improvement(target_tasks=50)
    
    print("\n" + "="*50)
    print("ğŸ† Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:")
    print(f"âœ… Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­: {results['success_rate']:.1%}")
    print(f"ğŸ¯ Ù…Ù‡Ø§Ù… Ù…Ø­Ù„ÙˆÙ„Ø©: {results['success_count']}/{results['total_tested']}")
    
    if results['success_rate'] >= 0.3:
        print("ğŸ‰ Ù†Ø¬Ø§Ø­ Ù…Ù…ØªØ§Ø²!")
    elif results['success_rate'] >= 0.1:
        print("ğŸ“ˆ ØªÙ‚Ø¯Ù… Ø¬ÙŠØ¯!")
    else:
        print("âš ï¸ ÙŠØ­ØªØ§Ø¬ Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªØ·ÙˆÙŠØ±")

if __name__ == "__main__":
    main()
