from __future__ import annotations
#!/usr/bin/env python3
"""
Ø§Ø®ØªØ¨Ø§Ø± Ø­Ù‚ÙŠÙ‚ÙŠ Ø¹Ù„Ù‰ Ù…Ù‡Ø§Ù… Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ù…Ø¹ Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø­Ù„ÙˆÙ„
"""

import os
import json
import numpy as np
import time
from collections.abc import Callable
from typing import Dict, List, Any

def load_evaluation_data():
    """ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙ‚ÙŠÙŠÙ… ÙˆØ§Ù„Ø­Ù„ÙˆÙ„"""
    
    print("ğŸ“‚ ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙ‚ÙŠÙŠÙ…...")
    
    # Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ù…Ù„ÙØ§Øª
    eval_path = 'Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø³Ø§Ø¨Ù‚Ø©arc-prize-2025/arc-agi_evaluation_challenges.json'
    solutions_path = 'Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø³Ø§Ø¨Ù‚Ø©arc-prize-2025/arc-agi_evaluation_solutions.json'
    
    # ØªØ­Ù…ÙŠÙ„ Ù…Ù‡Ø§Ù… Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
    if os.path.exists(eval_path):
        with open(eval_path, 'r') as f:
            eval_tasks = json.load(f)
        print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(eval_tasks)} Ù…Ù‡Ù…Ø© ØªÙ‚ÙŠÙŠÙ…")
    else:
        print("âŒ Ù…Ù„Ù Ù…Ù‡Ø§Ù… Ø§Ù„ØªÙ‚ÙŠÙŠÙ… ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
        return None, None
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„Ø±Ø³Ù…ÙŠØ©
    if os.path.exists(solutions_path):
        with open(solutions_path, 'r') as f:
            official_solutions = json.load(f)
        print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(official_solutions)} Ø­Ù„ Ø±Ø³Ù…ÙŠ")
    else:
        print("âŒ Ù…Ù„Ù Ø§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„Ø±Ø³Ù…ÙŠØ© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
        return eval_tasks, None
    
    return eval_tasks, official_solutions

def test_system_on_evaluation(system_name, system_class, eval_tasks, official_solutions, max_tasks=10):
    """Ø§Ø®ØªØ¨Ø§Ø± Ù†Ø¸Ø§Ù… Ø¹Ù„Ù‰ Ù…Ù‡Ø§Ù… Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©"""
    
    print(f"\nğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± {system_name} Ø¹Ù„Ù‰ Ù…Ù‡Ø§Ù… Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©")
    print("=" * 60)
    
    try:
        system = system_class()
        print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ {system_name}")
    except Exception as e:
        print(f"âŒ ÙØ´Ù„ Ø¥Ù†Ø´Ø§Ø¡ {system_name}: {e}")
        return {'system': system_name, 'error': str(e), 'correct': 0, 'total': 0}
    
    results = {
        'system': system_name,
        'correct': 0,
        'total': 0,
        'details': [],
        'execution_time': 0
    }
    
    task_ids = list(eval_tasks.keys())[:max_tasks]
    start_time = time.time()
    
    for i, task_id in enumerate(task_ids):
        print(f"\nğŸ“‹ Ù…Ù‡Ù…Ø© {i+1}/{len(task_ids)}: {task_id}")
        
        task = eval_tasks[task_id]
        official_solution = official_solutions.get(task_id, []) if official_solutions else []
        
        try:
            # Ø­Ù„ Ø§Ù„Ù…Ù‡Ù…Ø©
            task_start = time.time()
            solutions = system.solve_task(task)
            task_time = time.time() - task_start
            
            # Ù…Ù‚Ø§Ø±Ù†Ø© Ù…Ø¹ Ø§Ù„Ø­Ù„ Ø§Ù„Ø±Ø³Ù…ÙŠ
            is_correct = False
            if solutions and official_solution:
                is_correct = compare_solutions(solutions, official_solution)
            
            if is_correct:
                results['correct'] += 1
                print(f"   âœ… ØµØ­ÙŠØ­ ({task_time:.2f}s)")
            else:
                print(f"   âŒ Ø®Ø·Ø£ ({task_time:.2f}s)")
            
            results['details'].append({
                'task_id': task_id,
                'correct': is_correct,
                'time': task_time,
                'has_solution': len(solutions) > 0 if solutions else False
            })
            
        except Exception as e:
            print(f"   ğŸ’¥ Ø®Ø·Ø£: {str(e)[:50]}...")
            results['details'].append({
                'task_id': task_id,
                'correct': False,
                'error': str(e),
                'time': 0
            })
        
        results['total'] += 1
        
        # Ø¹Ø±Ø¶ Ø§Ù„ØªÙ‚Ø¯Ù…
        accuracy = (results['correct'] / results['total']) * 100
        print(f"   ğŸ“Š Ø§Ù„ØªÙ‚Ø¯Ù…: {results['correct']}/{results['total']} ({accuracy:.1f}%)")
    
    results['execution_time'] = time.time() - start_time
    accuracy = (results['correct'] / results['total']) * 100 if results['total'] > 0 else 0
    
    print(f"\nğŸ“ˆ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ù„Ù€ {system_name}:")
    print(f"   âœ… ØµØ­ÙŠØ­: {results['correct']}")
    print(f"   âŒ Ø®Ø·Ø£: {results['total'] - results['correct']}")
    print(f"   ğŸ¯ Ø§Ù„Ø¯Ù‚Ø©: {accuracy:.1f}%")
    print(f"   â±ï¸ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {results['execution_time']:.2f}s")
    
    return results

def compare_solutions(system_solutions, official_solutions):
    """Ù…Ù‚Ø§Ø±Ù†Ø© Ø­Ù„ÙˆÙ„ Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ø¹ Ø§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„Ø±Ø³Ù…ÙŠØ©"""
    
    if not system_solutions or not official_solutions:
        return False
    
    if len(system_solutions) != len(official_solutions):
        return False
    
    for sys_sol, off_sol in zip(system_solutions, official_solutions):
        if sys_sol is None or not isinstance(sys_sol, np.ndarray):
            return False
        
        official_array = np.array(off_sol)
        
        if not np.array_equal(sys_sol, official_array):
            return False
    
    return True

def run_comprehensive_evaluation():
    """ØªØ´ØºÙŠÙ„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø´Ø§Ù…Ù„"""
    
    print("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø´Ø§Ù…Ù„ Ø¹Ù„Ù‰ Ù…Ù‡Ø§Ù… ARC Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©")
    print("=" * 70)
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    eval_tasks, official_solutions = load_evaluation_data()
    
    if not eval_tasks:
        print("âŒ Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø© Ø¨Ø¯ÙˆÙ† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙ‚ÙŠÙŠÙ…")
        return
    
    # Ø§Ù„Ø£Ù†Ø¸Ù…Ø© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
    systems_to_test = [
        ('final_arc_system', 'FinalARCSystem'),
        ('arc_clean_integrated_system', 'ARCCleanIntegratedSystem'),
        ('arc_ultimate_perfect_system', 'ARCUltimatePerfectSolver'),
    ]
    
    all_results = []
    
    for system_file, class_name in systems_to_test:
        print(f"\n{'='*70}")
        print(f"ğŸ¯ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ø¸Ø§Ù…: {system_file}")
        print(f"{'='*70}")
        
        try:
            # Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù†Ø¸Ø§Ù…
            module = __import__(system_file)
            system_class = getattr(module, class_name)
            
            # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ø¸Ø§Ù…
            result = test_system_on_evaluation(
                system_file, system_class, eval_tasks, official_solutions, max_tasks=5
            )
            all_results.append(result)
            
        except Exception as e:
            print(f"âŒ ÙØ´Ù„ ÙÙŠ Ø§Ø®ØªØ¨Ø§Ø± {system_file}: {e}")
            all_results.append({
                'system': system_file,
                'error': str(e),
                'correct': 0,
                'total': 0
            })
    
    # Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
    print(f"\n{'='*70}")
    print("ğŸ“Š Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©")
    print(f"{'='*70}")
    
    print(f"{'Ø§Ù„Ù†Ø¸Ø§Ù…':<35} {'ØµØ­ÙŠØ­':<8} {'Ø¥Ø¬Ù…Ø§Ù„ÙŠ':<8} {'Ø¯Ù‚Ø©%':<8} {'ÙˆÙ‚Øª(s)':<10}")
    print("-" * 70)
    
    best_system = None
    best_accuracy = 0
    
    for result in all_results:
        if 'error' in result:
            print(f"{result['system']:<35} {'Ø®Ø·Ø£':<8} {'Ø®Ø·Ø£':<8} {'0.0':<8} {'N/A':<10}")
        else:
            accuracy = (result['correct'] / result['total']) * 100 if result['total'] > 0 else 0
            exec_time = result.get('execution_time', 0)
            
            print(f"{result['system']:<35} {result['correct']:<8} {result['total']:<8} {accuracy:<8.1f} {exec_time:<10.2f}")
            
            if accuracy > best_accuracy:
                best_accuracy = accuracy
                best_system = result['system']
    
    print(f"\nğŸ† Ø£ÙØ¶Ù„ Ù†Ø¸Ø§Ù…: {best_system} Ø¨Ø¯Ù‚Ø© {best_accuracy:.1f}%")
    
    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    timestamp = int(time.time())
    filename = f'real_evaluation_results_{timestamp}.json'
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ: {filename}")
    
    return all_results

if __name__ == "__main__":
    results = run_comprehensive_evaluation()
