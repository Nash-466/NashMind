from __future__ import annotations
# -*- coding: utf-8 -*-
"""
ARC PRIZE 2025 - MAIN INTEGRATION SYSTEM
========================================
Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„ Ù„Ù…Ø³Ø§Ø¨Ù‚Ø© ARC Prize 2025
ÙŠØ¬Ù…Ø¹ Ø¨ÙŠÙ† Ø§Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ù…Ø¹ ÙˆØ§Ø¬Ù‡Ø© Ù…ÙˆØ­Ø¯Ø©

Ø§Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ù…Ø¯Ù…Ø¬Ø©:
1. MasterOrchestrator - Ù†Ø¸Ø§Ù… Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
2. UltimateOrchestrator - Ù†Ø¸Ø§Ù… Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ù…Ø¹Ø±ÙÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
3. UltimateSystem - Ù†Ø¸Ø§Ù… Ø§Ù„ÙˆØ¹ÙŠ Ø§Ù„Ø°Ø§ØªÙŠ ÙˆØ§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ø³Ø¨Ø¨ÙŠ
4. ARCInteractiveSystem - Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠ Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„

Ø§Ù„Ù…Ø¤Ù„Ù: Ù…Ø³Ø§Ø¹Ø¯ AI
Ø§Ù„ØªØ§Ø±ÙŠØ®: 2025
"""

import argparse
import json
import logging
import os
import sys
import time
from collections.abc import Callable
from typing import Dict, Any, List, Optional
import numpy as np

# Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(levelname)s] - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø£Ù†Ø¸Ù…Ø©
try:
    from arc_interactive_system import ARCInteractiveSystem
    from arc_ultimate_mind_part7 import MasterOrchestrator
    from arc_ultimate_system import UltimateOrchestrator
    from arc_revolutionary_system import UltimateSystem
    from kaggle_io import load_arc_tasks_from_dir, load_arc_solutions_from_dir
    SYSTEMS_AVAILABLE = True
    logger.info("âœ… Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ù†Ø¸Ù…Ø© Ù…ØªØ§Ø­Ø©")
except ImportError as e:
    SYSTEMS_AVAILABLE = False
    logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø£Ù†Ø¸Ù…Ø©: {e}")
    sys.exit(1)

class ARCPrize2025System:
    """Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù…Ø³Ø§Ø¨Ù‚Ø© ARC Prize 2025"""
    
    def __init__(self, config: Dict[str, Any] = None):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ"""
        self.config = config or {}
        
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠ Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„
        self.interactive_system = ARCInteractiveSystem(self.config)
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø³Ø§Ø¨Ù‚Ø©
        self.competition_config = {
            'max_time_per_task': 30.0,
            'max_memory_mb': 8192,
            'output_format': 'submission',
            'enable_validation': True,
            'enable_learning': True
        }
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø³Ø§Ø¨Ù‚Ø©
        self.competition_stats = {
            'total_tasks': 0,
            'solved_tasks': 0,
            'failed_tasks': 0,
            'total_time': 0.0,
            'average_consensus': 0.0,
            'system_performance': {}
        }
        
        logger.info("ğŸ† Ù†Ø¸Ø§Ù… ARC Prize 2025 Ø¬Ø§Ù‡Ø² Ù„Ù„Ù…Ø³Ø§Ø¨Ù‚Ø©!")
    
    def process_training_data(self, data_path: str) -> Dict[str, Any]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
        logger.info(f"ğŸ“š Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ù†: {data_path}")
        
        try:
            # ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
            training_tasks = load_arc_tasks_from_dir(data_path, 'train')
            training_solutions = load_arc_solutions_from_dir(data_path, 'train')
            
            logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(training_tasks)} Ù…Ù‡Ù…Ø© ØªØ¯Ø±ÙŠØ¨")
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ÙŠØ©
            training_results = self._process_task_batch(training_tasks, training_solutions, 'training')
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            self.competition_stats.update(training_results['stats'])
            
            return training_results
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {e}")
            return {'error': str(e), 'stats': self.competition_stats}
    
    def process_evaluation_data(self, data_path: str) -> Dict[str, Any]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙ‚ÙŠÙŠÙ…"""
        logger.info(f"ğŸ“Š Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ù…Ù†: {data_path}")
        
        try:
            # ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
            eval_tasks = load_arc_tasks_from_dir(data_path, 'eval')
            eval_solutions = load_arc_solutions_from_dir(data_path, 'eval')
            
            logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(eval_tasks)} Ù…Ù‡Ù…Ø© ØªÙ‚ÙŠÙŠÙ…")
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù‡Ø§Ù… Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
            eval_results = self._process_task_batch(eval_tasks, eval_solutions, 'evaluation')
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            self.competition_stats.update(eval_results['stats'])
            
            return eval_results
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙ‚ÙŠÙŠÙ…: {e}")
            return {'error': str(e), 'stats': self.competition_stats}
    
    def process_test_data(self, data_path: str, output_path: str = 'submission.json') -> Dict[str, Any]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± ÙˆØ¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ø§Ù„ØªÙ‚Ø¯ÙŠÙ…"""
        logger.info(f"ğŸ§ª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ù…Ù†: {data_path}")
        
        try:
            # ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
            test_tasks = load_arc_tasks_from_dir(data_path, 'test')
            
            logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(test_tasks)} Ù…Ù‡Ù…Ø© Ø§Ø®ØªØ¨Ø§Ø±")
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù‡Ø§Ù… Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
            test_results = self._process_task_batch(test_tasks, None, 'test')
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ø§Ù„ØªÙ‚Ø¯ÙŠÙ…
            submission_data = self._create_submission_file(test_results['solutions'], output_path)
            
            logger.info(f"ğŸ“ ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ø§Ù„ØªÙ‚Ø¯ÙŠÙ…: {output_path}")
            
            return {
                'submission_file': output_path,
                'stats': test_results['stats'],
                'solutions_count': len(test_results['solutions'])
            }
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {e}")
            return {'error': str(e)}
    
    def _process_task_batch(self, tasks: Dict[str, Any], solutions: Dict[str, Any] = None, 
                           mode: str = 'test') -> Dict[str, Any]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ù…Ù‡Ø§Ù…"""
        logger.info(f"ğŸ”„ Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© {len(tasks)} Ù…Ù‡Ù…Ø© ÙÙŠ ÙˆØ¶Ø¹: {mode}")
        
        batch_start_time = time.time()
        processed_solutions = {}
        batch_stats = {
            'total_tasks': len(tasks),
            'solved_tasks': 0,
            'failed_tasks': 0,
            'total_time': 0.0,
            'average_consensus': 0.0,
            'system_performance': {}
        }
        
        consensus_scores = []
        
        for task_id, task_data in tasks.items():
            logger.info(f"ğŸ¯ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù‡Ù…Ø©: {task_id}")
            task_start_time = time.time()
            
            try:
                # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù‡Ù…Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠ
                result = self.interactive_system.process_task_interactive(task_data, task_id)
                
                task_time = time.time() - task_start_time
                
                # Ø­ÙØ¸ Ø§Ù„Ø­Ù„
                if result.final_solution is not None:
                    processed_solutions[task_id] = result.final_solution.tolist()
                    batch_stats['solved_tasks'] += 1
                else:
                    # Ø­Ù„ Ø§ÙØªØ±Ø§Ø¶ÙŠ ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„ÙØ´Ù„
                    processed_solutions[task_id] = [[0]]
                    batch_stats['failed_tasks'] += 1
                
                # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
                batch_stats['total_time'] += task_time
                consensus_scores.append(result.consensus_score)
                
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø­Ù„ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø­Ù„ÙˆÙ„ Ù…ØªØ§Ø­Ø©
                if solutions and task_id in solutions:
                    validation_result = self._validate_solution(
                        result.final_solution, 
                        solutions[task_id], 
                        task_id
                    )
                    logger.info(f"âœ… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…Ù‡Ù…Ø© {task_id}: {validation_result}")
                
                logger.info(f"âœ… Ø§ÙƒØªÙ…Ù„Øª Ø§Ù„Ù…Ù‡Ù…Ø© {task_id} ÙÙŠ {task_time:.2f}s - Ø§Ù„Ø¥Ø¬Ù…Ø§Ø¹: {result.consensus_score:.3f}")
                
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù‡Ù…Ø© {task_id}: {e}")
                processed_solutions[task_id] = [[0]]  # Ø­Ù„ Ø§ÙØªØ±Ø§Ø¶ÙŠ
                batch_stats['failed_tasks'] += 1
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        batch_stats['average_consensus'] = np.mean(consensus_scores) if consensus_scores else 0.0
        batch_stats['success_rate'] = batch_stats['solved_tasks'] / batch_stats['total_tasks']
        batch_stats['average_time_per_task'] = batch_stats['total_time'] / batch_stats['total_tasks']
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ù„Ø®Øµ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø£Ù†Ø¸Ù…Ø©
        batch_stats['system_performance'] = self.interactive_system.get_system_performance_summary()
        
        total_batch_time = time.time() - batch_start_time
        logger.info(f"ğŸ Ø§Ù†ØªÙ‡Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¯ÙØ¹Ø© ÙÙŠ {total_batch_time:.2f}s")
        logger.info(f"ğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬: {batch_stats['solved_tasks']}/{batch_stats['total_tasks']} Ù…Ù‡Ø§Ù… Ù…Ø­Ù„ÙˆÙ„Ø©")
        logger.info(f"ğŸ“ˆ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­: {batch_stats['success_rate']:.1%}")
        logger.info(f"ğŸ¤ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø¥Ø¬Ù…Ø§Ø¹: {batch_stats['average_consensus']:.3f}")
        
        return {
            'solutions': processed_solutions,
            'stats': batch_stats
        }
    
    def _validate_solution(self, predicted_solution: np.ndarray, expected_solution: Any, task_id: str) -> str:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø­Ù„"""
        try:
            if predicted_solution is None:
                return "ÙØ´Ù„ - Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø­Ù„"
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø­Ù„ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ Ø¥Ù„Ù‰ numpy array Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
            if isinstance(expected_solution, list):
                expected_array = np.array(expected_solution)
            else:
                expected_array = expected_solution
            
            # Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø­Ù„ÙˆÙ„
            if np.array_equal(predicted_solution, expected_array):
                return "ØµØ­ÙŠØ­ âœ…"
            else:
                # Ø­Ø³Ø§Ø¨ Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ·Ø§Ø¨Ù‚
                matches = np.sum(predicted_solution == expected_array)
                total = predicted_solution.size
                accuracy = matches / total if total > 0 else 0.0
                return f"Ø®Ø·Ø£ - Ø¯Ù‚Ø©: {accuracy:.1%}"
                
        except Exception as e:
            return f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù‚Ù‚: {e}"
    
    def _create_submission_file(self, solutions: Dict[str, Any], output_path: str) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ø§Ù„ØªÙ‚Ø¯ÙŠÙ… Ø¨Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨"""
        logger.info(f"ğŸ“ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ø§Ù„ØªÙ‚Ø¯ÙŠÙ…: {output_path}")
        
        # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
        submission_data = {}
        
        for task_id, solution in solutions.items():
            submission_data[task_id] = [{'attempt_1': solution}]
        
        # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(submission_data, f, indent=2)
        
        logger.info(f"âœ… ØªÙ… Ø­ÙØ¸ Ù…Ù„Ù Ø§Ù„ØªÙ‚Ø¯ÙŠÙ…: {output_path}")
        return output_path
    
    def get_competition_summary(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ù„Ø®Øµ Ø§Ù„Ù…Ø³Ø§Ø¨Ù‚Ø©"""
        return {
            'competition_stats': self.competition_stats,
            'system_performance': self.interactive_system.get_system_performance_summary(),
            'interaction_config': self.interactive_system.interaction_config,
            'timestamp': time.time()
        }
    
    def optimize_system(self):
        """ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ø¸Ø§Ù… Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø³Ø§Ø¨Ù‚"""
        logger.info("âš¡ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ø¸Ø§Ù…")
        
        # ØªØ­Ø³ÙŠÙ† Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªÙØ§Ø¹Ù„
        self.interactive_system.optimize_interaction_config()
        
        logger.info("âœ… ØªÙ… ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ø¸Ø§Ù…")


def parse_arguments():
    """ØªØ­Ù„ÙŠÙ„ ÙˆØ³ÙŠØ·Ø§Øª Ø³Ø·Ø± Ø§Ù„Ø£ÙˆØ§Ù…Ø±"""
    parser = argparse.ArgumentParser(
        description='ARC Prize 2025 - Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„ Ù„Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ø«Ù„Ø§Ø«Ø©',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:
  python arc_prize_2025_main.py --train-data ./data --mode training
  python arc_prize_2025_main.py --eval-data ./data --mode evaluation  
  python arc_prize_2025_main.py --test-data ./data --mode test --output submission.json
  python arc_prize_2025_main.py --single-task ./data/task_001.json --mode single
        """
    )
    
    parser.add_argument('--data-path', '-d', required=True,
                       help='Ù…Ø³Ø§Ø± Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª')
    parser.add_argument('--mode', '-m', required=True,
                       choices=['training', 'evaluation', 'test', 'single'],
                       help='ÙˆØ¶Ø¹ Ø§Ù„ØªØ´ØºÙŠÙ„')
    parser.add_argument('--output', '-o', default='submission.json',
                       help='Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ (Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±)')
    parser.add_argument('--single-task', '-s',
                       help='Ù…Ø³Ø§Ø± Ù…Ù‡Ù…Ø© ÙˆØ§Ø­Ø¯Ø© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±')
    parser.add_argument('--config', '-c',
                       help='Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª JSON')
    parser.add_argument('--log-level', default='INFO',
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
                       help='Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ³Ø¬ÙŠÙ„')
    
    return parser.parse_args()


def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    args = parse_arguments()
    
    # ØªØ­Ø¯ÙŠØ« Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
    logging.getLogger().setLevel(getattr(logging, args.log_level.upper()))
    
    logger.info("ğŸš€ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ù†Ø¸Ø§Ù… ARC Prize 2025")
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
    config = {}
    if args.config and os.path.exists(args.config):
        with open(args.config, 'r', encoding='utf-8') as f:
            config = json.load(f)
        logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ù†: {args.config}")
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
    arc_system = ARCPrize2025System(config)
    
    try:
        if args.mode == 'training':
            # ÙˆØ¶Ø¹ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
            logger.info("ğŸ“š Ø¨Ø¯Ø¡ ÙˆØ¶Ø¹ Ø§Ù„ØªØ¯Ø±ÙŠØ¨")
            results = arc_system.process_training_data(args.data_path)
            
        elif args.mode == 'evaluation':
            # ÙˆØ¶Ø¹ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
            logger.info("ğŸ“Š Ø¨Ø¯Ø¡ ÙˆØ¶Ø¹ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…")
            results = arc_system.process_evaluation_data(args.data_path)
            
        elif args.mode == 'test':
            # ÙˆØ¶Ø¹ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
            logger.info("ğŸ§ª Ø¨Ø¯Ø¡ ÙˆØ¶Ø¹ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±")
            results = arc_system.process_test_data(args.data_path, args.output)
            
        elif args.mode == 'single':
            # Ù…Ù‡Ù…Ø© ÙˆØ§Ø­Ø¯Ø©
            logger.info("ğŸ¯ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù‡Ù…Ø© ÙˆØ§Ø­Ø¯Ø©")
            if args.single_task:
                with open(args.single_task, 'r', encoding='utf-8') as f:
                    task_data = json.load(f)
                result = arc_system.interactive_system.process_task_interactive(task_data)
                logger.info(f"âœ… Ø§Ù„Ù†ØªÙŠØ¬Ø©: {'Ù†Ø¬Ø­' if result.final_solution is not None else 'ÙØ´Ù„'}")
            else:
                logger.error("âŒ ÙŠØ¬Ø¨ ØªØ­Ø¯ÙŠØ¯ Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù‡Ù…Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… --single-task")
                return
        
        # Ø¹Ø±Ø¶ Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        summary = arc_system.get_competition_summary()
        logger.info("ğŸ“ˆ Ù…Ù„Ø®Øµ Ø§Ù„Ù…Ø³Ø§Ø¨Ù‚Ø©:")
        logger.info(f"   â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù‡Ø§Ù…: {summary['competition_stats']['total_tasks']}")
        logger.info(f"   â€¢ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…Ø­Ù„ÙˆÙ„Ø©: {summary['competition_stats']['solved_tasks']}")
        logger.info(f"   â€¢ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­: {summary['competition_stats'].get('success_rate', 0):.1%}")
        logger.info(f"   â€¢ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø¥Ø¬Ù…Ø§Ø¹: {summary['competition_stats']['average_consensus']:.3f}")
        
        # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ø¸Ø§Ù…
        arc_system.optimize_system()
        
        logger.info("ğŸ† Ø§Ù†ØªÙ‡Ù‰ ØªØ´ØºÙŠÙ„ Ù†Ø¸Ø§Ù… ARC Prize 2025 Ø¨Ù†Ø¬Ø§Ø­!")
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ´ØºÙŠÙ„: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()

