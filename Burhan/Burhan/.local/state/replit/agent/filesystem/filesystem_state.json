{"file_contents":{"main.py":{"content":"#!/usr/bin/env python3\n\"\"\"\nARC Prize 2025 Solver - Comprehensive Task Parser and Grid Manipulator\nDemonstrates all capabilities of the ARC system\n\"\"\"\n\nimport os\nimport sys\nimport numpy as np\nimport json\nfrom pathlib import Path\nfrom typing import List, Tuple, Dict\n\n# Add project root to path\nsys.path.insert(0, str(Path(__file__).parent))\n\n# Import our ARC modules\nfrom src.arc.task_loader import TaskLoader, ARCTask, ARCExample\nfrom src.arc.grid_operations import Grid\nfrom src.arc.pattern_detector import PatternDetector\nfrom src.arc.transformation_rules import (\n    TransformationRule, TransformationType, RuleChain,\n    TransformationInference, RuleLibrary\n)\n\n# Import the new solvers\nfrom src.solvers.program_synthesis import ProgramSynthesisEngine\nfrom src.solvers.csp_solver import CSPSolver\nfrom src.solvers.pattern_solver import PatternSolver\nfrom src.solvers.ensemble_solver import EnsembleSolver\nfrom src.strategies.strategy_selector import StrategySelector\n\n\ndef print_header(text: str):\n    \"\"\"Print a formatted header\"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(text.center(60))\n    print(\"=\" * 60)\n\n\ndef visualize_grid(grid: Grid, title: str = \"Grid\"):\n    \"\"\"Visualize a grid with ASCII characters\"\"\"\n    print(f\"\\n{title} ({grid.height}x{grid.width}):\")\n    \n    # Simple ASCII representation\n    for row in grid.data:\n        print(' '.join(str(cell) if cell != 0 else '.' for cell in row))\n\n\ndef demonstrate_task_loading():\n    \"\"\"Demonstrate task loading capabilities\"\"\"\n    print_header(\"TASK LOADING DEMONSTRATION\")\n    \n    loader = TaskLoader()\n    \n    # Load from attached files if they exist\n    attached_challenges = \"attached_assets/arc-agi_training_challenges (2)_1757573784094.txt\"\n    attached_solutions = \"attached_assets/arc-agi_training_solutions_1757573784094.txt\"\n    \n    tasks_loaded = False\n    \n    if os.path.exists(attached_challenges):\n        print(f\"Loading challenges from: {attached_challenges}\")\n        try:\n            with open(attached_challenges, 'r') as f:\n                content = f.read()\n                tasks = loader.load_from_json_string(content)\n                print(f\"âœ“ Loaded {len(tasks)} tasks\")\n                tasks_loaded = True\n        except Exception as e:\n            print(f\"Error loading challenges: {e}\")\n    \n    if os.path.exists(attached_solutions):\n        print(f\"Loading solutions from: {attached_solutions}\")\n        try:\n            with open(attached_solutions, 'r') as f:\n                content = f.read()\n                solutions = loader.load_from_json_string(content, is_solution=True)\n                print(f\"âœ“ Loaded solutions for {len(solutions)} tasks\")\n        except Exception as e:\n            print(f\"Error loading solutions: {e}\")\n    \n    # Display statistics\n    stats = loader.get_statistics()\n    print(\"\\nTask Statistics:\")\n    print(f\"  Total tasks: {stats['total_tasks']}\")\n    print(f\"  Total training examples: {stats['total_train_examples']}\")\n    print(f\"  Total test examples: {stats['total_test_examples']}\")\n    \n    # Show sample task\n    task_ids = loader.list_tasks()\n    if task_ids and len(task_ids) > 0:\n        sample_task_id = task_ids[0]\n        task = loader.get_task(sample_task_id)\n        \n        if task and task.num_train > 0:\n            print(f\"\\nSample Task: {sample_task_id}\")\n            input_grid, output_grid = task.get_train_pair(0)\n            visualize_grid(Grid(input_grid), \"First Training Input\")\n            if output_grid is not None:\n                visualize_grid(Grid(output_grid), \"First Training Output\")\n    \n    return loader\n\n\ndef demonstrate_grid_operations():\n    \"\"\"Demonstrate grid manipulation operations\"\"\"\n    print_header(\"GRID OPERATIONS DEMONSTRATION\")\n    \n    # Create sample grid\n    sample_data = np.array([\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ])\n    grid = Grid(sample_data)\n    \n    print(\"\\n--- Basic Transformations ---\")\n    visualize_grid(grid, \"Original\")\n    \n    # Rotation\n    rotated = grid.rotate(90)\n    visualize_grid(rotated, \"Rotated 90Â°\")\n    \n    # Scale\n    small_grid = Grid(np.array([[1, 2], [3, 4]]))\n    scaled = small_grid.scale(2)\n    visualize_grid(scaled, \"Scaled 2x\")\n    \n    # Tile\n    tiled = small_grid.tile(2, 2)\n    visualize_grid(tiled, \"Tiled 2x2\")\n    \n    print(f\"\\nGrid shape: {grid.shape}\")\n    print(f\"Unique colors: {grid.unique_colors}\")\n    print(f\"Color counts: {grid.count_colors()}\")\n\n\ndef demonstrate_pattern_detection():\n    \"\"\"Demonstrate pattern detection capabilities\"\"\"\n    print_header(\"PATTERN DETECTION DEMONSTRATION\")\n    \n    # Create grid with patterns\n    symmetric_grid = Grid(np.array([\n        [1, 2, 3, 2, 1],\n        [4, 5, 6, 5, 4],\n        [7, 8, 9, 8, 7]\n    ]))\n    \n    detector = PatternDetector(symmetric_grid)\n    symmetries = detector.get_symmetries()\n    \n    print(\"\\nSymmetry Detection:\")\n    for sym_type, has_sym in symmetries.items():\n        status = \"âœ“\" if has_sym else \"âœ—\"\n        print(f\"  {status} {sym_type}\")\n    \n    # Pattern statistics\n    stats = detector.get_pattern_statistics()\n    print(f\"\\nPattern Statistics:\")\n    print(f\"  Grid shape: {stats['grid_shape']}\")\n    print(f\"  Unique colors: {stats['unique_colors']}\")\n    print(f\"  Sparsity: {stats['sparsity']:.2f}\")\n\n\ndef demonstrate_transformation_rules():\n    \"\"\"Demonstrate transformation rules and inference\"\"\"\n    print_header(\"TRANSFORMATION RULES DEMONSTRATION\")\n    \n    grid = Grid(np.array([[1, 2], [3, 4]]))\n    \n    print(\"\\n--- Applying Individual Rules ---\")\n    visualize_grid(grid, \"Original\")\n    \n    # Apply rotation\n    rotate_rule = RuleLibrary.get_rotation_rules()[0]\n    rotated = rotate_rule.apply(grid)\n    visualize_grid(rotated, f\"After {rotate_rule.description}\")\n    \n    # Infer transformation\n    print(\"\\n--- Transformation Inference ---\")\n    input_grid = Grid(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))\n    output_grid = input_grid.rotate(90)\n    \n    inferred_rule = TransformationInference.infer_simple_transformation(input_grid, output_grid)\n    if inferred_rule:\n        print(f\"Inferred: {inferred_rule.description}\")\n\n\ndef demonstrate_solvers():\n    \"\"\"Demonstrate the new solving capabilities\"\"\"\n    print_header(\"SOLVER DEMONSTRATIONS\")\n    \n    # Example 1: Scaling task\n    print(\"\\n--- Program Synthesis Solver ---\")\n    train_inputs = [\n        np.array([[1, 2], [3, 4]], dtype=np.int8),\n        np.array([[5, 6], [7, 8]], dtype=np.int8)\n    ]\n    train_outputs = [\n        np.array([[1, 1, 2, 2], [1, 1, 2, 2], [3, 3, 4, 4], [3, 3, 4, 4]], dtype=np.int8),\n        np.array([[5, 5, 6, 6], [5, 5, 6, 6], [7, 7, 8, 8], [7, 7, 8, 8]], dtype=np.int8)\n    ]\n    test_input = np.array([[2, 3], [4, 5]], dtype=np.int8)\n    \n    ps_solver = ProgramSynthesisEngine()\n    program = ps_solver.get_program(train_inputs, train_outputs)\n    if program:\n        print(f\"âœ“ Synthesized program: {program.to_string()}\")\n        result = ps_solver.solve(train_inputs, train_outputs, test_input)\n        if result is not None:\n            print(f\"âœ“ Applied to test input - output shape: {result.shape}\")\n    \n    # Example 2: Pattern Solver\n    print(\"\\n--- Pattern Solver ---\")\n    pattern_solver = PatternSolver()\n    result = pattern_solver.solve(train_inputs, train_outputs, test_input)\n    if result is not None:\n        print(f\"âœ“ Pattern solver found solution with shape: {result.shape}\")\n    \n    # Example 3: Strategy Selection\n    print(\"\\n--- Strategy Selector ---\")\n    selector = StrategySelector()\n    recommendation = selector.select_strategy(train_inputs, train_outputs)\n    print(f\"âœ“ Recommended strategy: {recommendation.primary_strategy}\")\n    print(f\"  Confidence: {recommendation.confidence:.3f}\")\n    print(f\"  Reasoning: {recommendation.reasoning}\")\n    \n    # Example 4: Ensemble Solver\n    print(\"\\n--- Ensemble Solver ---\")\n    ensemble_config = {\n        'use_program_synthesis': True,\n        'use_csp': False,  # Disable for speed\n        'use_pattern': True,\n        'use_hybrid': False,\n        'parallel': False,\n        'timeout_per_solver': 2.0\n    }\n    ensemble = EnsembleSolver(ensemble_config)\n    ensemble_result = ensemble.solve(train_inputs, train_outputs, test_input)\n    \n    print(f\"âœ“ Ensemble completed in {ensemble_result.total_time:.2f}s\")\n    print(f\"  Consensus score: {ensemble_result.consensus_score:.3f}\")\n    print(f\"  Strategy used: {ensemble_result.strategy_used}\")\n    if ensemble_result.final_solution is not None:\n        print(f\"  Solution shape: {ensemble_result.final_solution.shape}\")\n\n\ndef run_quick_tests():\n    \"\"\"Run a quick test of the system\"\"\"\n    print_header(\"SYSTEM VALIDATION\")\n    \n    try:\n        # Test Grid operations\n        grid = Grid(np.array([[1, 2], [3, 4]]))\n        assert grid.shape == (2, 2), \"Grid shape test failed\"\n        \n        # Test rotation\n        rotated = grid.rotate(90)\n        assert rotated.data[0, 0] == 2, \"Rotation test failed\"\n        \n        # Test pattern detector\n        detector = PatternDetector(grid)\n        stats = detector.get_pattern_statistics()\n        assert 'unique_colors' in stats, \"Pattern detector test failed\"\n        \n        # Test transformation rules\n        rule = RuleLibrary.get_scale_rule(2)\n        scaled = rule.apply(grid)\n        assert scaled.shape == (4, 4), \"Transformation rule test failed\"\n        \n        # Test new solvers\n        ps_solver = ProgramSynthesisEngine()\n        assert ps_solver is not None, \"Program synthesis initialization failed\"\n        \n        csp_solver = CSPSolver()\n        assert csp_solver is not None, \"CSP solver initialization failed\"\n        \n        pattern_solver = PatternSolver()\n        assert pattern_solver is not None, \"Pattern solver initialization failed\"\n        \n        ensemble_solver = EnsembleSolver()\n        assert ensemble_solver is not None, \"Ensemble solver initialization failed\"\n        \n        selector = StrategySelector()\n        assert selector is not None, \"Strategy selector initialization failed\"\n        \n        print(\"âœ“ All system components validated successfully!\")\n        \n    except AssertionError as e:\n        print(f\"âœ— Validation failed: {e}\")\n    except Exception as e:\n        print(f\"âœ— Error during validation: {e}\")\n\n\ndef main():\n    \"\"\"Main demonstration function\"\"\"\n    print(\"=\" * 60)\n    print(\"ARC PRIZE 2025 SOLVER\")\n    print(\"Task Parser & Grid Manipulator System\")\n    print(\"=\" * 60)\n    \n    print(\"\\nEnvironment Setup Complete!\")\n    \n    # Check library versions\n    print(\"\\nInstalled Libraries:\")\n    print(\"  âœ“ NumPy:\", np.__version__)\n    \n    try:\n        import scipy\n        print(\"  âœ“ SciPy:\", scipy.__version__)\n    except ImportError:\n        print(\"  âœ— SciPy: Not installed\")\n    \n    print(\"\\nSystem Modules:\")\n    print(\"  âœ“ src/arc/task_loader.py - Task loading and parsing\")\n    print(\"  âœ“ src/arc/grid_operations.py - Grid manipulation\")\n    print(\"  âœ“ src/arc/pattern_detector.py - Pattern detection\")\n    print(\"  âœ“ src/arc/transformation_rules.py - Transformation rules\")\n    print(\"  âœ“ tests/test_arc_operations.py - Test suite\")\n    \n    # Run demonstrations\n    print(\"\\n\" + \"=\" * 60)\n    print(\"RUNNING DEMONSTRATIONS\")\n    print(\"=\" * 60)\n    \n    # 1. Task Loading\n    loader = demonstrate_task_loading()\n    \n    # 2. Grid Operations\n    demonstrate_grid_operations()\n    \n    # 3. Pattern Detection\n    demonstrate_pattern_detection()\n    \n    # 4. Transformation Rules\n    demonstrate_transformation_rules()\n    \n    # 5. Solver Demonstrations\n    demonstrate_solvers()\n    \n    # 6. System Validation\n    run_quick_tests()\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"DEMONSTRATION COMPLETE\")\n    print(\"=\" * 60)\n    \n    print(\"\\nThe ARC Task Parser and Grid Manipulator system is ready!\")\n    print(\"\\nCapabilities:\")\n    print(\"  âœ“ Load and parse ARC tasks from JSON\")\n    print(\"  âœ“ Comprehensive grid manipulation operations\")\n    print(\"  âœ“ Pattern and symmetry detection\")\n    print(\"  âœ“ Transformation rule inference\")\n    print(\"  âœ“ Task solving framework\")\n    \n    print(\"\\nReady to solve ARC puzzles!\")\n\n\nif __name__ == \"__main__\":\n    main()","size_bytes":12137},"pyproject.toml":{"content":"[project]\nname = \"python-template\"\nversion = \"0.1.0\"\ndescription = \"\"\nauthors = [\"Your Name <you@example.com>\"]\nrequires-python = \">=3.11\"\ndependencies = [\n    \"matplotlib>=3.10.6\",\n    \"networkx>=3.5\",\n    \"numpy>=2.3.3\",\n    \"ortools>=9.14.6206\",\n    \"pillow>=11.3.0\",\n    \"scikit-learn>=1.7.2\",\n    \"scipy>=1.16.1\",\n]\n","size_bytes":321},"arc_solver_main.py":{"content":"#!/usr/bin/env python3\n\"\"\"\nARC Solver Main Entry Point\nCommand-line interface for solving ARC Prize 2025 tasks\n\"\"\"\n\nimport argparse\nimport json\nimport os\nimport sys\nimport time\nimport logging\nfrom pathlib import Path\nimport numpy as np\nfrom typing import List, Dict, Optional, Tuple\n\n# Add src to path\nsys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))\n\nfrom src.arc_agent import ARCAgent, AgentMode\nfrom src.arc.task_loader import TaskLoader, ARCTask\nfrom src.utils.evaluator import SolutionEvaluator\nfrom src.arc.grid_operations import Grid\n\n\ndef setup_logging(verbose: bool = False):\n    \"\"\"Setup logging configuration\"\"\"\n    level = logging.DEBUG if verbose else logging.INFO\n    logging.basicConfig(\n        level=level,\n        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n        datefmt='%Y-%m-%d %H:%M:%S'\n    )\n\n\ndef print_grid(grid: np.ndarray, title: str = \"\"):\n    \"\"\"Pretty print a grid\"\"\"\n    if title:\n        print(f\"\\n{title}\")\n    \n    # Use colors for better visualization\n    color_map = {\n        0: 'â¬œ',  # White (background)\n        1: 'ðŸŸ¦',  # Blue\n        2: 'ðŸŸ¥',  # Red\n        3: 'ðŸŸ©',  # Green\n        4: 'ðŸŸ¨',  # Yellow\n        5: 'â¬œ',  # Gray (using white)\n        6: 'ðŸŸª',  # Purple\n        7: 'ðŸŸ§',  # Orange\n        8: 'ðŸŸ«',  # Brown\n        9: 'â¬›'   # Black\n    }\n    \n    for row in grid:\n        row_str = ' '.join(color_map.get(int(cell), str(cell)) for cell in row)\n        print(row_str)\n    \n    print(f\"Shape: {grid.shape}\")\n\n\ndef solve_single_task(agent: ARCAgent, \n                     task: ARCTask, \n                     evaluator: SolutionEvaluator,\n                     verbose: bool = False) -> Dict:\n    \"\"\"\n    Solve a single ARC task\n    \n    Args:\n        agent: ARC agent instance\n        task: Task to solve\n        evaluator: Solution evaluator\n        verbose: Print detailed output\n        \n    Returns:\n        Results dictionary\n    \"\"\"\n    \n    print(f\"\\n{'='*70}\")\n    print(f\"Solving Task: {task.task_id}\")\n    print(f\"{'='*70}\")\n    \n    if verbose:\n        print(f\"Training examples: {task.num_train}\")\n        print(f\"Test examples: {task.num_test}\")\n        \n        # Show first training example\n        if task.num_train > 0:\n            train_input, train_output = task.get_train_pair(0)\n            print_grid(train_input, \"First Training Input:\")\n            print_grid(train_output, \"First Training Output:\")\n    \n    # Solve the task\n    start_time = time.time()\n    solution = agent.solve_task(task)\n    solve_time = time.time() - start_time\n    \n    print(f\"\\nSolution found in {solve_time:.2f} seconds\")\n    print(f\"Strategy used: {solution.strategy_used}\")\n    print(f\"Confidence: {solution.confidence:.2%}\")\n    \n    # Evaluate if ground truth available\n    results = {\n        'task_id': task.task_id,\n        'solved': len(solution.test_solutions),\n        'time': solve_time,\n        'confidence': solution.confidence,\n        'strategy': solution.strategy_used\n    }\n    \n    if any(ex.output is not None for ex in task.test_examples):\n        # We have ground truth\n        correct = 0\n        for i, (test_ex, pred) in enumerate(zip(task.test_examples, solution.test_solutions)):\n            if test_ex.output is not None and pred is not None:\n                metrics = evaluator.evaluate_solution(\n                    pred, test_ex.output,\n                    task_id=f\"{task.task_id}_test_{i}\",\n                    solver_used=solution.strategy_used,\n                    execution_time=solve_time,\n                    confidence=solution.confidence\n                )\n                \n                if metrics.is_correct:\n                    correct += 1\n                    print(f\"âœ“ Test {i+1}: CORRECT\")\n                else:\n                    print(f\"âœ— Test {i+1}: INCORRECT (Pixel accuracy: {metrics.pixel_accuracy:.2%})\")\n                \n                if verbose:\n                    print_grid(pred, f\"Test {i+1} Prediction:\")\n                    if test_ex.output is not None:\n                        print_grid(test_ex.output, f\"Test {i+1} Ground Truth:\")\n        \n        accuracy = correct / len(task.test_examples)\n        results['correct'] = correct\n        results['total'] = len(task.test_examples)\n        results['accuracy'] = accuracy\n        \n        print(f\"\\nAccuracy: {correct}/{len(task.test_examples)} ({accuracy:.2%})\")\n    else:\n        print(\"No ground truth available for evaluation\")\n        \n        if verbose:\n            # Show predictions\n            for i, pred in enumerate(solution.test_solutions):\n                if pred is not None:\n                    print_grid(pred, f\"Test {i+1} Prediction:\")\n    \n    return results\n\n\ndef solve_batch(agent: ARCAgent,\n               tasks: List[ARCTask],\n               evaluator: SolutionEvaluator,\n               verbose: bool = False,\n               max_tasks: Optional[int] = None) -> Dict:\n    \"\"\"\n    Solve multiple tasks\n    \n    Args:\n        agent: ARC agent instance\n        tasks: List of tasks to solve\n        evaluator: Solution evaluator\n        verbose: Print detailed output\n        max_tasks: Maximum number of tasks to solve\n        \n    Returns:\n        Batch results\n    \"\"\"\n    \n    if max_tasks:\n        tasks = tasks[:max_tasks]\n    \n    print(f\"\\n{'='*70}\")\n    print(f\"Solving {len(tasks)} tasks...\")\n    print(f\"{'='*70}\")\n    \n    all_results = []\n    total_correct = 0\n    total_examples = 0\n    \n    for i, task in enumerate(tasks, 1):\n        print(f\"\\n[{i}/{len(tasks)}] \", end='')\n        \n        try:\n            results = solve_single_task(agent, task, evaluator, verbose=False)\n            all_results.append(results)\n            \n            if 'correct' in results:\n                total_correct += results['correct']\n                total_examples += results['total']\n        \n        except Exception as e:\n            print(f\"Error solving {task.task_id}: {e}\")\n            all_results.append({\n                'task_id': task.task_id,\n                'error': str(e)\n            })\n    \n    # Summary\n    print(f\"\\n{'='*70}\")\n    print(\"BATCH SUMMARY\")\n    print(f\"{'='*70}\")\n    \n    successful = [r for r in all_results if 'error' not in r]\n    print(f\"Tasks attempted: {len(tasks)}\")\n    print(f\"Tasks completed: {len(successful)}\")\n    \n    if total_examples > 0:\n        print(f\"Total test examples: {total_examples}\")\n        print(f\"Correct predictions: {total_correct}\")\n        print(f\"Overall accuracy: {total_correct/total_examples:.2%}\")\n    \n    # Time statistics\n    if successful:\n        avg_time = np.mean([r['time'] for r in successful])\n        print(f\"Average solve time: {avg_time:.2f}s\")\n    \n    # Strategy breakdown\n    strategies = {}\n    for r in successful:\n        strat = r.get('strategy', 'unknown')\n        strategies[strat] = strategies.get(strat, 0) + 1\n    \n    if strategies:\n        print(\"\\nStrategy usage:\")\n        for strat, count in sorted(strategies.items(), key=lambda x: x[1], reverse=True):\n            print(f\"  {strat}: {count}\")\n    \n    return {\n        'results': all_results,\n        'summary': {\n            'total_tasks': len(tasks),\n            'completed': len(successful),\n            'total_correct': total_correct,\n            'total_examples': total_examples,\n            'accuracy': total_correct/total_examples if total_examples > 0 else 0\n        }\n    }\n\n\ndef save_solutions(solutions: Dict[str, List[List[int]]], output_path: str):\n    \"\"\"Save solutions to JSON file\"\"\"\n    \n    # Convert numpy arrays to lists\n    json_solutions = {}\n    for task_id, outputs in solutions.items():\n        json_solutions[task_id] = [\n            out.tolist() if isinstance(out, np.ndarray) else out\n            for out in outputs\n        ]\n    \n    with open(output_path, 'w') as f:\n        json.dump(json_solutions, f, indent=2)\n    \n    print(f\"Solutions saved to {output_path}\")\n\n\ndef main():\n    \"\"\"Main entry point\"\"\"\n    \n    parser = argparse.ArgumentParser(\n        description=\"ARC Prize 2025 Solver - Command Line Interface\"\n    )\n    \n    parser.add_argument(\n        'command',\n        choices=['solve', 'evaluate', 'demo'],\n        help='Command to execute'\n    )\n    \n    parser.add_argument(\n        '--challenges',\n        type=str,\n        help='Path to challenges JSON file or directory'\n    )\n    \n    parser.add_argument(\n        '--solutions',\n        type=str,\n        help='Path to solutions JSON file (for evaluation)'\n    )\n    \n    parser.add_argument(\n        '--task-id',\n        type=str,\n        help='Specific task ID to solve'\n    )\n    \n    parser.add_argument(\n        '--mode',\n        choices=['fast', 'balanced', 'comprehensive', 'adaptive'],\n        default='balanced',\n        help='Agent solving mode (default: balanced)'\n    )\n    \n    parser.add_argument(\n        '--max-tasks',\n        type=int,\n        help='Maximum number of tasks to solve'\n    )\n    \n    parser.add_argument(\n        '--output',\n        type=str,\n        help='Output file for solutions'\n    )\n    \n    parser.add_argument(\n        '--verbose',\n        action='store_true',\n        help='Verbose output'\n    )\n    \n    args = parser.parse_args()\n    \n    # Setup logging\n    setup_logging(args.verbose)\n    \n    # Initialize components\n    print(\"Initializing ARC Agent...\")\n    mode = AgentMode[args.mode.upper()]\n    agent = ARCAgent(mode=mode)\n    evaluator = SolutionEvaluator()\n    loader = TaskLoader()\n    \n    if args.command == 'demo':\n        # Run demo with built-in example\n        print(\"\\n\" + \"=\"*70)\n        print(\"ARC SOLVER DEMONSTRATION\")\n        print(\"=\"*70)\n        \n        # Try to load from attached files\n        challenges_file = \"attached_assets/arc-agi_training_challenges (2)_1757573784094.txt\"\n        solutions_file = \"attached_assets/arc-agi_training_solutions_1757573784094.txt\"\n        \n        if os.path.exists(challenges_file):\n            print(f\"\\nLoading tasks from: {challenges_file}\")\n            tasks = loader.load_from_json_file(challenges_file)\n            \n            if os.path.exists(solutions_file):\n                print(f\"Loading solutions from: {solutions_file}\")\n                # Load solutions and merge with tasks\n                with open(solutions_file, 'r') as f:\n                    solutions_json = json.load(f)\n                \n                # Merge solutions into tasks\n                for task_id, outputs in solutions_json.items():\n                    if task_id in tasks:\n                        for i, output in enumerate(outputs):\n                            if i < len(tasks[task_id].test_examples):\n                                tasks[task_id].test_examples[i].output = np.array(output, dtype=np.int8)\n            \n            # Select a few tasks for demo\n            demo_task_ids = ['00576224', '0692e18c', '007bbfb7']  # Simple to moderate\n            demo_tasks = []\n            \n            for task_id in demo_task_ids:\n                if task_id in tasks:\n                    demo_tasks.append(tasks[task_id])\n            \n            if demo_tasks:\n                print(f\"\\nRunning demo on {len(demo_tasks)} tasks...\")\n                results = solve_batch(agent, demo_tasks, evaluator, verbose=args.verbose)\n            else:\n                print(\"Demo tasks not found in loaded data\")\n        else:\n            print(\"Demo files not found. Please ensure training data is available.\")\n            \n            # Create a simple demo task\n            print(\"\\nCreating synthetic demo task...\")\n            from src.arc.task_loader import ARCTask, ARCExample\n            \n            # Simple scaling task\n            demo_task = ARCTask(\n                task_id=\"demo_scale\",\n                train_examples=[\n                    ARCExample(\n                        input=np.array([[1, 2], [3, 4]]),\n                        output=np.array([[1, 1, 2, 2], \n                                        [1, 1, 2, 2],\n                                        [3, 3, 4, 4],\n                                        [3, 3, 4, 4]])\n                    )\n                ],\n                test_examples=[\n                    ARCExample(\n                        input=np.array([[5, 6], [7, 8]]),\n                        output=np.array([[5, 5, 6, 6],\n                                        [5, 5, 6, 6],\n                                        [7, 7, 8, 8],\n                                        [7, 7, 8, 8]])\n                    )\n                ]\n            )\n            \n            results = solve_single_task(agent, demo_task, evaluator, verbose=True)\n    \n    elif args.command == 'solve':\n        if not args.challenges:\n            print(\"Error: --challenges required for solve command\")\n            sys.exit(1)\n        \n        # Load tasks\n        print(f\"Loading tasks from: {args.challenges}\")\n        tasks = loader.load_from_json_file(args.challenges)\n        \n        if args.solutions:\n            print(f\"Loading ground truth from: {args.solutions}\")\n            # Load solutions and merge with tasks\n            with open(args.solutions, 'r') as f:\n                solutions_json = json.load(f)\n            \n            # Merge solutions into tasks\n            for task_id, outputs in solutions_json.items():\n                if task_id in tasks:\n                    for i, output in enumerate(outputs):\n                        if i < len(tasks[task_id].test_examples):\n                            tasks[task_id].test_examples[i].output = np.array(output, dtype=np.int8)\n        \n        # Filter by task ID if specified\n        if args.task_id:\n            if args.task_id in tasks:\n                task = tasks[args.task_id]\n                results = solve_single_task(agent, task, evaluator, verbose=args.verbose)\n                \n                # Save solution if requested\n                if args.output:\n                    solution = agent.solutions_cache.get(args.task_id)\n                    if solution:\n                        save_solutions(\n                            {args.task_id: solution.test_solutions},\n                            args.output\n                        )\n            else:\n                print(f\"Task {args.task_id} not found\")\n                sys.exit(1)\n        else:\n            # Solve all or limited tasks\n            task_list = list(tasks.values())\n            results = solve_batch(\n                agent, task_list, evaluator,\n                verbose=args.verbose,\n                max_tasks=args.max_tasks\n            )\n            \n            # Save solutions if requested\n            if args.output:\n                all_solutions = {}\n                for task_id, solution in agent.solutions_cache.items():\n                    all_solutions[task_id] = solution.test_solutions\n                save_solutions(all_solutions, args.output)\n    \n    elif args.command == 'evaluate':\n        if not args.challenges or not args.solutions:\n            print(\"Error: Both --challenges and --solutions required for evaluate command\")\n            sys.exit(1)\n        \n        print(f\"Loading tasks from: {args.challenges}\")\n        tasks = loader.load_from_json_file(args.challenges)\n        \n        print(f\"Loading solutions from: {args.solutions}\")\n        solutions = {}\n        with open(args.solutions, 'r') as f:\n            solutions = json.load(f)\n        \n        # Evaluate solutions\n        print(\"\\nEvaluating solutions...\")\n        results = []\n        \n        for task_id, task in tasks.items():\n            if task_id not in solutions:\n                continue\n            \n            task_solutions = solutions[task_id]\n            for i, (test_ex, pred) in enumerate(zip(task.test_examples, task_solutions)):\n                if test_ex.output is not None:\n                    pred_array = np.array(pred)\n                    metrics = evaluator.evaluate_solution(\n                        pred_array, test_ex.output,\n                        task_id=f\"{task_id}_test_{i}\"\n                    )\n                    results.append(metrics)\n                    \n                    if args.verbose:\n                        print(evaluator.generate_report(metrics))\n        \n        # Summary\n        summary = evaluator.get_summary_stats()\n        print(f\"\\n{'='*70}\")\n        print(\"EVALUATION SUMMARY\")\n        print(f\"{'='*70}\")\n        print(f\"Total evaluations: {summary.get('total_evaluations', 0)}\")\n        print(f\"Correct: {summary.get('correct', 0)}\")\n        print(f\"Accuracy: {summary.get('accuracy', 0):.2%}\")\n        print(f\"Average pixel accuracy: {summary.get('average_pixel_accuracy', 0):.2%}\")\n    \n    # Print final statistics\n    print(f\"\\n{'='*70}\")\n    print(\"Agent Performance Statistics:\")\n    stats = agent.get_performance_stats()\n    for key, value in stats.items():\n        if key != 'solver_accuracy':\n            print(f\"  {key}: {value}\")\n    \n    print(\"\\nThank you for using ARC Solver!\")\n\n\nif __name__ == \"__main__\":\n    main()","size_bytes":16938},"replit.md":{"content":"# ARC Prize 2025 Solver\n\n## Overview\nThis project is a Python-based solver for the ARC (Abstraction and Reasoning Corpus) Prize 2025. The system is designed to tackle abstract reasoning puzzles using a combination of constraint satisfaction, pattern recognition, and various AI techniques.\n\n**Purpose**: Develop algorithms to solve ARC puzzles by identifying patterns and transformations between input and output grids.\n\n**Current State**: Environment setup complete with all required libraries installed and project structure initialized.\n\n## Recent Changes\n- **2025-09-11**: Initial project setup\n  - Installed Python 3.11 environment\n  - Added scientific computing libraries (NumPy, SciPy, Matplotlib, scikit-learn)\n  - Added specialized libraries (OR-Tools for constraint programming, NetworkX for graph algorithms, Pillow for image processing)\n  - Created organized project structure with dedicated folders for different components\n\n## Project Architecture\n\n### Directory Structure\n```\n.\nâ”œâ”€â”€ src/                    # Source code\nâ”‚   â”œâ”€â”€ arc/               # ARC-specific algorithms and transformations\nâ”‚   â”œâ”€â”€ csp/               # Constraint satisfaction problem solvers\nâ”‚   â”œâ”€â”€ solvers/           # Main solver implementations\nâ”‚   â””â”€â”€ utils/             # Utility functions and helpers\nâ”œâ”€â”€ data/                  # Data directory for ARC puzzles\nâ”œâ”€â”€ tests/                 # Test suite\nâ”œâ”€â”€ main.py                # Main entry point\nâ””â”€â”€ pyproject.toml         # Project configuration\n```\n\n### Key Components\n1. **ARC Module** (`src/arc/`): Handles ARC-specific logic, grid transformations, and pattern recognition\n2. **CSP Module** (`src/csp/`): Implements constraint satisfaction algorithms for solving puzzles\n3. **Solvers** (`src/solvers/`): Contains different solver strategies and approaches\n4. **Utils** (`src/utils/`): Helper functions for data processing, visualization, and common operations\n\n### Technology Stack\n- **Python 3.11**: Core programming language\n- **NumPy**: Numerical computations and array operations\n- **SciPy**: Scientific computing and optimization\n- **scikit-learn**: Machine learning algorithms\n- **Matplotlib**: Visualization and plotting\n- **OR-Tools**: Constraint programming and optimization\n- **NetworkX**: Graph algorithms and network analysis\n- **Pillow**: Image processing and manipulation\n\n## Development Notes\n\n### Running the Project\nExecute `python main.py` to verify the environment setup and see installed libraries.\n\n### Next Steps\n1. Implement data loading utilities for ARC puzzles\n2. Develop basic grid transformation functions\n3. Create constraint satisfaction problem formulations\n4. Build pattern recognition algorithms\n5. Implement solver strategies\n6. Add visualization tools for debugging\n7. Create comprehensive test suite\n\n### Design Decisions\n- **Modular Architecture**: Separated concerns into distinct modules for maintainability\n- **Multiple Solver Strategies**: Support for different approaches (CSP, pattern matching, ML-based)\n- **Extensible Framework**: Easy to add new transformation rules and solving strategies\n\n## User Preferences\n- Clean, well-documented code with type hints where appropriate\n- Modular design with clear separation of concerns\n- Focus on algorithmic efficiency for puzzle solving\n- Comprehensive testing for all components","size_bytes":3376},"src/__init__.py":{"content":"\"\"\"ARC Prize 2025 Solver - Core Package\"\"\"\n\n__version__ = \"0.1.0\"","size_bytes":65},"src/arc_agent.py":{"content":"\"\"\"\nARC Agent - Main orchestrator for solving ARC Prize 2025 tasks\nIntegrates multiple solvers and strategies for robust solutions\n\"\"\"\n\nimport numpy as np\nfrom typing import List, Dict, Optional, Tuple, Any\nfrom dataclasses import dataclass, field\nimport time\nimport logging\nfrom enum import Enum\n\nfrom .arc.task_loader import ARCTask, ARCExample\nfrom .arc.grid_operations import Grid\nfrom .strategies.strategy_selector import StrategySelector, TaskCategory\nfrom .solvers.ensemble_solver import EnsembleSolver, SolverType\nfrom .solvers.program_synthesis import ProgramSynthesisEngine\nfrom .solvers.csp_solver import CSPSolver\nfrom .solvers.pattern_solver import PatternSolver\n\n\nclass AgentMode(Enum):\n    \"\"\"Operating modes for the agent\"\"\"\n    FAST = \"fast\"  # Quick solving with basic strategies\n    BALANCED = \"balanced\"  # Balance between speed and accuracy\n    COMPREHENSIVE = \"comprehensive\"  # Try all strategies thoroughly\n    ADAPTIVE = \"adaptive\"  # Adapt strategy based on task analysis\n\n\n@dataclass\nclass SolutionAttempt:\n    \"\"\"Records a solution attempt\"\"\"\n    task_id: str\n    solution: Optional[np.ndarray]\n    confidence: float\n    solver_used: str\n    execution_time: float\n    metadata: Dict[str, Any] = field(default_factory=dict)\n    success: bool = False\n\n\n@dataclass\nclass TaskSolution:\n    \"\"\"Complete solution for a task\"\"\"\n    task_id: str\n    test_solutions: List[np.ndarray]\n    attempts: List[SolutionAttempt]\n    total_time: float\n    strategy_used: str\n    confidence: float\n    metadata: Dict[str, Any] = field(default_factory=dict)\n\n\nclass ARCAgent:\n    \"\"\"\n    Main ARC Agent that orchestrates solving strategies\n    \"\"\"\n    \n    def __init__(self, \n                 mode: AgentMode = AgentMode.BALANCED,\n                 config: Optional[Dict[str, Any]] = None):\n        \"\"\"\n        Initialize ARC Agent\n        \n        Args:\n            mode: Operating mode for the agent\n            config: Configuration dictionary\n        \"\"\"\n        self.mode = mode\n        self.config = config or {}\n        \n        # Initialize components\n        self.strategy_selector = StrategySelector()\n        self.ensemble_solver = None\n        self._initialize_solvers()\n        \n        # Performance tracking\n        self.solutions_cache = {}\n        self.performance_stats = {\n            'tasks_attempted': 0,\n            'tasks_solved': 0,\n            'average_time': 0,\n            'solver_accuracy': {}\n        }\n        \n        # Setup logging\n        self.logger = logging.getLogger(__name__)\n        \n    def _initialize_solvers(self):\n        \"\"\"Initialize solver ensemble based on mode\"\"\"\n        \n        # Configure solvers based on mode\n        if self.mode == AgentMode.FAST:\n            solver_config = {\n                'timeout_per_solver': 5.0,\n                'solvers': [SolverType.PATTERN]\n            }\n        elif self.mode == AgentMode.BALANCED:\n            solver_config = {\n                'timeout_per_solver': 10.0,\n                'solvers': [SolverType.PATTERN, SolverType.PROGRAM_SYNTHESIS]\n            }\n        elif self.mode == AgentMode.COMPREHENSIVE:\n            solver_config = {\n                'timeout_per_solver': 20.0,\n                'solvers': [SolverType.PATTERN, SolverType.PROGRAM_SYNTHESIS, SolverType.CSP]\n            }\n        else:  # ADAPTIVE\n            solver_config = {\n                'timeout_per_solver': 15.0,\n                'solvers': [SolverType.PATTERN, SolverType.PROGRAM_SYNTHESIS, SolverType.CSP],\n                'adaptive': True\n            }\n        \n        # Merge with user config\n        solver_config.update(self.config.get('solver_config', {}))\n        \n        # Create ensemble solver\n        self.ensemble_solver = EnsembleSolver(solver_config)\n    \n    def solve_task(self, task: ARCTask) -> TaskSolution:\n        \"\"\"\n        Solve a complete ARC task\n        \n        Args:\n            task: ARC task to solve\n            \n        Returns:\n            TaskSolution with results\n        \"\"\"\n        start_time = time.time()\n        self.performance_stats['tasks_attempted'] += 1\n        \n        # Check cache\n        if task.task_id in self.solutions_cache:\n            self.logger.info(f\"Using cached solution for task {task.task_id}\")\n            return self.solutions_cache[task.task_id]\n        \n        # Prepare training data\n        train_inputs = [ex.input for ex in task.train_examples]\n        train_outputs = [ex.output for ex in task.train_examples]\n        \n        # Analyze task and select strategy\n        recommendation = self.strategy_selector.select_strategy(\n            train_inputs, train_outputs\n        )\n        \n        self.logger.info(f\"Task {task.task_id}: Using strategy {recommendation.primary_strategy}\")\n        self.logger.info(f\"Expected difficulty: {recommendation.expected_difficulty.name}\")\n        \n        # Configure ensemble based on recommendation\n        if self.mode == AgentMode.ADAPTIVE:\n            self._adapt_solvers(recommendation)\n        \n        # Solve test examples\n        test_solutions = []\n        attempts = []\n        \n        for i, test_example in enumerate(task.test_examples):\n            self.logger.info(f\"Solving test example {i+1}/{len(task.test_examples)}\")\n            \n            # Try to solve\n            attempt_start = time.time()\n            \n            try:\n                # Use ensemble solver\n                result = self.ensemble_solver.solve(\n                    train_inputs,\n                    train_outputs,\n                    test_example.input\n                )\n                \n                if result.final_solution is not None:\n                    test_solutions.append(result.final_solution)\n                    success = True\n                    confidence = result.consensus_score\n                    solver_used = result.strategy_used\n                else:\n                    # Fallback: try simple pattern matching\n                    self.logger.warning(f\"Ensemble failed, trying fallback for test {i+1}\")\n                    fallback_solution = self._fallback_solve(\n                        train_inputs, train_outputs, test_example.input\n                    )\n                    test_solutions.append(fallback_solution)\n                    success = fallback_solution is not None\n                    confidence = 0.3 if success else 0.0\n                    solver_used = \"fallback\"\n                \n            except Exception as e:\n                self.logger.error(f\"Error solving test {i+1}: {e}\")\n                test_solutions.append(None)\n                success = False\n                confidence = 0.0\n                solver_used = \"error\"\n            \n            attempt_time = time.time() - attempt_start\n            \n            # Record attempt\n            attempt = SolutionAttempt(\n                task_id=task.task_id,\n                solution=test_solutions[-1],\n                confidence=confidence,\n                solver_used=solver_used,\n                execution_time=attempt_time,\n                success=success\n            )\n            attempts.append(attempt)\n        \n        # Create solution\n        total_time = time.time() - start_time\n        overall_confidence = np.mean([a.confidence for a in attempts])\n        \n        solution = TaskSolution(\n            task_id=task.task_id,\n            test_solutions=test_solutions,\n            attempts=attempts,\n            total_time=total_time,\n            strategy_used=recommendation.primary_strategy,\n            confidence=overall_confidence,\n            metadata={\n                'recommendation': recommendation,\n                'mode': self.mode.value,\n                'num_test_examples': len(task.test_examples)\n            }\n        )\n        \n        # Update stats\n        if all(s is not None for s in test_solutions):\n            self.performance_stats['tasks_solved'] += 1\n        \n        self.performance_stats['average_time'] = (\n            (self.performance_stats['average_time'] * \n             (self.performance_stats['tasks_attempted'] - 1) + total_time) /\n            self.performance_stats['tasks_attempted']\n        )\n        \n        # Cache solution\n        self.solutions_cache[task.task_id] = solution\n        \n        return solution\n    \n    def _adapt_solvers(self, recommendation):\n        \"\"\"Adapt solver configuration based on task analysis\"\"\"\n        \n        # Adjust timeout based on expected difficulty\n        difficulty_timeouts = {\n            1: 5.0,   # TRIVIAL\n            2: 8.0,   # SIMPLE\n            3: 12.0,  # MODERATE\n            4: 18.0,  # COMPLEX\n            5: 25.0   # VERY_COMPLEX\n        }\n        \n        timeout = difficulty_timeouts.get(\n            recommendation.expected_difficulty.value, 15.0\n        )\n        \n        # Select solvers based on task category\n        if recommendation.primary_strategy == \"pattern_matching\":\n            solvers = [SolverType.PATTERN]\n        elif recommendation.primary_strategy == \"program_synthesis\":\n            solvers = [SolverType.PROGRAM_SYNTHESIS, SolverType.PATTERN]\n        elif recommendation.primary_strategy == \"constraint_satisfaction\":\n            solvers = [SolverType.CSP, SolverType.PATTERN]\n        else:\n            solvers = [SolverType.PATTERN, SolverType.PROGRAM_SYNTHESIS]\n        \n        # Reconfigure ensemble\n        new_config = {\n            'timeout_per_solver': timeout,\n            'solvers': solvers\n        }\n        self.ensemble_solver = EnsembleSolver(new_config)\n    \n    def _fallback_solve(self, \n                       train_inputs: List[np.ndarray],\n                       train_outputs: List[np.ndarray],\n                       test_input: np.ndarray) -> Optional[np.ndarray]:\n        \"\"\"\n        Simple fallback solver for when main strategies fail\n        \"\"\"\n        try:\n            # Strategy 1: If all outputs are same, return first output\n            if all(np.array_equal(train_outputs[0], out) for out in train_outputs[1:]):\n                self.logger.info(\"Fallback: All outputs identical\")\n                return train_outputs[0].copy()\n            \n            # Strategy 2: If all inputs same size as outputs, check for simple transform\n            if all(inp.shape == out.shape for inp, out in zip(train_inputs, train_outputs)):\n                # Check for simple color mapping\n                color_maps = []\n                for inp, out in zip(train_inputs, train_outputs):\n                    if inp.shape == out.shape:\n                        color_map = {}\n                        for i in range(inp.shape[0]):\n                            for j in range(inp.shape[1]):\n                                color_map[inp[i,j]] = out[i,j]\n                        color_maps.append(color_map)\n                \n                # If consistent color mapping\n                if all(cm == color_maps[0] for cm in color_maps[1:]):\n                    self.logger.info(\"Fallback: Simple color mapping\")\n                    result = test_input.copy()\n                    for i in range(result.shape[0]):\n                        for j in range(result.shape[1]):\n                            if result[i,j] in color_maps[0]:\n                                result[i,j] = color_maps[0][result[i,j]]\n                    return result\n            \n            # Strategy 3: Check for scaling\n            scale_factors = []\n            for inp, out in zip(train_inputs, train_outputs):\n                if (out.shape[0] % inp.shape[0] == 0 and \n                    out.shape[1] % inp.shape[1] == 0):\n                    h_scale = out.shape[0] // inp.shape[0]\n                    w_scale = out.shape[1] // inp.shape[1]\n                    if h_scale == w_scale:\n                        # Verify it's actually scaled\n                        scaled = np.repeat(np.repeat(inp, h_scale, axis=0), w_scale, axis=1)\n                        if np.array_equal(scaled, out):\n                            scale_factors.append(h_scale)\n            \n            if scale_factors and all(s == scale_factors[0] for s in scale_factors):\n                self.logger.info(f\"Fallback: Scaling by {scale_factors[0]}\")\n                factor = scale_factors[0]\n                return np.repeat(np.repeat(test_input, factor, axis=0), factor, axis=1)\n            \n        except Exception as e:\n            self.logger.error(f\"Fallback solver error: {e}\")\n        \n        return None\n    \n    def evaluate_solution(self, \n                         task: ARCTask,\n                         solution: TaskSolution) -> Dict[str, Any]:\n        \"\"\"\n        Evaluate a solution against ground truth if available\n        \n        Args:\n            task: Original task\n            solution: Generated solution\n            \n        Returns:\n            Evaluation metrics\n        \"\"\"\n        metrics = {\n            'task_id': task.task_id,\n            'num_test_examples': len(task.test_examples),\n            'solutions_found': sum(1 for s in solution.test_solutions if s is not None),\n            'confidence': solution.confidence,\n            'execution_time': solution.total_time\n        }\n        \n        # If we have ground truth\n        correct = 0\n        for i, (test_ex, sol) in enumerate(zip(task.test_examples, solution.test_solutions)):\n            if test_ex.output is not None and sol is not None:\n                if np.array_equal(test_ex.output, sol):\n                    correct += 1\n                    self.logger.info(f\"Test {i+1}: Correct!\")\n                else:\n                    self.logger.info(f\"Test {i+1}: Incorrect\")\n        \n        if any(ex.output is not None for ex in task.test_examples):\n            metrics['accuracy'] = correct / len(task.test_examples)\n            metrics['correct'] = correct\n        \n        return metrics\n    \n    def get_performance_stats(self) -> Dict[str, Any]:\n        \"\"\"Get agent performance statistics\"\"\"\n        return self.performance_stats.copy()\n    \n    def reset_cache(self):\n        \"\"\"Clear solution cache\"\"\"\n        self.solutions_cache.clear()\n        self.logger.info(\"Solution cache cleared\")","size_bytes":14078},"tests/test_arc_operations.py":{"content":"\"\"\"\nComprehensive tests for ARC operations modules\n\"\"\"\n\nimport unittest\nimport numpy as np\nimport json\nimport sys\nimport os\n\n# Add parent directory to path for imports\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nfrom src.arc.task_loader import TaskLoader, ARCTask, ARCExample\nfrom src.arc.grid_operations import Grid\nfrom src.arc.pattern_detector import PatternDetector\nfrom src.arc.transformation_rules import (\n    TransformationRule, TransformationType, RuleChain, \n    TransformationInference, RuleLibrary\n)\n\n\nclass TestTaskLoader(unittest.TestCase):\n    \"\"\"Test the TaskLoader class\"\"\"\n    \n    def setUp(self):\n        self.loader = TaskLoader()\n        \n        # Sample task data\n        self.sample_data = {\n            \"test_task\": {\n                \"train\": [\n                    {\"input\": [[1, 2], [3, 4]], \"output\": [[4, 3], [2, 1]]},\n                    {\"input\": [[5, 6], [7, 8]], \"output\": [[8, 7], [6, 5]]}\n                ],\n                \"test\": [\n                    {\"input\": [[9, 0], [1, 2]]}\n                ]\n            }\n        }\n    \n    def test_load_from_json_string(self):\n        \"\"\"Test loading tasks from JSON string\"\"\"\n        tasks = self.loader.load_from_json_string(json.dumps(self.sample_data))\n        \n        self.assertEqual(len(tasks), 1)\n        self.assertIn(\"test_task\", tasks)\n        \n        task = tasks[\"test_task\"]\n        self.assertEqual(task.num_train, 2)\n        self.assertEqual(task.num_test, 1)\n    \n    def test_arc_example(self):\n        \"\"\"Test ARCExample dataclass\"\"\"\n        example = ARCExample(\n            input=[[1, 2], [3, 4]],\n            output=[[4, 3], [2, 1]]\n        )\n        \n        self.assertEqual(example.input_shape, (2, 2))\n        self.assertEqual(example.output_shape, (2, 2))\n        self.assertTrue(isinstance(example.input, np.ndarray))\n    \n    def test_task_validation(self):\n        \"\"\"Test task validation\"\"\"\n        task = self.loader.get_task(\"test_task\")\n        if task:\n            self.assertTrue(task.validate())\n    \n    def test_get_statistics(self):\n        \"\"\"Test statistics generation\"\"\"\n        self.loader.load_from_json_string(json.dumps(self.sample_data))\n        stats = self.loader.get_statistics()\n        \n        self.assertEqual(stats['total_tasks'], 1)\n        self.assertEqual(stats['total_train_examples'], 2)\n        self.assertEqual(stats['total_test_examples'], 1)\n\n\nclass TestGridOperations(unittest.TestCase):\n    \"\"\"Test the Grid class and operations\"\"\"\n    \n    def setUp(self):\n        self.grid = Grid(np.array([\n            [1, 2, 3],\n            [4, 5, 6],\n            [7, 8, 9]\n        ]))\n    \n    def test_grid_creation(self):\n        \"\"\"Test grid creation and properties\"\"\"\n        self.assertEqual(self.grid.shape, (3, 3))\n        self.assertEqual(self.grid.height, 3)\n        self.assertEqual(self.grid.width, 3)\n        self.assertEqual(self.grid.unique_colors, {1, 2, 3, 4, 5, 6, 7, 8, 9})\n    \n    def test_rotation(self):\n        \"\"\"Test grid rotation\"\"\"\n        rotated_90 = self.grid.rotate(90)\n        expected = np.array([\n            [7, 4, 1],\n            [8, 5, 2],\n            [9, 6, 3]\n        ])\n        np.testing.assert_array_equal(rotated_90.data, expected)\n        \n        rotated_180 = self.grid.rotate(180)\n        expected_180 = np.array([\n            [9, 8, 7],\n            [6, 5, 4],\n            [3, 2, 1]\n        ])\n        np.testing.assert_array_equal(rotated_180.data, expected_180)\n    \n    def test_flip(self):\n        \"\"\"Test grid flipping\"\"\"\n        flipped_h = self.grid.flip_horizontal()\n        expected_h = np.array([\n            [3, 2, 1],\n            [6, 5, 4],\n            [9, 8, 7]\n        ])\n        np.testing.assert_array_equal(flipped_h.data, expected_h)\n        \n        flipped_v = self.grid.flip_vertical()\n        expected_v = np.array([\n            [7, 8, 9],\n            [4, 5, 6],\n            [1, 2, 3]\n        ])\n        np.testing.assert_array_equal(flipped_v.data, expected_v)\n    \n    def test_transpose(self):\n        \"\"\"Test grid transpose\"\"\"\n        transposed = self.grid.transpose()\n        expected = np.array([\n            [1, 4, 7],\n            [2, 5, 8],\n            [3, 6, 9]\n        ])\n        np.testing.assert_array_equal(transposed.data, expected)\n    \n    def test_scale(self):\n        \"\"\"Test grid scaling\"\"\"\n        small_grid = Grid(np.array([\n            [1, 2],\n            [3, 4]\n        ]))\n        scaled = small_grid.scale(2)\n        expected = np.array([\n            [1, 1, 2, 2],\n            [1, 1, 2, 2],\n            [3, 3, 4, 4],\n            [3, 3, 4, 4]\n        ])\n        np.testing.assert_array_equal(scaled.data, expected)\n    \n    def test_tile(self):\n        \"\"\"Test grid tiling\"\"\"\n        small_grid = Grid(np.array([\n            [1, 2],\n            [3, 4]\n        ]))\n        tiled = small_grid.tile(2, 2)\n        expected = np.array([\n            [1, 2, 1, 2],\n            [3, 4, 3, 4],\n            [1, 2, 1, 2],\n            [3, 4, 3, 4]\n        ])\n        np.testing.assert_array_equal(tiled.data, expected)\n    \n    def test_color_operations(self):\n        \"\"\"Test color operations\"\"\"\n        # Replace color\n        replaced = self.grid.replace_color(5, 0)\n        self.assertEqual(replaced.data[1, 1], 0)\n        \n        # Map colors\n        color_map = {1: 9, 9: 1}\n        mapped = self.grid.map_colors(color_map)\n        self.assertEqual(mapped.data[0, 0], 9)\n        self.assertEqual(mapped.data[2, 2], 1)\n        \n        # Filter color\n        filtered = self.grid.filter_color(5, background=0)\n        self.assertEqual(filtered.data[1, 1], 5)\n        self.assertEqual(filtered.data[0, 0], 0)\n        \n        # Count colors\n        counts = self.grid.count_colors()\n        self.assertEqual(len(counts), 9)\n        self.assertEqual(counts[5], 1)\n    \n    def test_subgrid_operations(self):\n        \"\"\"Test subgrid extraction and replacement\"\"\"\n        # Extract subgrid\n        subgrid = self.grid.extract_subgrid(1, 1, 2, 2)\n        expected = np.array([\n            [5, 6],\n            [8, 9]\n        ])\n        np.testing.assert_array_equal(subgrid.data, expected)\n        \n        # Replace subgrid\n        new_subgrid = Grid(np.array([\n            [0, 0],\n            [0, 0]\n        ]))\n        replaced = self.grid.replace_subgrid(new_subgrid, 1, 1)\n        self.assertEqual(replaced.data[1, 1], 0)\n        self.assertEqual(replaced.data[2, 2], 0)\n    \n    def test_pattern_finding(self):\n        \"\"\"Test pattern finding\"\"\"\n        grid_with_pattern = Grid(np.array([\n            [1, 2, 1, 2],\n            [3, 4, 3, 4],\n            [1, 2, 1, 2],\n            [3, 4, 3, 4]\n        ]))\n        \n        pattern = Grid(np.array([\n            [1, 2],\n            [3, 4]\n        ]))\n        \n        positions = grid_with_pattern.find_pattern(pattern)\n        self.assertEqual(len(positions), 4)\n        self.assertIn((0, 0), positions)\n        self.assertIn((0, 2), positions)\n        self.assertIn((2, 0), positions)\n        self.assertIn((2, 2), positions)\n    \n    def test_bounding_box(self):\n        \"\"\"Test bounding box detection\"\"\"\n        sparse_grid = Grid(np.array([\n            [0, 0, 0, 0],\n            [0, 1, 2, 0],\n            [0, 3, 4, 0],\n            [0, 0, 0, 0]\n        ]))\n        \n        bbox = sparse_grid.get_bounding_box()\n        self.assertEqual(bbox, (1, 1, 3, 3))\n        \n        cropped = sparse_grid.crop_to_content()\n        self.assertEqual(cropped.shape, (2, 2))\n    \n    def test_padding(self):\n        \"\"\"Test grid padding\"\"\"\n        small_grid = Grid(np.array([\n            [1, 2],\n            [3, 4]\n        ]))\n        \n        padded = small_grid.pad(1, 1, 1, 1, fill_value=0)\n        expected = np.array([\n            [0, 0, 0, 0],\n            [0, 1, 2, 0],\n            [0, 3, 4, 0],\n            [0, 0, 0, 0]\n        ])\n        np.testing.assert_array_equal(padded.data, expected)\n\n\nclass TestPatternDetector(unittest.TestCase):\n    \"\"\"Test the PatternDetector class\"\"\"\n    \n    def test_symmetry_detection(self):\n        \"\"\"Test symmetry detection\"\"\"\n        # Horizontal symmetry\n        h_symmetric = Grid(np.array([\n            [1, 2, 1],\n            [3, 4, 3],\n            [5, 6, 5]\n        ]))\n        detector = PatternDetector(h_symmetric)\n        self.assertTrue(detector.has_horizontal_symmetry())\n        \n        # Vertical symmetry\n        v_symmetric = Grid(np.array([\n            [1, 2, 3],\n            [4, 5, 6],\n            [1, 2, 3]\n        ]))\n        detector = PatternDetector(v_symmetric)\n        self.assertTrue(detector.has_vertical_symmetry())\n        \n        # Diagonal symmetry\n        d_symmetric = Grid(np.array([\n            [1, 2, 3],\n            [2, 5, 6],\n            [3, 6, 9]\n        ]))\n        detector = PatternDetector(d_symmetric)\n        self.assertTrue(detector.has_diagonal_symmetry())\n    \n    def test_periodicity_detection(self):\n        \"\"\"Test periodicity detection\"\"\"\n        periodic_grid = Grid(np.array([\n            [1, 2, 1, 2],\n            [3, 4, 3, 4],\n            [1, 2, 1, 2],\n            [3, 4, 3, 4]\n        ]))\n        \n        detector = PatternDetector(periodic_grid)\n        periodicity = detector.detect_periodicity()\n        \n        self.assertEqual(periodicity['horizontal'], 2)\n        self.assertEqual(periodicity['vertical'], 2)\n    \n    def test_shape_detection(self):\n        \"\"\"Test rectangle and line detection\"\"\"\n        grid_with_shapes = Grid(np.array([\n            [0, 1, 1, 1, 0],\n            [0, 1, 1, 1, 0],\n            [0, 0, 0, 0, 0],\n            [2, 2, 2, 2, 2],\n            [0, 0, 0, 0, 0]\n        ]))\n        \n        detector = PatternDetector(grid_with_shapes)\n        \n        # Find rectangles\n        rectangles = detector.find_rectangles()\n        self.assertTrue(len(rectangles) > 0)\n        \n        # Find lines\n        lines = detector.find_lines(min_length=3)\n        self.assertTrue(len(lines) > 0)\n        \n        # Check for horizontal line of 2s\n        has_horizontal_line = any(\n            line['type'] == 'horizontal' and line['color'] == 2 \n            for line in lines\n        )\n        self.assertTrue(has_horizontal_line)\n    \n    def test_connected_components(self):\n        \"\"\"Test connected component detection\"\"\"\n        grid_with_components = Grid(np.array([\n            [1, 1, 0, 2, 2],\n            [1, 0, 0, 0, 2],\n            [0, 0, 3, 0, 0],\n            [4, 0, 3, 3, 0],\n            [4, 4, 0, 0, 0]\n        ]))\n        \n        detector = PatternDetector(grid_with_components)\n        components = detector.find_connected_components(connectivity=4)\n        \n        # Should find 4 components (one for each color)\n        self.assertEqual(len(components), 4)\n        \n        # Check component properties\n        for component in components:\n            self.assertIn('color', component)\n            self.assertIn('area', component)\n            self.assertIn('centroid', component)\n            self.assertIn('bbox', component)\n\n\nclass TestTransformationRules(unittest.TestCase):\n    \"\"\"Test the transformation rules system\"\"\"\n    \n    def setUp(self):\n        self.grid = Grid(np.array([\n            [1, 2],\n            [3, 4]\n        ]))\n    \n    def test_transformation_rule_application(self):\n        \"\"\"Test applying transformation rules\"\"\"\n        # Test rotation rule\n        rotate_rule = TransformationRule(\n            name=\"rotate_90\",\n            transformation_type=TransformationType.ROTATE,\n            parameters={'degrees': 90},\n            description=\"Rotate 90 degrees\"\n        )\n        \n        rotated = rotate_rule.apply(self.grid)\n        expected = np.array([\n            [3, 1],\n            [4, 2]\n        ])\n        np.testing.assert_array_equal(rotated.data, expected)\n        \n        # Test scale rule\n        scale_rule = TransformationRule(\n            name=\"scale_2x\",\n            transformation_type=TransformationType.SCALE,\n            parameters={'factor': 2},\n            description=\"Scale by factor 2\"\n        )\n        \n        scaled = scale_rule.apply(self.grid)\n        self.assertEqual(scaled.shape, (4, 4))\n    \n    def test_rule_chain(self):\n        \"\"\"Test chaining multiple rules\"\"\"\n        chain = RuleChain([\n            TransformationRule(\n                name=\"flip_h\",\n                transformation_type=TransformationType.FLIP,\n                parameters={'direction': 'horizontal'},\n                description=\"Flip horizontally\"\n            ),\n            TransformationRule(\n                name=\"flip_v\",\n                transformation_type=TransformationType.FLIP,\n                parameters={'direction': 'vertical'},\n                description=\"Flip vertically\"\n            )\n        ])\n        \n        result = chain.apply(self.grid)\n        expected = np.array([\n            [4, 3],\n            [2, 1]\n        ])\n        np.testing.assert_array_equal(result.data, expected)\n    \n    def test_transformation_inference(self):\n        \"\"\"Test inferring transformations from examples\"\"\"\n        # Create example with rotation\n        input_grid = Grid(np.array([\n            [1, 2],\n            [3, 4]\n        ]))\n        \n        output_grid = Grid(np.array([\n            [3, 1],\n            [4, 2]\n        ]))\n        \n        rule = TransformationInference.infer_simple_transformation(input_grid, output_grid)\n        \n        self.assertIsNotNone(rule)\n        self.assertEqual(rule.transformation_type, TransformationType.ROTATE)\n        self.assertEqual(rule.parameters['degrees'], 90)\n    \n    def test_rule_library(self):\n        \"\"\"Test the rule library\"\"\"\n        # Get rotation rules\n        rotation_rules = RuleLibrary.get_rotation_rules()\n        self.assertEqual(len(rotation_rules), 3)\n        \n        # Get flip rules\n        flip_rules = RuleLibrary.get_flip_rules()\n        self.assertEqual(len(flip_rules), 2)\n        \n        # Get scale rule\n        scale_rule = RuleLibrary.get_scale_rule(3)\n        self.assertEqual(scale_rule.parameters['factor'], 3)\n        \n        # Test color swap\n        grid_with_colors = Grid(np.array([\n            [1, 2, 1],\n            [2, 1, 2],\n            [1, 2, 1]\n        ]))\n        \n        swap_chain = RuleLibrary.get_color_swap_rule(1, 2)\n        swapped = swap_chain.apply(grid_with_colors)\n        \n        # Colors should be swapped\n        self.assertEqual(swapped.data[0, 0], 2)\n        self.assertEqual(swapped.data[0, 1], 1)\n\n\ndef run_tests():\n    \"\"\"Run all tests\"\"\"\n    unittest.main(argv=[''], exit=False, verbosity=2)\n\n\nif __name__ == '__main__':\n    run_tests()","size_bytes":14543},"tests/test_solvers.py":{"content":"\"\"\"\nTest Suite for ARC Solvers\nValidates all solving strategies and components\n\"\"\"\n\nimport sys\nimport os\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nimport numpy as np\nfrom src.arc.task_loader import TaskLoader\nfrom src.arc.grid_operations import Grid\nfrom src.solvers.program_synthesis import ProgramSynthesisEngine, Program, DSLOperation, DSLInstruction\nfrom src.solvers.csp_solver import CSPSolver\nfrom src.solvers.pattern_solver import PatternSolver\nfrom src.solvers.ensemble_solver import EnsembleSolver\nfrom src.strategies.strategy_selector import StrategySelector\n\n\ndef test_program_synthesis():\n    \"\"\"Test program synthesis solver\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"Testing Program Synthesis Solver\")\n    print(\"=\"*60)\n    \n    # Create simple test case - scaling\n    train_inputs = [\n        np.array([[1, 2], [3, 4]], dtype=np.int8),\n        np.array([[5, 6], [7, 8]], dtype=np.int8)\n    ]\n    \n    train_outputs = [\n        np.array([[1, 1, 2, 2], [1, 1, 2, 2], [3, 3, 4, 4], [3, 3, 4, 4]], dtype=np.int8),\n        np.array([[5, 5, 6, 6], [5, 5, 6, 6], [7, 7, 8, 8], [7, 7, 8, 8]], dtype=np.int8)\n    ]\n    \n    test_input = np.array([[2, 3], [4, 5]], dtype=np.int8)\n    \n    # Initialize solver\n    solver = ProgramSynthesisEngine()\n    \n    # Get synthesized program\n    program = solver.get_program(train_inputs, train_outputs)\n    \n    if program:\n        print(f\"âœ“ Synthesized program with {len(program.instructions)} instructions\")\n        print(f\"  Program: {program.to_string()}\")\n        print(f\"  Score: {program.score:.3f}\")\n        \n        # Apply to test input\n        result = solver.solve(train_inputs, train_outputs, test_input)\n        if result is not None:\n            print(f\"âœ“ Applied program to test input\")\n            print(f\"  Input shape: {test_input.shape}\")\n            print(f\"  Output shape: {result.shape}\")\n    else:\n        print(\"âœ— Failed to synthesize program\")\n    \n    return program is not None\n\n\ndef test_csp_solver():\n    \"\"\"Test CSP solver\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"Testing CSP Solver\")\n    print(\"=\"*60)\n    \n    # Create simple test case - color mapping\n    train_inputs = [\n        np.array([[1, 2, 1], [2, 1, 2], [1, 2, 1]], dtype=np.int8),\n        np.array([[3, 4, 3], [4, 3, 4], [3, 4, 3]], dtype=np.int8)\n    ]\n    \n    train_outputs = [\n        np.array([[2, 1, 2], [1, 2, 1], [2, 1, 2]], dtype=np.int8),\n        np.array([[4, 3, 4], [3, 4, 3], [4, 3, 4]], dtype=np.int8)\n    ]\n    \n    test_input = np.array([[5, 6, 5], [6, 5, 6], [5, 6, 5]], dtype=np.int8)\n    \n    # Initialize solver\n    solver = CSPSolver()\n    \n    # Solve\n    result = solver.solve(train_inputs, train_outputs, test_input)\n    \n    if result is not None:\n        print(\"âœ“ CSP solver found solution\")\n        print(f\"  Input shape: {test_input.shape}\")\n        print(f\"  Output shape: {result.shape}\")\n        print(f\"  Unique colors in output: {np.unique(result)}\")\n    else:\n        print(\"âœ— CSP solver failed to find solution\")\n    \n    return result is not None\n\n\ndef test_pattern_solver():\n    \"\"\"Test pattern solver\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"Testing Pattern Solver\")\n    print(\"=\"*60)\n    \n    # Create test case - tiling pattern\n    train_inputs = [\n        np.array([[1, 2], [3, 4]], dtype=np.int8),\n        np.array([[5]], dtype=np.int8)\n    ]\n    \n    train_outputs = [\n        np.array([[1, 2, 1, 2], [3, 4, 3, 4], [1, 2, 1, 2], [3, 4, 3, 4]], dtype=np.int8),\n        np.array([[5, 5], [5, 5]], dtype=np.int8)\n    ]\n    \n    test_input = np.array([[7, 8], [9, 0]], dtype=np.int8)\n    \n    # Initialize solver\n    solver = PatternSolver()\n    \n    # Solve\n    result = solver.solve(train_inputs, train_outputs, test_input)\n    \n    if result is not None:\n        print(\"âœ“ Pattern solver found solution\")\n        print(f\"  Input shape: {test_input.shape}\")\n        print(f\"  Output shape: {result.shape}\")\n        \n        # Check if it's a tiling pattern\n        if result.shape[0] == test_input.shape[0] * 2 and result.shape[1] == test_input.shape[1] * 2:\n            print(\"  Pattern type: Tiling (2x2)\")\n    else:\n        print(\"âœ— Pattern solver failed to find solution\")\n    \n    return result is not None\n\n\ndef test_ensemble_solver():\n    \"\"\"Test ensemble solver\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"Testing Ensemble Solver\")\n    print(\"=\"*60)\n    \n    # Create test case\n    train_inputs = [\n        np.array([[1, 0], [0, 1]], dtype=np.int8),\n        np.array([[2, 0], [0, 2]], dtype=np.int8)\n    ]\n    \n    train_outputs = [\n        np.array([[0, 1], [1, 0]], dtype=np.int8),\n        np.array([[0, 2], [2, 0]], dtype=np.int8)\n    ]\n    \n    test_input = np.array([[3, 0], [0, 3]], dtype=np.int8)\n    \n    # Initialize solver with configuration\n    config = {\n        'use_program_synthesis': True,\n        'use_csp': True,\n        'use_pattern': True,\n        'use_hybrid': False,  # Skip for speed\n        'voting_type': 'weighted',\n        'parallel': False,  # Sequential for testing\n        'timeout_per_solver': 5.0\n    }\n    \n    solver = EnsembleSolver(config)\n    \n    # Solve\n    result = solver.solve(train_inputs, train_outputs, test_input)\n    \n    print(f\"âœ“ Ensemble solver completed\")\n    print(f\"  Total time: {result.total_time:.2f}s\")\n    print(f\"  Consensus score: {result.consensus_score:.3f}\")\n    print(f\"  Strategy used: {result.strategy_used}\")\n    print(f\"  Individual results:\")\n    \n    for res in result.individual_results:\n        status = \"âœ“\" if res.solution is not None else \"âœ—\"\n        print(f\"    {status} {res.solver_type.value}: confidence={res.confidence:.3f}, time={res.execution_time:.2f}s\")\n    \n    if result.final_solution is not None:\n        print(f\"âœ“ Final solution found\")\n        print(f\"  Shape: {result.final_solution.shape}\")\n    else:\n        print(\"âœ— No consensus solution found\")\n    \n    return result.final_solution is not None\n\n\ndef test_strategy_selector():\n    \"\"\"Test strategy selection\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"Testing Strategy Selector\")\n    print(\"=\"*60)\n    \n    # Create test cases with different characteristics\n    test_cases = [\n        {\n            'name': 'Scaling Task',\n            'inputs': [np.array([[1, 2], [3, 4]], dtype=np.int8)],\n            'outputs': [np.array([[1, 1, 2, 2], [1, 1, 2, 2], [3, 3, 4, 4], [3, 3, 4, 4]], dtype=np.int8)]\n        },\n        {\n            'name': 'Color Swap Task',\n            'inputs': [np.array([[1, 2, 1], [2, 1, 2]], dtype=np.int8)],\n            'outputs': [np.array([[2, 1, 2], [1, 2, 1]], dtype=np.int8)]\n        },\n        {\n            'name': 'Rotation Task',\n            'inputs': [np.array([[1, 2], [3, 4]], dtype=np.int8)],\n            'outputs': [np.array([[2, 4], [1, 3]], dtype=np.int8)]\n        }\n    ]\n    \n    selector = StrategySelector()\n    \n    for test_case in test_cases:\n        print(f\"\\n  {test_case['name']}:\")\n        \n        recommendation = selector.select_strategy(\n            test_case['inputs'],\n            test_case['outputs']\n        )\n        \n        print(f\"    Primary strategy: {recommendation.primary_strategy}\")\n        print(f\"    Alternatives: {', '.join(recommendation.alternative_strategies)}\")\n        print(f\"    Confidence: {recommendation.confidence:.3f}\")\n        print(f\"    Difficulty: {recommendation.expected_difficulty.name}\")\n        print(f\"    Reasoning: {recommendation.reasoning}\")\n    \n    return True\n\n\ndef test_real_arc_task():\n    \"\"\"Test on a real ARC task from the dataset\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"Testing on Real ARC Task\")\n    print(\"=\"*60)\n    \n    # Load a real task\n    loader = TaskLoader()\n    \n    # Try to load from attached assets\n    try:\n        loader.load_from_json_file(\"attached_assets/arc-agi_training_challenges (2)_1757573784094.txt\")\n        loader.load_from_json_file(\"attached_assets/arc-agi_training_solutions_1757573784094.txt\")\n    except:\n        print(\"  Could not load real ARC tasks from files\")\n        return False\n    \n    # Get first task\n    task_ids = loader.list_tasks()[:5]  # Test first 5 tasks\n    \n    if not task_ids:\n        print(\"  No tasks loaded\")\n        return False\n    \n    # Initialize ensemble solver\n    solver = EnsembleSolver({\n        'use_program_synthesis': True,\n        'use_csp': False,  # Disable for speed\n        'use_pattern': True,\n        'use_hybrid': False,\n        'parallel': False,\n        'timeout_per_solver': 3.0\n    })\n    \n    success_count = 0\n    \n    for task_id in task_ids:\n        task = loader.get_task(task_id)\n        solution = loader.get_solution(task_id)\n        \n        if not task or not solution:\n            continue\n        \n        # Get training data\n        train_inputs = []\n        train_outputs = []\n        \n        for example in task.train_examples:\n            train_inputs.append(example.input)\n            train_outputs.append(example.output)\n        \n        # Get test input\n        test_input = task.test_examples[0].input\n        expected_output = solution[0]\n        \n        # Solve\n        result = solver.solve(train_inputs, train_outputs, test_input)\n        \n        if result.final_solution is not None:\n            # Check if solution matches expected\n            if np.array_equal(result.final_solution, expected_output):\n                print(f\"  âœ“ Task {task_id}: CORRECT (strategy: {result.strategy_used})\")\n                success_count += 1\n            else:\n                print(f\"  âœ— Task {task_id}: INCORRECT (strategy: {result.strategy_used})\")\n        else:\n            print(f\"  âœ— Task {task_id}: NO SOLUTION\")\n    \n    print(f\"\\n  Success rate: {success_count}/{len(task_ids)} tasks solved correctly\")\n    \n    return success_count > 0\n\n\ndef run_all_tests():\n    \"\"\"Run all solver tests\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"         ARC SOLVER TEST SUITE\")\n    print(\"=\"*60)\n    \n    results = {}\n    \n    # Test individual solvers\n    results['Program Synthesis'] = test_program_synthesis()\n    results['CSP Solver'] = test_csp_solver()\n    results['Pattern Solver'] = test_pattern_solver()\n    \n    # Test ensemble and strategy\n    results['Ensemble Solver'] = test_ensemble_solver()\n    results['Strategy Selector'] = test_strategy_selector()\n    \n    # Test on real data\n    results['Real ARC Task'] = test_real_arc_task()\n    \n    # Summary\n    print(\"\\n\" + \"=\"*60)\n    print(\"                TEST SUMMARY\")\n    print(\"=\"*60)\n    \n    for test_name, passed in results.items():\n        status = \"âœ“ PASSED\" if passed else \"âœ— FAILED\"\n        print(f\"  {test_name}: {status}\")\n    \n    total = len(results)\n    passed = sum(results.values())\n    \n    print(f\"\\n  Overall: {passed}/{total} tests passed\")\n    \n    if passed == total:\n        print(\"\\n  ðŸŽ‰ All tests passed successfully!\")\n    else:\n        print(f\"\\n  âš ï¸  {total - passed} test(s) failed\")\n    \n    return passed == total\n\n\nif __name__ == \"__main__\":\n    success = run_all_tests()\n    sys.exit(0 if success else 1)","size_bytes":10988},"src/arc/__init__.py":{"content":"\"\"\"ARC-specific algorithms and transformations\"\"\"","size_bytes":49},"src/arc/grid_operations.py":{"content":"\"\"\"\nARC Grid Operations Module\nProvides comprehensive grid manipulation operations for ARC tasks\n\"\"\"\n\nimport numpy as np\nfrom typing import List, Tuple, Optional, Dict, Set, Any, Callable\nfrom scipy import ndimage\nfrom collections import Counter\nimport copy\n\n\nclass Grid:\n    \"\"\"Class representing an ARC grid with manipulation operations\"\"\"\n    \n    def __init__(self, data: np.ndarray):\n        \"\"\"Initialize grid with 2D numpy array\"\"\"\n        if isinstance(data, list):\n            data = np.array(data, dtype=np.int8)\n        elif not isinstance(data, np.ndarray):\n            raise TypeError(\"Grid data must be a numpy array or list\")\n        \n        if data.ndim != 2:\n            raise ValueError(\"Grid must be 2-dimensional\")\n        \n        # Ensure valid color range (0-9)\n        if not np.all((data >= 0) & (data <= 9)):\n            raise ValueError(\"Grid values must be in range 0-9\")\n        \n        self.data = data.astype(np.int8)\n    \n    def __repr__(self) -> str:\n        return f\"Grid(shape={self.shape}, unique_colors={self.unique_colors})\"\n    \n    def __str__(self) -> str:\n        \"\"\"Pretty print the grid\"\"\"\n        return '\\n'.join([' '.join(str(cell) for cell in row) for row in self.data])\n    \n    def __eq__(self, other) -> bool:\n        \"\"\"Check if two grids are equal\"\"\"\n        if not isinstance(other, Grid):\n            return False\n        return np.array_equal(self.data, other.data)\n    \n    @property\n    def shape(self) -> Tuple[int, int]:\n        return self.data.shape\n    \n    @property\n    def height(self) -> int:\n        return self.data.shape[0]\n    \n    @property\n    def width(self) -> int:\n        return self.data.shape[1]\n    \n    @property\n    def unique_colors(self) -> Set[int]:\n        return set(np.unique(self.data))\n    \n    def copy(self) -> 'Grid':\n        \"\"\"Create a deep copy of the grid\"\"\"\n        return Grid(self.data.copy())\n    \n    # Basic Operations\n    \n    def rotate(self, degrees: int) -> 'Grid':\n        \"\"\"Rotate grid by specified degrees (90, 180, 270)\"\"\"\n        if degrees not in [90, 180, 270]:\n            raise ValueError(\"Rotation must be 90, 180, or 270 degrees\")\n        \n        k = degrees // 90\n        return Grid(np.rot90(self.data, k))\n    \n    def flip_horizontal(self) -> 'Grid':\n        \"\"\"Flip grid horizontally (left-right)\"\"\"\n        return Grid(np.fliplr(self.data))\n    \n    def flip_vertical(self) -> 'Grid':\n        \"\"\"Flip grid vertically (up-down)\"\"\"\n        return Grid(np.flipud(self.data))\n    \n    def transpose(self) -> 'Grid':\n        \"\"\"Transpose the grid (swap rows and columns)\"\"\"\n        return Grid(self.data.T)\n    \n    # Advanced Operations\n    \n    def scale(self, factor: int) -> 'Grid':\n        \"\"\"Scale grid by integer factor (each cell becomes factor x factor cells)\"\"\"\n        if factor <= 0:\n            raise ValueError(\"Scale factor must be positive\")\n        \n        new_height = self.height * factor\n        new_width = self.width * factor\n        scaled = np.zeros((new_height, new_width), dtype=np.int8)\n        \n        for i in range(self.height):\n            for j in range(self.width):\n                scaled[i*factor:(i+1)*factor, j*factor:(j+1)*factor] = self.data[i, j]\n        \n        return Grid(scaled)\n    \n    def tile(self, rows: int, cols: int) -> 'Grid':\n        \"\"\"Tile grid in a rows x cols pattern\"\"\"\n        if rows <= 0 or cols <= 0:\n            raise ValueError(\"Tile dimensions must be positive\")\n        \n        return Grid(np.tile(self.data, (rows, cols)))\n    \n    def mirror(self, axis: str = 'horizontal') -> 'Grid':\n        \"\"\"Mirror grid along specified axis\"\"\"\n        if axis == 'horizontal':\n            # Mirror left to right\n            mirrored = np.hstack([self.data, np.fliplr(self.data)])\n        elif axis == 'vertical':\n            # Mirror top to bottom\n            mirrored = np.vstack([self.data, np.flipud(self.data)])\n        elif axis == 'both':\n            # Mirror in all four directions\n            top = np.hstack([self.data, np.fliplr(self.data)])\n            bottom = np.hstack([np.flipud(self.data), np.flipud(np.fliplr(self.data))])\n            mirrored = np.vstack([top, bottom])\n        else:\n            raise ValueError(\"Axis must be 'horizontal', 'vertical', or 'both'\")\n        \n        return Grid(mirrored)\n    \n    def overlay(self, other: 'Grid', x: int = 0, y: int = 0, \n                transparent_color: int = 0) -> 'Grid':\n        \"\"\"Overlay another grid on this grid at position (x, y)\"\"\"\n        result = self.copy()\n        \n        # Calculate the valid region for overlay\n        y_start = max(0, y)\n        y_end = min(self.height, y + other.height)\n        x_start = max(0, x)\n        x_end = min(self.width, x + other.width)\n        \n        # Calculate corresponding region in other grid\n        other_y_start = max(0, -y)\n        other_y_end = other_y_start + (y_end - y_start)\n        other_x_start = max(0, -x)\n        other_x_end = other_x_start + (x_end - x_start)\n        \n        # Apply overlay\n        for i in range(y_end - y_start):\n            for j in range(x_end - x_start):\n                color = other.data[other_y_start + i, other_x_start + j]\n                if color != transparent_color:\n                    result.data[y_start + i, x_start + j] = color\n        \n        return result\n    \n    def mask(self, mask_grid: 'Grid', mask_value: int = 0) -> 'Grid':\n        \"\"\"Apply a mask to the grid (keep values where mask is non-zero)\"\"\"\n        if self.shape != mask_grid.shape:\n            raise ValueError(\"Grid and mask must have the same shape\")\n        \n        result = np.where(mask_grid.data != 0, self.data, mask_value)\n        return Grid(result)\n    \n    # Color Operations\n    \n    def replace_color(self, old_color: int, new_color: int) -> 'Grid':\n        \"\"\"Replace all occurrences of old_color with new_color\"\"\"\n        result = self.data.copy()\n        result[result == old_color] = new_color\n        return Grid(result)\n    \n    def map_colors(self, color_map: Dict[int, int]) -> 'Grid':\n        \"\"\"Map colors according to the provided dictionary\"\"\"\n        result = self.data.copy()\n        for old_color, new_color in color_map.items():\n            result[self.data == old_color] = new_color\n        return Grid(result)\n    \n    def filter_color(self, color: int, background: int = 0) -> 'Grid':\n        \"\"\"Keep only specified color, replace others with background\"\"\"\n        result = np.where(self.data == color, self.data, background)\n        return Grid(result)\n    \n    def count_colors(self) -> Dict[int, int]:\n        \"\"\"Count occurrences of each color\"\"\"\n        unique, counts = np.unique(self.data, return_counts=True)\n        return dict(zip(unique, counts))\n    \n    def get_color_positions(self, color: int) -> List[Tuple[int, int]]:\n        \"\"\"Get all positions of a specific color\"\"\"\n        positions = np.argwhere(self.data == color)\n        return [(int(y), int(x)) for y, x in positions]\n    \n    # Pattern Operations\n    \n    def extract_subgrid(self, y: int, x: int, height: int, width: int) -> 'Grid':\n        \"\"\"Extract a subgrid from the specified position\"\"\"\n        if y < 0 or x < 0 or y + height > self.height or x + width > self.width:\n            raise ValueError(\"Subgrid bounds exceed grid dimensions\")\n        \n        return Grid(self.data[y:y+height, x:x+width])\n    \n    def replace_subgrid(self, subgrid: 'Grid', y: int, x: int) -> 'Grid':\n        \"\"\"Replace a region with the provided subgrid\"\"\"\n        result = self.copy()\n        \n        # Calculate the valid region\n        y_end = min(self.height, y + subgrid.height)\n        x_end = min(self.width, x + subgrid.width)\n        \n        result.data[y:y_end, x:x_end] = subgrid.data[:y_end-y, :x_end-x]\n        return result\n    \n    def find_pattern(self, pattern: 'Grid') -> List[Tuple[int, int]]:\n        \"\"\"Find all occurrences of a pattern in the grid\"\"\"\n        positions = []\n        p_h, p_w = pattern.shape\n        \n        if p_h > self.height or p_w > self.width:\n            return positions\n        \n        for y in range(self.height - p_h + 1):\n            for x in range(self.width - p_w + 1):\n                subgrid = self.data[y:y+p_h, x:x+p_w]\n                if np.array_equal(subgrid, pattern.data):\n                    positions.append((y, x))\n        \n        return positions\n    \n    def get_bounding_box(self, color: Optional[int] = None) -> Optional[Tuple[int, int, int, int]]:\n        \"\"\"Get bounding box of non-background colors or specific color\"\"\"\n        if color is not None:\n            mask = self.data == color\n        else:\n            # Assume 0 is background\n            mask = self.data != 0\n        \n        if not np.any(mask):\n            return None\n        \n        rows = np.any(mask, axis=1)\n        cols = np.any(mask, axis=0)\n        \n        y_min, y_max = np.where(rows)[0][[0, -1]]\n        x_min, x_max = np.where(cols)[0][[0, -1]]\n        \n        return (int(y_min), int(x_min), int(y_max + 1), int(x_max + 1))\n    \n    def crop_to_content(self, background: int = 0) -> 'Grid':\n        \"\"\"Crop grid to remove background borders\"\"\"\n        bbox = self.get_bounding_box()\n        if bbox is None:\n            return self.copy()\n        \n        y_min, x_min, y_max, x_max = bbox\n        return Grid(self.data[y_min:y_max, x_min:x_max])\n    \n    def pad(self, top: int, bottom: int, left: int, right: int, \n            fill_value: int = 0) -> 'Grid':\n        \"\"\"Pad grid with specified values on each side\"\"\"\n        padded = np.pad(self.data, \n                       ((top, bottom), (left, right)), \n                       constant_values=fill_value)\n        return Grid(padded)\n    \n    def resize(self, new_height: int, new_width: int, fill_value: int = 0) -> 'Grid':\n        \"\"\"Resize grid to new dimensions, centering content\"\"\"\n        if new_height <= 0 or new_width <= 0:\n            raise ValueError(\"New dimensions must be positive\")\n        \n        result = np.full((new_height, new_width), fill_value, dtype=np.int8)\n        \n        # Calculate centering offsets\n        y_offset = (new_height - self.height) // 2\n        x_offset = (new_width - self.width) // 2\n        \n        # Calculate valid regions\n        src_y_start = max(0, -y_offset)\n        src_y_end = min(self.height, new_height - y_offset)\n        src_x_start = max(0, -x_offset)\n        src_x_end = min(self.width, new_width - x_offset)\n        \n        dst_y_start = max(0, y_offset)\n        dst_y_end = dst_y_start + (src_y_end - src_y_start)\n        dst_x_start = max(0, x_offset)\n        dst_x_end = dst_x_start + (src_x_end - src_x_start)\n        \n        result[dst_y_start:dst_y_end, dst_x_start:dst_x_end] = \\\n            self.data[src_y_start:src_y_end, src_x_start:src_x_end]\n        \n        return Grid(result)\n    \n    def apply_function(self, func: Callable[[int], int]) -> 'Grid':\n        \"\"\"Apply a function to each cell value\"\"\"\n        vectorized = np.vectorize(func)\n        result = vectorized(self.data)\n        return Grid(result)\n    \n    def get_connected_components(self, connectivity: int = 4) -> List['Grid']:\n        \"\"\"Get all connected components as separate grids\"\"\"\n        components = []\n        \n        for color in self.unique_colors:\n            if color == 0:  # Skip background\n                continue\n            \n            # Create binary mask for this color\n            mask = (self.data == color).astype(int)\n            \n            # Label connected components\n            if connectivity == 4:\n                structure = np.array([[0,1,0],[1,1,1],[0,1,0]])\n            else:  # 8-connectivity\n                structure = np.ones((3,3))\n            \n            labeled, num_features = ndimage.label(mask, structure=structure)\n            \n            # Extract each component\n            for i in range(1, num_features + 1):\n                component_mask = (labeled == i)\n                bbox = self._get_bbox_from_mask(component_mask)\n                if bbox:\n                    y_min, x_min, y_max, x_max = bbox\n                    component_data = np.zeros_like(self.data[y_min:y_max, x_min:x_max])\n                    component_data[component_mask[y_min:y_max, x_min:x_max]] = color\n                    components.append(Grid(component_data))\n        \n        return components\n    \n    def _get_bbox_from_mask(self, mask: np.ndarray) -> Optional[Tuple[int, int, int, int]]:\n        \"\"\"Helper to get bounding box from boolean mask\"\"\"\n        if not np.any(mask):\n            return None\n        \n        rows = np.any(mask, axis=1)\n        cols = np.any(mask, axis=0)\n        \n        y_min, y_max = np.where(rows)[0][[0, -1]]\n        x_min, x_max = np.where(cols)[0][[0, -1]]\n        \n        return (int(y_min), int(x_min), int(y_max + 1), int(x_max + 1))\n    \n    def to_numpy(self) -> np.ndarray:\n        \"\"\"Convert grid to numpy array\"\"\"\n        return self.data.copy()\n    \n    @classmethod\n    def from_numpy(cls, array: np.ndarray) -> 'Grid':\n        \"\"\"Create grid from numpy array\"\"\"\n        return cls(array)\n    \n    @classmethod\n    def create_empty(cls, height: int, width: int, fill_value: int = 0) -> 'Grid':\n        \"\"\"Create an empty grid with specified dimensions\"\"\"\n        return cls(np.full((height, width), fill_value, dtype=np.int8))","size_bytes":13291},"src/arc/pattern_detector.py":{"content":"\"\"\"\nARC Pattern Detection Module\nDetects patterns, symmetries, and transformations in ARC grids\n\"\"\"\n\nimport numpy as np\nfrom typing import List, Tuple, Dict, Optional, Set\nfrom scipy import ndimage\nfrom collections import Counter\nfrom .grid_operations import Grid\n\n\nclass PatternDetector:\n    \"\"\"Detects various patterns and features in ARC grids\"\"\"\n    \n    def __init__(self, grid: Grid):\n        self.grid = grid\n        self.data = grid.data\n    \n    # Symmetry Detection\n    \n    def has_horizontal_symmetry(self) -> bool:\n        \"\"\"Check if grid has horizontal (left-right) symmetry\"\"\"\n        return np.array_equal(self.data, np.fliplr(self.data))\n    \n    def has_vertical_symmetry(self) -> bool:\n        \"\"\"Check if grid has vertical (top-bottom) symmetry\"\"\"\n        return np.array_equal(self.data, np.flipud(self.data))\n    \n    def has_diagonal_symmetry(self) -> bool:\n        \"\"\"Check if grid has diagonal symmetry (main diagonal)\"\"\"\n        if self.grid.height != self.grid.width:\n            return False\n        return np.array_equal(self.data, self.data.T)\n    \n    def has_anti_diagonal_symmetry(self) -> bool:\n        \"\"\"Check if grid has anti-diagonal symmetry\"\"\"\n        if self.grid.height != self.grid.width:\n            return False\n        return np.array_equal(self.data, np.rot90(np.rot90(self.data.T)))\n    \n    def has_rotational_symmetry(self, order: int = 2) -> bool:\n        \"\"\"Check if grid has rotational symmetry of given order\"\"\"\n        if self.grid.height != self.grid.width:\n            return False\n        \n        angle = 360 // order\n        rotated = self.data.copy()\n        \n        for _ in range(order - 1):\n            rotated = np.rot90(rotated)\n            if not np.array_equal(self.data, rotated) and angle * (_ + 1) < 360:\n                return False\n        \n        return True\n    \n    def get_symmetries(self) -> Dict[str, bool]:\n        \"\"\"Get all symmetries of the grid\"\"\"\n        return {\n            'horizontal': self.has_horizontal_symmetry(),\n            'vertical': self.has_vertical_symmetry(),\n            'diagonal': self.has_diagonal_symmetry(),\n            'anti_diagonal': self.has_anti_diagonal_symmetry(),\n            'rotational_90': self.has_rotational_symmetry(4),\n            'rotational_180': self.has_rotational_symmetry(2)\n        }\n    \n    # Pattern Detection\n    \n    def find_repeating_patterns(self, min_size: int = 2, \n                               max_size: Optional[int] = None) -> List[Tuple[Grid, int]]:\n        \"\"\"Find repeating subpatterns in the grid\"\"\"\n        if max_size is None:\n            max_size = min(self.grid.height // 2, self.grid.width // 2)\n        \n        patterns = {}\n        \n        for h in range(min_size, min(max_size + 1, self.grid.height + 1)):\n            for w in range(min_size, min(max_size + 1, self.grid.width + 1)):\n                for y in range(self.grid.height - h + 1):\n                    for x in range(self.grid.width - w + 1):\n                        subgrid = self.grid.extract_subgrid(y, x, h, w)\n                        pattern_key = tuple(subgrid.data.flatten())\n                        \n                        if pattern_key not in patterns:\n                            patterns[pattern_key] = {\n                                'grid': subgrid,\n                                'count': 0,\n                                'positions': []\n                            }\n                        \n                        patterns[pattern_key]['count'] += 1\n                        patterns[pattern_key]['positions'].append((y, x))\n        \n        # Filter patterns that appear more than once\n        repeating = []\n        for pattern_info in patterns.values():\n            if pattern_info['count'] > 1:\n                repeating.append((pattern_info['grid'], pattern_info['count']))\n        \n        # Sort by frequency\n        repeating.sort(key=lambda x: x[1], reverse=True)\n        \n        return repeating\n    \n    def detect_periodicity(self) -> Dict[str, Optional[int]]:\n        \"\"\"Detect periodic patterns in horizontal and vertical directions\"\"\"\n        result = {'horizontal': None, 'vertical': None}\n        \n        # Check horizontal periodicity\n        for period in range(2, self.grid.width):\n            if self.grid.width % period == 0:\n                is_periodic = True\n                for row in self.data:\n                    for i in range(period, self.grid.width, period):\n                        if not np.array_equal(row[:period], row[i:i+period]):\n                            is_periodic = False\n                            break\n                    if not is_periodic:\n                        break\n                \n                if is_periodic:\n                    result['horizontal'] = period\n                    break\n        \n        # Check vertical periodicity\n        for period in range(2, self.grid.height):\n            if self.grid.height % period == 0:\n                is_periodic = True\n                for col_idx in range(self.grid.width):\n                    col = self.data[:, col_idx]\n                    for i in range(period, self.grid.height, period):\n                        if not np.array_equal(col[:period], col[i:i+period]):\n                            is_periodic = False\n                            break\n                    if not is_periodic:\n                        break\n                \n                if is_periodic:\n                    result['vertical'] = period\n                    break\n        \n        return result\n    \n    # Shape Detection\n    \n    def find_rectangles(self, color: Optional[int] = None) -> List[Dict]:\n        \"\"\"Find rectangular regions of a specific color or any non-background color\"\"\"\n        rectangles = []\n        colors_to_check = [color] if color is not None else [c for c in self.grid.unique_colors if c != 0]\n        \n        for check_color in colors_to_check:\n            mask = (self.data == check_color).astype(int)\n            \n            # Use connected components to find separate rectangles\n            labeled, num_features = ndimage.label(mask)\n            \n            for i in range(1, num_features + 1):\n                component = (labeled == i)\n                \n                # Check if component is rectangular\n                bbox = self._get_bbox_from_mask(component)\n                if bbox:\n                    y_min, x_min, y_max, x_max = bbox\n                    rect_region = component[y_min:y_max, x_min:x_max]\n                    \n                    # A perfect rectangle should be all True in the bbox\n                    if np.all(rect_region):\n                        rectangles.append({\n                            'color': check_color,\n                            'position': (y_min, x_min),\n                            'height': y_max - y_min,\n                            'width': x_max - x_min,\n                            'area': (y_max - y_min) * (x_max - x_min)\n                        })\n        \n        return rectangles\n    \n    def find_lines(self, min_length: int = 3) -> List[Dict]:\n        \"\"\"Find straight lines (horizontal, vertical, diagonal) in the grid\"\"\"\n        lines = []\n        \n        for color in self.grid.unique_colors:\n            if color == 0:  # Skip background\n                continue\n            \n            # Horizontal lines\n            for y in range(self.grid.height):\n                x = 0\n                while x < self.grid.width:\n                    if self.data[y, x] == color:\n                        # Found start of potential line\n                        length = 1\n                        while x + length < self.grid.width and self.data[y, x + length] == color:\n                            length += 1\n                        \n                        if length >= min_length:\n                            lines.append({\n                                'type': 'horizontal',\n                                'color': color,\n                                'start': (y, x),\n                                'end': (y, x + length - 1),\n                                'length': length\n                            })\n                        \n                        x += length\n                    else:\n                        x += 1\n            \n            # Vertical lines\n            for x in range(self.grid.width):\n                y = 0\n                while y < self.grid.height:\n                    if self.data[y, x] == color:\n                        # Found start of potential line\n                        length = 1\n                        while y + length < self.grid.height and self.data[y + length, x] == color:\n                            length += 1\n                        \n                        if length >= min_length:\n                            lines.append({\n                                'type': 'vertical',\n                                'color': color,\n                                'start': (y, x),\n                                'end': (y + length - 1, x),\n                                'length': length\n                            })\n                        \n                        y += length\n                    else:\n                        y += 1\n            \n            # Diagonal lines (top-left to bottom-right)\n            for y in range(self.grid.height):\n                for x in range(self.grid.width):\n                    if self.data[y, x] == color:\n                        length = 1\n                        while (y + length < self.grid.height and \n                               x + length < self.grid.width and \n                               self.data[y + length, x + length] == color):\n                            length += 1\n                        \n                        if length >= min_length:\n                            lines.append({\n                                'type': 'diagonal',\n                                'color': color,\n                                'start': (y, x),\n                                'end': (y + length - 1, x + length - 1),\n                                'length': length\n                            })\n            \n            # Anti-diagonal lines (top-right to bottom-left)\n            for y in range(self.grid.height):\n                for x in range(self.grid.width):\n                    if self.data[y, x] == color:\n                        length = 1\n                        while (y + length < self.grid.height and \n                               x - length >= 0 and \n                               self.data[y + length, x - length] == color):\n                            length += 1\n                        \n                        if length >= min_length:\n                            lines.append({\n                                'type': 'anti-diagonal',\n                                'color': color,\n                                'start': (y, x),\n                                'end': (y + length - 1, x - length + 1),\n                                'length': length\n                            })\n        \n        return lines\n    \n    def get_connected_components(self, connectivity: int = 4, include_background: bool = False) -> List[Dict]:\n        \"\"\"Find and analyze connected components\"\"\"\n        components = []\n        \n        for color in self.grid.unique_colors:\n            if color == 0 and not include_background:  # Skip background unless requested\n                continue\n            \n            mask = (self.data == color).astype(int)\n            \n            if connectivity == 4:\n                structure = np.array([[0,1,0],[1,1,1],[0,1,0]])\n            else:  # 8-connectivity\n                structure = np.ones((3,3))\n            \n            labeled, num_features = ndimage.label(mask, structure=structure)\n            \n            for i in range(1, num_features + 1):\n                component_mask = (labeled == i)\n                bbox = self._get_bbox_from_mask(component_mask)\n                \n                if bbox:\n                    y_min, x_min, y_max, x_max = bbox\n                    area = np.sum(component_mask)\n                    \n                    # Calculate centroid\n                    positions = np.argwhere(component_mask)\n                    centroid = np.mean(positions, axis=0)\n                    \n                    components.append({\n                        'color': color,\n                        'bbox': bbox,\n                        'area': int(area),\n                        'centroid': tuple(centroid),\n                        'positions': [(int(y), int(x)) for y, x in positions],\n                        'width': x_max - x_min,\n                        'height': y_max - y_min\n                    })\n        \n        return components\n    \n    # Transformation Detection\n    \n    def compare_with(self, other: 'Grid') -> Dict[str, any]:\n        \"\"\"Compare this grid with another to detect transformations\"\"\"\n        comparison = {\n            'same_size': self.grid.shape == other.shape,\n            'size_change': None,\n            'colors_added': set(),\n            'colors_removed': set(),\n            'colors_changed': {},\n            'is_scaled': False,\n            'scale_factor': None,\n            'is_rotated': False,\n            'rotation_angle': None,\n            'is_flipped': False,\n            'flip_type': None,\n            'pattern_repeated': False\n        }\n        \n        # Check size changes\n        if not comparison['same_size']:\n            comparison['size_change'] = {\n                'height_ratio': other.height / self.grid.height,\n                'width_ratio': other.width / self.grid.width\n            }\n            \n            # Check if it's a simple scale\n            if (other.height % self.grid.height == 0 and \n                other.width % self.grid.width == 0 and\n                other.height // self.grid.height == other.width // self.grid.width):\n                \n                factor = other.height // self.grid.height\n                scaled = self.grid.scale(factor)\n                if scaled == other:\n                    comparison['is_scaled'] = True\n                    comparison['scale_factor'] = factor\n        \n        # Check color changes\n        self_colors = self.grid.unique_colors\n        other_colors = other.unique_colors\n        \n        comparison['colors_added'] = other_colors - self_colors\n        comparison['colors_removed'] = self_colors - other_colors\n        \n        # Check for simple transformations\n        if comparison['same_size']:\n            # Check rotations\n            for angle in [90, 180, 270]:\n                if self.grid.rotate(angle) == other:\n                    comparison['is_rotated'] = True\n                    comparison['rotation_angle'] = angle\n                    break\n            \n            # Check flips\n            if self.grid.flip_horizontal() == other:\n                comparison['is_flipped'] = True\n                comparison['flip_type'] = 'horizontal'\n            elif self.grid.flip_vertical() == other:\n                comparison['is_flipped'] = True\n                comparison['flip_type'] = 'vertical'\n        \n        # Check for pattern repetition\n        if other.height >= self.grid.height and other.width >= self.grid.width:\n            if (other.height % self.grid.height == 0 and \n                other.width % self.grid.width == 0):\n                \n                rows = other.height // self.grid.height\n                cols = other.width // self.grid.width\n                tiled = self.grid.tile(rows, cols)\n                \n                if tiled == other:\n                    comparison['pattern_repeated'] = True\n                    comparison['repetition'] = {'rows': rows, 'cols': cols}\n        \n        return comparison\n    \n    def _get_bbox_from_mask(self, mask: np.ndarray) -> Optional[Tuple[int, int, int, int]]:\n        \"\"\"Helper to get bounding box from boolean mask\"\"\"\n        if not np.any(mask):\n            return None\n        \n        rows = np.any(mask, axis=1)\n        cols = np.any(mask, axis=0)\n        \n        y_min, y_max = np.where(rows)[0][[0, -1]]\n        x_min, x_max = np.where(cols)[0][[0, -1]]\n        \n        return (int(y_min), int(x_min), int(y_max + 1), int(x_max + 1))\n    \n    def get_pattern_statistics(self) -> Dict:\n        \"\"\"Get comprehensive statistics about patterns in the grid\"\"\"\n        stats = {\n            'grid_shape': self.grid.shape,\n            'unique_colors': len(self.grid.unique_colors),\n            'color_distribution': self.grid.count_colors(),\n            'symmetries': self.get_symmetries(),\n            'periodicity': self.detect_periodicity(),\n            'num_rectangles': len(self.find_rectangles()),\n            'num_lines': len(self.find_lines()),\n            'num_components': len(self.find_connected_components()),\n            'dominant_color': None,\n            'sparsity': None\n        }\n        \n        # Find dominant color\n        color_counts = stats['color_distribution']\n        if color_counts:\n            stats['dominant_color'] = max(color_counts, key=color_counts.get)\n        \n        # Calculate sparsity (assuming 0 is background)\n        total_cells = self.grid.height * self.grid.width\n        non_zero_cells = total_cells - color_counts.get(0, 0)\n        stats['sparsity'] = non_zero_cells / total_cells if total_cells > 0 else 0\n        \n        return stats","size_bytes":17385},"src/arc/task_loader.py":{"content":"\"\"\"\nARC Task Loader Module\nHandles loading and parsing ARC tasks from JSON files\n\"\"\"\n\nimport json\nimport os\nfrom typing import Dict, List, Any, Optional, Tuple\nfrom dataclasses import dataclass\nimport numpy as np\n\n\n@dataclass\nclass ARCExample:\n    \"\"\"Represents a single input/output example\"\"\"\n    input: np.ndarray\n    output: Optional[np.ndarray] = None\n    \n    def __post_init__(self):\n        \"\"\"Convert lists to numpy arrays if needed\"\"\"\n        if not isinstance(self.input, np.ndarray):\n            self.input = np.array(self.input, dtype=np.int8)\n        if self.output is not None and not isinstance(self.output, np.ndarray):\n            self.output = np.array(self.output, dtype=np.int8)\n    \n    @property\n    def input_shape(self) -> Tuple[int, int]:\n        return self.input.shape\n    \n    @property\n    def output_shape(self) -> Optional[Tuple[int, int]]:\n        return self.output.shape if self.output is not None else None\n\n\n@dataclass\nclass ARCTask:\n    \"\"\"Represents a complete ARC task with train and test examples\"\"\"\n    task_id: str\n    train_examples: List[ARCExample]\n    test_examples: List[ARCExample]\n    \n    @property\n    def num_train(self) -> int:\n        return len(self.train_examples)\n    \n    @property\n    def num_test(self) -> int:\n        return len(self.test_examples)\n    \n    def get_train_pair(self, index: int) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Get a specific training input/output pair\"\"\"\n        if index >= self.num_train:\n            raise IndexError(f\"Train index {index} out of range (max: {self.num_train - 1})\")\n        example = self.train_examples[index]\n        return example.input, example.output\n    \n    def get_test_input(self, index: int) -> np.ndarray:\n        \"\"\"Get a specific test input\"\"\"\n        if index >= self.num_test:\n            raise IndexError(f\"Test index {index} out of range (max: {self.num_test - 1})\")\n        return self.test_examples[index].input\n    \n    def validate(self) -> bool:\n        \"\"\"Validate task data integrity\"\"\"\n        if not self.task_id:\n            return False\n        \n        if not self.train_examples:\n            return False\n        \n        # Check all train examples have outputs\n        for example in self.train_examples:\n            if example.output is None:\n                return False\n            \n            # Check valid color range (0-9)\n            if not np.all((example.input >= 0) & (example.input <= 9)):\n                return False\n            if not np.all((example.output >= 0) & (example.output <= 9)):\n                return False\n        \n        # Check test examples have valid inputs\n        for example in self.test_examples:\n            if not np.all((example.input >= 0) & (example.input <= 9)):\n                return False\n        \n        return True\n\n\nclass TaskLoader:\n    \"\"\"Loads and manages ARC tasks from various sources\"\"\"\n    \n    def __init__(self):\n        self.tasks: Dict[str, ARCTask] = {}\n        self.solutions: Dict[str, List[np.ndarray]] = {}\n    \n    def load_from_json_file(self, filepath: str, is_solution: bool = False) -> Dict[str, ARCTask]:\n        \"\"\"Load tasks from a JSON file\"\"\"\n        with open(filepath, 'r') as f:\n            data = json.load(f)\n        \n        if is_solution:\n            return self._parse_solutions(data)\n        else:\n            return self._parse_tasks(data)\n    \n    def load_from_json_string(self, json_str: str, is_solution: bool = False) -> Dict[str, ARCTask]:\n        \"\"\"Load tasks from a JSON string\"\"\"\n        data = json.loads(json_str)\n        \n        if is_solution:\n            return self._parse_solutions(data)\n        else:\n            return self._parse_tasks(data)\n    \n    def _parse_tasks(self, data: Dict) -> Dict[str, ARCTask]:\n        \"\"\"Parse task data from JSON format\"\"\"\n        tasks = {}\n        \n        for task_id, task_data in data.items():\n            train_examples = []\n            test_examples = []\n            \n            # Parse training examples\n            if 'train' in task_data:\n                for example in task_data['train']:\n                    train_examples.append(ARCExample(\n                        input=example['input'],\n                        output=example.get('output')\n                    ))\n            \n            # Parse test examples\n            if 'test' in task_data:\n                for example in task_data['test']:\n                    test_examples.append(ARCExample(\n                        input=example['input'],\n                        output=example.get('output')\n                    ))\n            \n            task = ARCTask(\n                task_id=task_id,\n                train_examples=train_examples,\n                test_examples=test_examples\n            )\n            \n            if task.validate():\n                tasks[task_id] = task\n                self.tasks[task_id] = task\n        \n        return tasks\n    \n    def _parse_solutions(self, data: Dict) -> Dict:\n        \"\"\"Parse solution data from JSON format\"\"\"\n        for task_id, solutions in data.items():\n            self.solutions[task_id] = [np.array(sol, dtype=np.int8) for sol in solutions]\n        return self.solutions\n    \n    def get_task(self, task_id: str) -> Optional[ARCTask]:\n        \"\"\"Get a specific task by ID\"\"\"\n        return self.tasks.get(task_id)\n    \n    def get_solution(self, task_id: str) -> Optional[List[np.ndarray]]:\n        \"\"\"Get solutions for a specific task\"\"\"\n        return self.solutions.get(task_id)\n    \n    def list_tasks(self) -> List[str]:\n        \"\"\"List all available task IDs\"\"\"\n        return list(self.tasks.keys())\n    \n    def load_directory(self, directory: str) -> int:\n        \"\"\"Load all JSON files from a directory\"\"\"\n        count = 0\n        for filename in os.listdir(directory):\n            if filename.endswith('.json'):\n                filepath = os.path.join(directory, filename)\n                is_solution = 'solution' in filename.lower()\n                self.load_from_json_file(filepath, is_solution)\n                count += 1\n        return count\n    \n    def get_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get statistics about loaded tasks\"\"\"\n        stats = {\n            'total_tasks': len(self.tasks),\n            'total_train_examples': sum(task.num_train for task in self.tasks.values()),\n            'total_test_examples': sum(task.num_test for task in self.tasks.values()),\n            'tasks_with_solutions': len(self.solutions),\n            'avg_train_per_task': 0,\n            'avg_test_per_task': 0,\n            'grid_sizes': {}\n        }\n        \n        if self.tasks:\n            stats['avg_train_per_task'] = stats['total_train_examples'] / stats['total_tasks']\n            stats['avg_test_per_task'] = stats['total_test_examples'] / stats['total_tasks']\n            \n            # Collect grid size statistics\n            for task in self.tasks.values():\n                for example in task.train_examples:\n                    size_key = f\"{example.input_shape[0]}x{example.input_shape[1]}\"\n                    stats['grid_sizes'][size_key] = stats['grid_sizes'].get(size_key, 0) + 1\n        \n        return stats\n    \n    def validate_all(self) -> Tuple[List[str], List[str]]:\n        \"\"\"Validate all loaded tasks and return valid and invalid task IDs\"\"\"\n        valid = []\n        invalid = []\n        \n        for task_id, task in self.tasks.items():\n            if task.validate():\n                valid.append(task_id)\n            else:\n                invalid.append(task_id)\n        \n        return valid, invalid","size_bytes":7564},"src/arc/transformation_rules.py":{"content":"\"\"\"\nARC Transformation Rules Module\nDefines and applies transformation rules for ARC tasks\n\"\"\"\n\nimport numpy as np\nfrom typing import List, Dict, Optional, Callable, Tuple, Any\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom .grid_operations import Grid\nfrom .pattern_detector import PatternDetector\n\n\nclass TransformationType(Enum):\n    \"\"\"Types of transformations\"\"\"\n    # Geometric\n    ROTATE = \"rotate\"\n    FLIP = \"flip\"\n    TRANSPOSE = \"transpose\"\n    SCALE = \"scale\"\n    TILE = \"tile\"\n    MIRROR = \"mirror\"\n    \n    # Color\n    COLOR_MAP = \"color_map\"\n    COLOR_REPLACE = \"color_replace\"\n    COLOR_FILTER = \"color_filter\"\n    \n    # Pattern\n    PATTERN_REPEAT = \"pattern_repeat\"\n    PATTERN_REPLACE = \"pattern_replace\"\n    OVERLAY = \"overlay\"\n    MASK = \"mask\"\n    \n    # Structural\n    CROP = \"crop\"\n    PAD = \"pad\"\n    RESIZE = \"resize\"\n    EXTRACT = \"extract\"\n    \n    # Complex\n    COMPOSITE = \"composite\"\n    CONDITIONAL = \"conditional\"\n    CUSTOM = \"custom\"\n\n\n@dataclass\nclass TransformationRule:\n    \"\"\"Represents a transformation rule\"\"\"\n    name: str\n    transformation_type: TransformationType\n    parameters: Dict[str, Any]\n    description: str = \"\"\n    \n    def apply(self, grid: Grid) -> Grid:\n        \"\"\"Apply the transformation to a grid\"\"\"\n        if self.transformation_type == TransformationType.ROTATE:\n            return grid.rotate(self.parameters['degrees'])\n        \n        elif self.transformation_type == TransformationType.FLIP:\n            if self.parameters['direction'] == 'horizontal':\n                return grid.flip_horizontal()\n            else:\n                return grid.flip_vertical()\n        \n        elif self.transformation_type == TransformationType.TRANSPOSE:\n            return grid.transpose()\n        \n        elif self.transformation_type == TransformationType.SCALE:\n            return grid.scale(self.parameters['factor'])\n        \n        elif self.transformation_type == TransformationType.TILE:\n            return grid.tile(self.parameters['rows'], self.parameters['cols'])\n        \n        elif self.transformation_type == TransformationType.MIRROR:\n            return grid.mirror(self.parameters.get('axis', 'horizontal'))\n        \n        elif self.transformation_type == TransformationType.COLOR_MAP:\n            return grid.map_colors(self.parameters['color_map'])\n        \n        elif self.transformation_type == TransformationType.COLOR_REPLACE:\n            return grid.replace_color(\n                self.parameters['old_color'], \n                self.parameters['new_color']\n            )\n        \n        elif self.transformation_type == TransformationType.COLOR_FILTER:\n            return grid.filter_color(\n                self.parameters['color'],\n                self.parameters.get('background', 0)\n            )\n        \n        elif self.transformation_type == TransformationType.CROP:\n            return grid.crop_to_content(self.parameters.get('background', 0))\n        \n        elif self.transformation_type == TransformationType.PAD:\n            return grid.pad(\n                self.parameters.get('top', 0),\n                self.parameters.get('bottom', 0),\n                self.parameters.get('left', 0),\n                self.parameters.get('right', 0),\n                self.parameters.get('fill_value', 0)\n            )\n        \n        elif self.transformation_type == TransformationType.RESIZE:\n            return grid.resize(\n                self.parameters['height'],\n                self.parameters['width'],\n                self.parameters.get('fill_value', 0)\n            )\n        \n        elif self.transformation_type == TransformationType.CUSTOM:\n            # Custom transformation using a provided function\n            func = self.parameters.get('function')\n            if func and callable(func):\n                return func(grid)\n        \n        else:\n            raise NotImplementedError(f\"Transformation {self.transformation_type} not implemented\")\n    \n    def validate(self, grid: Grid) -> bool:\n        \"\"\"Check if this rule can be applied to the given grid\"\"\"\n        if self.transformation_type == TransformationType.ROTATE:\n            return grid.height == grid.width or self.parameters['degrees'] == 180\n        \n        elif self.transformation_type == TransformationType.TRANSPOSE:\n            return True  # Always valid\n        \n        elif self.transformation_type == TransformationType.SCALE:\n            factor = self.parameters['factor']\n            return factor > 0 and isinstance(factor, int)\n        \n        return True  # Default to valid\n\n\nclass RuleChain:\n    \"\"\"Chain multiple transformation rules\"\"\"\n    \n    def __init__(self, rules: List[TransformationRule]):\n        self.rules = rules\n    \n    def apply(self, grid: Grid) -> Grid:\n        \"\"\"Apply all rules in sequence\"\"\"\n        result = grid.copy()\n        for rule in self.rules:\n            if rule.validate(result):\n                result = rule.apply(result)\n        return result\n    \n    def add_rule(self, rule: TransformationRule):\n        \"\"\"Add a rule to the chain\"\"\"\n        self.rules.append(rule)\n    \n    def remove_rule(self, index: int):\n        \"\"\"Remove a rule from the chain\"\"\"\n        if 0 <= index < len(self.rules):\n            self.rules.pop(index)\n\n\nclass TransformationInference:\n    \"\"\"Infer transformation rules from examples\"\"\"\n    \n    @staticmethod\n    def infer_simple_transformation(input_grid: Grid, output_grid: Grid) -> Optional[TransformationRule]:\n        \"\"\"Try to infer a simple transformation between input and output\"\"\"\n        \n        # Check for rotation\n        for degrees in [90, 180, 270]:\n            if input_grid.rotate(degrees) == output_grid:\n                return TransformationRule(\n                    name=\"rotation\",\n                    transformation_type=TransformationType.ROTATE,\n                    parameters={'degrees': degrees},\n                    description=f\"Rotate {degrees} degrees\"\n                )\n        \n        # Check for flips\n        if input_grid.flip_horizontal() == output_grid:\n            return TransformationRule(\n                name=\"horizontal_flip\",\n                transformation_type=TransformationType.FLIP,\n                parameters={'direction': 'horizontal'},\n                description=\"Flip horizontally\"\n            )\n        \n        if input_grid.flip_vertical() == output_grid:\n            return TransformationRule(\n                name=\"vertical_flip\",\n                transformation_type=TransformationType.FLIP,\n                parameters={'direction': 'vertical'},\n                description=\"Flip vertically\"\n            )\n        \n        # Check for transpose\n        if input_grid.transpose() == output_grid:\n            return TransformationRule(\n                name=\"transpose\",\n                transformation_type=TransformationType.TRANSPOSE,\n                parameters={},\n                description=\"Transpose matrix\"\n            )\n        \n        # Check for scaling\n        if output_grid.height % input_grid.height == 0 and output_grid.width % input_grid.width == 0:\n            if output_grid.height // input_grid.height == output_grid.width // input_grid.width:\n                factor = output_grid.height // input_grid.height\n                if input_grid.scale(factor) == output_grid:\n                    return TransformationRule(\n                        name=\"scale\",\n                        transformation_type=TransformationType.SCALE,\n                        parameters={'factor': factor},\n                        description=f\"Scale by factor {factor}\"\n                    )\n        \n        # Check for tiling\n        if output_grid.height >= input_grid.height and output_grid.width >= input_grid.width:\n            if output_grid.height % input_grid.height == 0 and output_grid.width % input_grid.width == 0:\n                rows = output_grid.height // input_grid.height\n                cols = output_grid.width // input_grid.width\n                if input_grid.tile(rows, cols) == output_grid:\n                    return TransformationRule(\n                        name=\"tile\",\n                        transformation_type=TransformationType.TILE,\n                        parameters={'rows': rows, 'cols': cols},\n                        description=f\"Tile {rows}x{cols}\"\n                    )\n        \n        # Check for color mapping\n        input_colors = input_grid.unique_colors\n        output_colors = output_grid.unique_colors\n        \n        if input_grid.shape == output_grid.shape:\n            # Try to find color mapping\n            color_map = {}\n            is_color_map = True\n            \n            for y in range(input_grid.height):\n                for x in range(input_grid.width):\n                    input_color = input_grid.data[y, x]\n                    output_color = output_grid.data[y, x]\n                    \n                    if input_color in color_map:\n                        if color_map[input_color] != output_color:\n                            is_color_map = False\n                            break\n                    else:\n                        color_map[input_color] = output_color\n                \n                if not is_color_map:\n                    break\n            \n            if is_color_map and color_map:\n                return TransformationRule(\n                    name=\"color_map\",\n                    transformation_type=TransformationType.COLOR_MAP,\n                    parameters={'color_map': color_map},\n                    description=f\"Map colors: {color_map}\"\n                )\n        \n        return None\n    \n    @staticmethod\n    def infer_from_examples(examples: List[Tuple[Grid, Grid]]) -> List[TransformationRule]:\n        \"\"\"Infer transformation rules from multiple examples\"\"\"\n        rules = []\n        \n        # Try to find consistent simple transformations\n        for input_grid, output_grid in examples:\n            rule = TransformationInference.infer_simple_transformation(input_grid, output_grid)\n            if rule:\n                # Check if this rule works for all examples\n                is_consistent = True\n                for test_input, test_output in examples:\n                    if rule.apply(test_input) != test_output:\n                        is_consistent = False\n                        break\n                \n                if is_consistent:\n                    rules.append(rule)\n                    return rules  # Found a single consistent rule\n        \n        # If no single rule works, try to find composite rules\n        rules = TransformationInference._infer_composite_rules(examples)\n        \n        return rules\n    \n    @staticmethod\n    def _infer_composite_rules(examples: List[Tuple[Grid, Grid]]) -> List[TransformationRule]:\n        \"\"\"Try to infer composite transformation rules\"\"\"\n        rules = []\n        \n        # Check for common patterns across examples\n        for input_grid, output_grid in examples:\n            detector_in = PatternDetector(input_grid)\n            detector_out = PatternDetector(output_grid)\n            \n            comparison = detector_in.compare_with(output_grid)\n            \n            # Build composite rule based on comparison\n            if comparison['is_scaled'] and comparison['scale_factor']:\n                rules.append(TransformationRule(\n                    name=\"scale\",\n                    transformation_type=TransformationType.SCALE,\n                    parameters={'factor': comparison['scale_factor']},\n                    description=f\"Scale by {comparison['scale_factor']}\"\n                ))\n            \n            if comparison['is_rotated'] and comparison['rotation_angle']:\n                rules.append(TransformationRule(\n                    name=\"rotate\",\n                    transformation_type=TransformationType.ROTATE,\n                    parameters={'degrees': comparison['rotation_angle']},\n                    description=f\"Rotate {comparison['rotation_angle']} degrees\"\n                ))\n            \n            if comparison['is_flipped'] and comparison['flip_type']:\n                rules.append(TransformationRule(\n                    name=\"flip\",\n                    transformation_type=TransformationType.FLIP,\n                    parameters={'direction': comparison['flip_type']},\n                    description=f\"Flip {comparison['flip_type']}\"\n                ))\n            \n            # Only return rules if they're consistent across all examples\n            if rules:\n                chain = RuleChain(rules)\n                is_consistent = all(\n                    chain.apply(inp) == out \n                    for inp, out in examples\n                )\n                if is_consistent:\n                    return rules\n        \n        return []\n\n\nclass RuleLibrary:\n    \"\"\"Library of common transformation rules\"\"\"\n    \n    @staticmethod\n    def get_rotation_rules() -> List[TransformationRule]:\n        \"\"\"Get all rotation rules\"\"\"\n        return [\n            TransformationRule(\n                name=f\"rotate_{degrees}\",\n                transformation_type=TransformationType.ROTATE,\n                parameters={'degrees': degrees},\n                description=f\"Rotate {degrees} degrees clockwise\"\n            )\n            for degrees in [90, 180, 270]\n        ]\n    \n    @staticmethod\n    def get_flip_rules() -> List[TransformationRule]:\n        \"\"\"Get all flip rules\"\"\"\n        return [\n            TransformationRule(\n                name=\"flip_horizontal\",\n                transformation_type=TransformationType.FLIP,\n                parameters={'direction': 'horizontal'},\n                description=\"Flip horizontally (left-right)\"\n            ),\n            TransformationRule(\n                name=\"flip_vertical\",\n                transformation_type=TransformationType.FLIP,\n                parameters={'direction': 'vertical'},\n                description=\"Flip vertically (top-bottom)\"\n            )\n        ]\n    \n    @staticmethod\n    def get_scale_rule(factor: int) -> TransformationRule:\n        \"\"\"Get a scale rule with specified factor\"\"\"\n        return TransformationRule(\n            name=f\"scale_{factor}x\",\n            transformation_type=TransformationType.SCALE,\n            parameters={'factor': factor},\n            description=f\"Scale by factor of {factor}\"\n        )\n    \n    @staticmethod\n    def get_tile_rule(rows: int, cols: int) -> TransformationRule:\n        \"\"\"Get a tile rule with specified dimensions\"\"\"\n        return TransformationRule(\n            name=f\"tile_{rows}x{cols}\",\n            transformation_type=TransformationType.TILE,\n            parameters={'rows': rows, 'cols': cols},\n            description=f\"Tile in a {rows}x{cols} pattern\"\n        )\n    \n    @staticmethod\n    def get_color_swap_rule(color1: int, color2: int) -> RuleChain:\n        \"\"\"Get a rule chain that swaps two colors\"\"\"\n        temp_color = 10  # Temporary color outside normal range\n        \n        return RuleChain([\n            TransformationRule(\n                name=f\"swap_{color1}_{color2}_step1\",\n                transformation_type=TransformationType.COLOR_REPLACE,\n                parameters={'old_color': color1, 'new_color': temp_color},\n                description=f\"Replace {color1} with temp\"\n            ),\n            TransformationRule(\n                name=f\"swap_{color1}_{color2}_step2\",\n                transformation_type=TransformationType.COLOR_REPLACE,\n                parameters={'old_color': color2, 'new_color': color1},\n                description=f\"Replace {color2} with {color1}\"\n            ),\n            TransformationRule(\n                name=f\"swap_{color1}_{color2}_step3\",\n                transformation_type=TransformationType.COLOR_REPLACE,\n                parameters={'old_color': temp_color, 'new_color': color2},\n                description=f\"Replace temp with {color2}\"\n            )\n        ])\n    \n    @staticmethod\n    def get_common_rules() -> Dict[str, TransformationRule]:\n        \"\"\"Get a dictionary of common transformation rules\"\"\"\n        rules = {}\n        \n        # Add rotation rules\n        for rule in RuleLibrary.get_rotation_rules():\n            rules[rule.name] = rule\n        \n        # Add flip rules\n        for rule in RuleLibrary.get_flip_rules():\n            rules[rule.name] = rule\n        \n        # Add transpose\n        rules['transpose'] = TransformationRule(\n            name=\"transpose\",\n            transformation_type=TransformationType.TRANSPOSE,\n            parameters={},\n            description=\"Transpose (swap rows and columns)\"\n        )\n        \n        # Add common scale rules\n        for factor in [2, 3, 4]:\n            rule = RuleLibrary.get_scale_rule(factor)\n            rules[rule.name] = rule\n        \n        # Add crop rule\n        rules['crop'] = TransformationRule(\n            name=\"crop\",\n            transformation_type=TransformationType.CROP,\n            parameters={'background': 0},\n            description=\"Crop to content (remove background borders)\"\n        )\n        \n        return rules","size_bytes":17081},"src/csp/__init__.py":{"content":"\"\"\"\nCSP (Constraint Satisfaction Problem) module for ARC Prize 2025.\n\nThis module provides a comprehensive framework for solving constraint satisfaction\nproblems, with a focus on grid-based puzzles and pattern recognition tasks.\n\"\"\"\n\n# Core CSP classes\nfrom .core import (\n    CSP,\n    Variable,\n    Domain,\n    Constraint,\n    UnaryConstraint,\n    BinaryConstraint,\n    NAryConstraint,\n    ConstraintType,\n    VariableName,\n    DomainValue,\n    create_grid_csp,\n    create_pattern_csp\n)\n\n# Arc consistency algorithms\nfrom .arc_consistency import (\n    AC3,\n    AC4,\n    MAC,\n    ArcConsistencyStats,\n    achieve_arc_consistency,\n    is_arc_consistent,\n    PathConsistency\n)\n\n# Search algorithms\nfrom .search import (\n    BacktrackingSearch,\n    ForwardCheckingSearch,\n    MACSearch,\n    MinConflicts,\n    HybridSearch,\n    VariableOrdering,\n    ValueOrdering,\n    SearchStats,\n    solve_csp,\n    find_all_solutions\n)\n\n# ARC-specific constraints\nfrom .constraints import (\n    AllDifferentConstraint,\n    SamePatternConstraint,\n    TransformationConstraint,\n    ColorMappingConstraint,\n    SymmetryConstraint,\n    AdjacentConstraint,\n    CountConstraint,\n    ConnectedComponentConstraint,\n    RegionConstraint,\n    create_sudoku_constraints\n)\n\n__version__ = \"1.0.0\"\n\n__all__ = [\n    # Core\n    'CSP',\n    'Variable',\n    'Domain',\n    'Constraint',\n    'UnaryConstraint',\n    'BinaryConstraint',\n    'NAryConstraint',\n    'ConstraintType',\n    'VariableName',\n    'DomainValue',\n    'create_grid_csp',\n    'create_pattern_csp',\n    \n    # Arc consistency\n    'AC3',\n    'AC4',\n    'MAC',\n    'ArcConsistencyStats',\n    'achieve_arc_consistency',\n    'is_arc_consistent',\n    'PathConsistency',\n    \n    # Search\n    'BacktrackingSearch',\n    'ForwardCheckingSearch',\n    'MACSearch',\n    'MinConflicts',\n    'HybridSearch',\n    'VariableOrdering',\n    'ValueOrdering',\n    'SearchStats',\n    'solve_csp',\n    'find_all_solutions',\n    \n    # Constraints\n    'AllDifferentConstraint',\n    'SamePatternConstraint',\n    'TransformationConstraint',\n    'ColorMappingConstraint',\n    'SymmetryConstraint',\n    'AdjacentConstraint',\n    'CountConstraint',\n    'ConnectedComponentConstraint',\n    'RegionConstraint',\n    'create_sudoku_constraints'\n]","size_bytes":2243},"src/csp/arc_consistency.py":{"content":"\"\"\"\nArc Consistency algorithms for CSP solving in ARC Prize 2025.\n\nThis module implements various arc consistency algorithms for efficient\nconstraint propagation in grid-based puzzles.\n\"\"\"\n\nfrom typing import Dict, List, Set, Tuple, Optional, Deque\nfrom collections import deque, defaultdict\nfrom dataclasses import dataclass\nimport time\n\nfrom .core import (\n    CSP, Variable, Domain, Constraint, BinaryConstraint,\n    VariableName, DomainValue\n)\n\n\n@dataclass\nclass ArcConsistencyStats:\n    \"\"\"Statistics for arc consistency algorithms.\"\"\"\n    arcs_examined: int = 0\n    domain_reductions: int = 0\n    time_elapsed: float = 0.0\n    iterations: int = 0\n    \n    def reset(self):\n        \"\"\"Reset all statistics.\"\"\"\n        self.arcs_examined = 0\n        self.domain_reductions = 0\n        self.time_elapsed = 0.0\n        self.iterations = 0\n\n\nclass AC3:\n    \"\"\"\n    AC-3 (Arc Consistency Algorithm #3) implementation.\n    \n    This is the most popular arc consistency algorithm, with O(edÂ³) complexity\n    where e is the number of edges and d is the maximum domain size.\n    \"\"\"\n    \n    def __init__(self, csp: CSP):\n        \"\"\"\n        Initialize AC-3 algorithm.\n        \n        Args:\n            csp: The CSP to apply arc consistency to\n        \"\"\"\n        self.csp = csp\n        self.stats = ArcConsistencyStats()\n    \n    def run(self, domains: Optional[Domain] = None) -> bool:\n        \"\"\"\n        Run AC-3 algorithm to achieve arc consistency.\n        \n        Args:\n            domains: Domain manager (uses CSP's domains if not provided)\n            \n        Returns:\n            False if inconsistency detected, True otherwise\n        \"\"\"\n        start_time = time.time()\n        self.stats.reset()\n        \n        if domains is None:\n            domains = self.csp.domains\n        \n        # Initialize queue with all arcs\n        queue = self._initialize_queue()\n        \n        while queue:\n            self.stats.iterations += 1\n            xi, xj = queue.popleft()\n            self.stats.arcs_examined += 1\n            \n            if self._revise(xi, xj, domains):\n                if domains.is_empty(xi):\n                    # Domain wipeout - inconsistency detected\n                    self.stats.time_elapsed = time.time() - start_time\n                    return False\n                \n                # Add all arcs (xk, xi) for each neighbor xk of xi\n                for constraint in self.csp.get_constraints_for_variable(xi):\n                    if isinstance(constraint, BinaryConstraint):\n                        for var in constraint.variables:\n                            if var != xi and var != xj:\n                                queue.append((var, xi))\n        \n        self.stats.time_elapsed = time.time() - start_time\n        return True\n    \n    def _initialize_queue(self) -> Deque[Tuple[VariableName, VariableName]]:\n        \"\"\"\n        Initialize the queue with all arcs from binary constraints.\n        \n        Returns:\n            Deque containing all arcs\n        \"\"\"\n        queue = deque()\n        seen = set()\n        \n        for constraint in self.csp.constraints:\n            if isinstance(constraint, BinaryConstraint):\n                var1, var2 = constraint.variables\n                \n                # Add both directions of the arc\n                if (var1, var2) not in seen:\n                    queue.append((var1, var2))\n                    seen.add((var1, var2))\n                \n                if (var2, var1) not in seen:\n                    queue.append((var2, var1))\n                    seen.add((var2, var1))\n        \n        return queue\n    \n    def _revise(self, xi: VariableName, xj: VariableName, domains: Domain) -> bool:\n        \"\"\"\n        Revise the domain of xi with respect to xj.\n        \n        Remove values from domain of xi that have no support in domain of xj.\n        \n        Args:\n            xi: Variable whose domain is being revised\n            xj: Variable providing support\n            domains: Current domains\n            \n        Returns:\n            True if domain of xi was changed\n        \"\"\"\n        revised = False\n        xi_domain = domains.get(xi).copy()\n        xj_domain = domains.get(xj)\n        \n        # Get constraints between xi and xj\n        constraints = self.csp.get_binary_constraints(xi, xj)\n        \n        for vi in xi_domain:\n            has_support = False\n            \n            # Check if vi has support in xj's domain\n            for vj in xj_domain:\n                # Check all constraints between xi and xj\n                all_satisfied = True\n                for constraint in constraints:\n                    # Determine order of variables in constraint\n                    if constraint.variables[0] == xi:\n                        if not constraint.relation(vi, vj):\n                            all_satisfied = False\n                            break\n                    else:\n                        if not constraint.relation(vj, vi):\n                            all_satisfied = False\n                            break\n                \n                if all_satisfied and constraints:  # Found support\n                    has_support = True\n                    break\n            \n            # If no constraints between xi and xj, everything has support\n            if not constraints:\n                has_support = True\n            \n            if not has_support:\n                domains.remove(xi, vi)\n                revised = True\n                self.stats.domain_reductions += 1\n        \n        return revised\n    \n    def get_stats(self) -> ArcConsistencyStats:\n        \"\"\"Get algorithm statistics.\"\"\"\n        return self.stats\n\n\nclass AC4:\n    \"\"\"\n    AC-4 (Arc Consistency Algorithm #4) implementation.\n    \n    More complex but optimal O(edÂ²) algorithm, better for dense constraint graphs.\n    Uses support counters to avoid redundant checks.\n    \"\"\"\n    \n    def __init__(self, csp: CSP):\n        \"\"\"\n        Initialize AC-4 algorithm.\n        \n        Args:\n            csp: The CSP to apply arc consistency to\n        \"\"\"\n        self.csp = csp\n        self.stats = ArcConsistencyStats()\n    \n    def run(self, domains: Optional[Domain] = None) -> bool:\n        \"\"\"\n        Run AC-4 algorithm to achieve arc consistency.\n        \n        Args:\n            domains: Domain manager (uses CSP's domains if not provided)\n            \n        Returns:\n            False if inconsistency detected, True otherwise\n        \"\"\"\n        start_time = time.time()\n        self.stats.reset()\n        \n        if domains is None:\n            domains = self.csp.domains\n        \n        # Initialize support structures\n        support = self._initialize_support(domains)\n        counter = self._initialize_counter(domains, support)\n        \n        # Initialize queue with unsupported values\n        queue = deque()\n        \n        for (xi, vi), supporting_pairs in support.items():\n            if not supporting_pairs:\n                queue.append((xi, vi))\n                if not domains.remove(xi, vi):\n                    continue  # Value already removed\n                self.stats.domain_reductions += 1\n        \n        # Propagate removals\n        while queue:\n            self.stats.iterations += 1\n            xi, vi = queue.popleft()\n            \n            # Find all (xj, vj) supported by (xi, vi)\n            for constraint in self.csp.get_constraints_for_variable(xi):\n                if not isinstance(constraint, BinaryConstraint):\n                    continue\n                \n                xj = None\n                for var in constraint.variables:\n                    if var != xi:\n                        xj = var\n                        break\n                \n                if xj is None:\n                    continue\n                \n                for vj in domains.get(xj).copy():\n                    # Check if (xi, vi) supports (xj, vj)\n                    if self._is_support(xi, vi, xj, vj, constraint):\n                        counter[(xj, vj, xi)] -= 1\n                        \n                        if counter[(xj, vj, xi)] == 0:\n                            # No more support for (xj, vj) from xi\n                            if vj in domains.get(xj):\n                                queue.append((xj, vj))\n                                domains.remove(xj, vj)\n                                self.stats.domain_reductions += 1\n                                \n                                if domains.is_empty(xj):\n                                    self.stats.time_elapsed = time.time() - start_time\n                                    return False\n        \n        self.stats.time_elapsed = time.time() - start_time\n        return True\n    \n    def _initialize_support(self, domains: Domain) -> Dict[Tuple[VariableName, DomainValue], \n                                                           Set[Tuple[VariableName, DomainValue]]]:\n        \"\"\"\n        Initialize support sets for each (variable, value) pair.\n        \n        Returns:\n            Dictionary mapping (var, val) to set of supporting (var, val) pairs\n        \"\"\"\n        support = defaultdict(set)\n        \n        for constraint in self.csp.constraints:\n            if not isinstance(constraint, BinaryConstraint):\n                continue\n            \n            xi, xj = constraint.variables\n            \n            for vi in domains.get(xi):\n                for vj in domains.get(xj):\n                    if constraint.relation(vi, vj):\n                        support[(xi, vi)].add((xj, vj))\n                        support[(xj, vj)].add((xi, vi))\n        \n        return support\n    \n    def _initialize_counter(self, domains: Domain, \n                           support: Dict[Tuple[VariableName, DomainValue], \n                                       Set[Tuple[VariableName, DomainValue]]]) -> Dict:\n        \"\"\"\n        Initialize counters for support counts.\n        \n        Returns:\n            Dictionary mapping (xi, vi, xj) to count of supports from xj\n        \"\"\"\n        counter = defaultdict(int)\n        \n        for (xi, vi), supporting_pairs in support.items():\n            support_by_var = defaultdict(int)\n            for xj, vj in supporting_pairs:\n                support_by_var[xj] += 1\n            \n            for xj, count in support_by_var.items():\n                counter[(xi, vi, xj)] = count\n        \n        # Ensure all (xi, vi, xj) combinations have an entry\n        for constraint in self.csp.constraints:\n            if not isinstance(constraint, BinaryConstraint):\n                continue\n            \n            xi, xj = constraint.variables\n            \n            for vi in domains.get(xi):\n                if (xi, vi, xj) not in counter:\n                    counter[(xi, vi, xj)] = 0\n            \n            for vj in domains.get(xj):\n                if (xj, vj, xi) not in counter:\n                    counter[(xj, vj, xi)] = 0\n        \n        return counter\n    \n    def _is_support(self, xi: VariableName, vi: DomainValue,\n                   xj: VariableName, vj: DomainValue,\n                   constraint: BinaryConstraint) -> bool:\n        \"\"\"Check if (xi, vi) supports (xj, vj) under the constraint.\"\"\"\n        if constraint.variables[0] == xi:\n            return constraint.relation(vi, vj)\n        else:\n            return constraint.relation(vj, vi)\n    \n    def get_stats(self) -> ArcConsistencyStats:\n        \"\"\"Get algorithm statistics.\"\"\"\n        return self.stats\n\n\nclass MAC:\n    \"\"\"\n    Maintaining Arc Consistency (MAC) algorithm.\n    \n    Combines search with arc consistency maintenance for efficient solving.\n    \"\"\"\n    \n    def __init__(self, csp: CSP, ac_algorithm: str = 'ac3'):\n        \"\"\"\n        Initialize MAC algorithm.\n        \n        Args:\n            csp: The CSP to solve\n            ac_algorithm: Which AC algorithm to use ('ac3' or 'ac4')\n        \"\"\"\n        self.csp = csp\n        self.ac_algorithm = ac_algorithm\n        self.stats = ArcConsistencyStats()\n    \n    def maintain_arc_consistency(self, var: VariableName, value: DomainValue,\n                                assignment: Dict[VariableName, DomainValue],\n                                domains: Domain) -> bool:\n        \"\"\"\n        Maintain arc consistency after assigning var=value.\n        \n        Args:\n            var: Variable being assigned\n            value: Value being assigned\n            assignment: Current assignment\n            domains: Current domains\n            \n        Returns:\n            False if inconsistency detected, True otherwise\n        \"\"\"\n        # Create AC algorithm instance\n        if self.ac_algorithm == 'ac4':\n            ac = AC4(self.csp)\n        else:\n            ac = AC3(self.csp)\n        \n        # First, reduce domain of var to just the assigned value\n        domains.set(var, {value})\n        \n        # Run arc consistency from this variable\n        queue = deque()\n        \n        # Add all arcs (neighbor, var) to queue\n        for constraint in self.csp.get_constraints_for_variable(var):\n            if isinstance(constraint, BinaryConstraint):\n                for other_var in constraint.variables:\n                    if other_var != var and other_var not in assignment:\n                        queue.append((other_var, var))\n        \n        # Run limited AC-3 starting from affected arcs\n        return self._ac3_from_queue(queue, domains)\n    \n    def _ac3_from_queue(self, queue: Deque[Tuple[VariableName, VariableName]],\n                       domains: Domain) -> bool:\n        \"\"\"\n        Run AC-3 starting from a specific queue of arcs.\n        \n        Args:\n            queue: Initial queue of arcs to process\n            domains: Current domains\n            \n        Returns:\n            False if inconsistency detected, True otherwise\n        \"\"\"\n        processed = set()\n        \n        while queue:\n            self.stats.iterations += 1\n            xi, xj = queue.popleft()\n            \n            # Skip if already processed\n            if (xi, xj) in processed:\n                continue\n            processed.add((xi, xj))\n            \n            self.stats.arcs_examined += 1\n            \n            if self._revise(xi, xj, domains):\n                if domains.is_empty(xi):\n                    return False\n                \n                # Add all arcs (xk, xi) for neighbors xk != xj\n                for constraint in self.csp.get_constraints_for_variable(xi):\n                    if isinstance(constraint, BinaryConstraint):\n                        for var in constraint.variables:\n                            if var != xi and var != xj:\n                                if (var, xi) not in processed:\n                                    queue.append((var, xi))\n        \n        return True\n    \n    def _revise(self, xi: VariableName, xj: VariableName, domains: Domain) -> bool:\n        \"\"\"\n        Revise domain of xi with respect to xj (same as AC-3).\n        \n        Args:\n            xi: Variable whose domain is being revised\n            xj: Variable providing support\n            domains: Current domains\n            \n        Returns:\n            True if domain was changed\n        \"\"\"\n        revised = False\n        xi_domain = domains.get(xi).copy()\n        xj_domain = domains.get(xj)\n        \n        constraints = self.csp.get_binary_constraints(xi, xj)\n        \n        for vi in xi_domain:\n            has_support = False\n            \n            for vj in xj_domain:\n                all_satisfied = True\n                for constraint in constraints:\n                    if constraint.variables[0] == xi:\n                        if not constraint.relation(vi, vj):\n                            all_satisfied = False\n                            break\n                    else:\n                        if not constraint.relation(vj, vi):\n                            all_satisfied = False\n                            break\n                \n                if all_satisfied and constraints:\n                    has_support = True\n                    break\n            \n            if not constraints:\n                has_support = True\n            \n            if not has_support:\n                domains.remove(xi, vi)\n                revised = True\n                self.stats.domain_reductions += 1\n        \n        return revised\n    \n    def get_stats(self) -> ArcConsistencyStats:\n        \"\"\"Get algorithm statistics.\"\"\"\n        return self.stats\n\n\ndef achieve_arc_consistency(csp: CSP, algorithm: str = 'ac3',\n                           domains: Optional[Domain] = None) -> bool:\n    \"\"\"\n    Achieve arc consistency using specified algorithm.\n    \n    Args:\n        csp: The CSP to make arc consistent\n        algorithm: Algorithm to use ('ac3' or 'ac4')\n        domains: Domain manager (uses CSP's domains if not provided)\n        \n    Returns:\n        False if inconsistency detected, True otherwise\n    \"\"\"\n    if algorithm == 'ac4':\n        ac = AC4(csp)\n    else:\n        ac = AC3(csp)\n    \n    return ac.run(domains)\n\n\ndef is_arc_consistent(csp: CSP, domains: Optional[Domain] = None) -> bool:\n    \"\"\"\n    Check if a CSP is already arc consistent.\n    \n    Args:\n        csp: The CSP to check\n        domains: Domain manager (uses CSP's domains if not provided)\n        \n    Returns:\n        True if arc consistent, False otherwise\n    \"\"\"\n    if domains is None:\n        domains = csp.domains\n    \n    for constraint in csp.constraints:\n        if not isinstance(constraint, BinaryConstraint):\n            continue\n        \n        xi, xj = constraint.variables\n        \n        # Check if every value in xi has support in xj\n        for vi in domains.get(xi):\n            has_support = False\n            for vj in domains.get(xj):\n                if constraint.relation(vi, vj):\n                    has_support = True\n                    break\n            \n            if not has_support:\n                return False\n        \n        # Check if every value in xj has support in xi\n        for vj in domains.get(xj):\n            has_support = False\n            for vi in domains.get(xi):\n                if constraint.relation(vi, vj):\n                    has_support = True\n                    break\n            \n            if not has_support:\n                return False\n    \n    return True\n\n\nclass PathConsistency:\n    \"\"\"\n    Path Consistency algorithm for stronger consistency than arc consistency.\n    \n    Ensures that for any consistent assignment to two variables,\n    there exists a consistent assignment to any third variable.\n    \"\"\"\n    \n    def __init__(self, csp: CSP):\n        \"\"\"Initialize path consistency algorithm.\"\"\"\n        self.csp = csp\n        self.stats = ArcConsistencyStats()\n    \n    def run(self, domains: Optional[Domain] = None) -> bool:\n        \"\"\"\n        Achieve path consistency.\n        \n        Note: This is computationally expensive and typically not used\n        for large problems like 30x30 grids.\n        \n        Args:\n            domains: Domain manager\n            \n        Returns:\n            False if inconsistency detected, True otherwise\n        \"\"\"\n        start_time = time.time()\n        self.stats.reset()\n        \n        if domains is None:\n            domains = self.csp.domains\n        \n        variables = list(self.csp.variables.keys())\n        n = len(variables)\n        \n        # Initialize allowed tuples for each pair of variables\n        allowed = {}\n        for i in range(n):\n            for j in range(i + 1, n):\n                xi, xj = variables[i], variables[j]\n                allowed[(xi, xj)] = set()\n                \n                # Find all consistent value pairs\n                for vi in domains.get(xi):\n                    for vj in domains.get(xj):\n                        # Check if this pair satisfies all binary constraints\n                        assignment = {xi: vi, xj: vj}\n                        if self._check_binary_constraints(xi, xj, assignment):\n                            allowed[(xi, xj)].add((vi, vj))\n        \n        # Path consistency main loop\n        changed = True\n        while changed:\n            changed = False\n            self.stats.iterations += 1\n            \n            for i in range(n):\n                for j in range(i + 1, n):\n                    for k in range(n):\n                        if k == i or k == j:\n                            continue\n                        \n                        xi, xj, xk = variables[i], variables[j], variables[k]\n                        \n                        # Revise allowed[(xi, xj)] based on xk\n                        old_size = len(allowed.get((xi, xj), set()))\n                        self._revise_path(xi, xj, xk, allowed)\n                        \n                        if len(allowed.get((xi, xj), set())) < old_size:\n                            changed = True\n                            self.stats.domain_reductions += old_size - len(allowed.get((xi, xj), set()))\n                            \n                            if not allowed.get((xi, xj), set()):\n                                # No consistent pairs left\n                                self.stats.time_elapsed = time.time() - start_time\n                                return False\n        \n        # Update domains based on path consistency\n        for i in range(n):\n            xi = variables[i]\n            new_domain = set()\n            \n            for vi in domains.get(xi):\n                has_support = True\n                \n                for j in range(n):\n                    if i == j:\n                        continue\n                    \n                    xj = variables[j]\n                    pair_key = (xi, xj) if i < j else (xj, xi)\n                    \n                    # Check if vi participates in any allowed tuple with xj\n                    found = False\n                    for tuple_val in allowed.get(pair_key, set()):\n                        if i < j and tuple_val[0] == vi:\n                            found = True\n                            break\n                        elif i > j and tuple_val[1] == vi:\n                            found = True\n                            break\n                    \n                    if not found:\n                        has_support = False\n                        break\n                \n                if has_support:\n                    new_domain.add(vi)\n            \n            domains.set(xi, new_domain)\n        \n        self.stats.time_elapsed = time.time() - start_time\n        return True\n    \n    def _check_binary_constraints(self, xi: VariableName, xj: VariableName,\n                                 assignment: Dict[VariableName, DomainValue]) -> bool:\n        \"\"\"Check if assignment satisfies all binary constraints between xi and xj.\"\"\"\n        constraints = self.csp.get_binary_constraints(xi, xj)\n        \n        for constraint in constraints:\n            if not constraint.is_satisfied(assignment):\n                return False\n        \n        return True\n    \n    def _revise_path(self, xi: VariableName, xj: VariableName, xk: VariableName,\n                    allowed: Dict) -> None:\n        \"\"\"Revise allowed tuples for (xi, xj) based on xk.\"\"\"\n        pair_key = (xi, xj)\n        ik_key = (xi, xk) if xi < xk else (xk, xi)\n        jk_key = (xj, xk) if xj < xk else (xk, xj)\n        \n        new_allowed = set()\n        \n        for vi, vj in allowed.get(pair_key, set()):\n            # Check if there exists vk such that (vi, vk) and (vj, vk) are allowed\n            found_support = False\n            \n            for vk in self.csp.domains.get(xk):\n                # Check (xi, xk) compatibility\n                if xi < xk:\n                    ik_compatible = (vi, vk) in allowed.get(ik_key, set())\n                else:\n                    ik_compatible = (vk, vi) in allowed.get(ik_key, set())\n                \n                # Check (xj, xk) compatibility\n                if xj < xk:\n                    jk_compatible = (vj, vk) in allowed.get(jk_key, set())\n                else:\n                    jk_compatible = (vk, vj) in allowed.get(jk_key, set())\n                \n                if ik_compatible and jk_compatible:\n                    found_support = True\n                    break\n            \n            if found_support:\n                new_allowed.add((vi, vj))\n        \n        allowed[pair_key] = new_allowed","size_bytes":24378},"src/csp/constraints.py":{"content":"\"\"\"\nARC-specific constraints for CSP solving in ARC Prize 2025.\n\nThis module provides specialized constraints for grid-based puzzles\nand pattern recognition tasks.\n\"\"\"\n\nfrom typing import Dict, List, Set, Tuple, Optional, Callable, Any\nfrom abc import ABC, abstractmethod\nimport numpy as np\nfrom collections import defaultdict, Counter\nfrom itertools import combinations, permutations\n\nfrom .core import (\n    Constraint, BinaryConstraint, NAryConstraint,\n    Variable, Domain, VariableName, DomainValue\n)\n\n\nclass FunctionConstraint(NAryConstraint):\n    \"\"\"\n    General-purpose constraint defined by a custom function.\n    \n    Useful for complex constraints that don't fit standard patterns.\n    \"\"\"\n    \n    def __init__(self, variables: List[VariableName], \n                 function: Callable[[Dict[VariableName, DomainValue]], bool],\n                 name: Optional[str] = None):\n        \"\"\"\n        Initialize function constraint.\n        \n        Args:\n            variables: Variables involved in the constraint\n            function: Function that checks if assignment is valid\n            name: Optional name for the constraint\n        \"\"\"\n        super().__init__(variables, function, name or \"FunctionConstraint\")\n\n\nclass AllDifferentConstraint(NAryConstraint):\n    \"\"\"\n    Constraint requiring all variables to have different values.\n    \n    Useful for puzzles where each cell in a region must have a unique color.\n    \"\"\"\n    \n    def __init__(self, variables: List[VariableName], name: Optional[str] = None):\n        \"\"\"\n        Initialize all-different constraint.\n        \n        Args:\n            variables: List of variables that must all be different\n            name: Optional name for the constraint\n        \"\"\"\n        def all_different_predicate(assignment: Dict[VariableName, DomainValue]) -> bool:\n            values = list(assignment.values())\n            return len(values) == len(set(values))\n        \n        super().__init__(variables, all_different_predicate, \n                        name or f\"AllDifferent({','.join(variables[:3])}...)\")\n    \n    def prune_domains(self, domains: Domain) -> bool:\n        \"\"\"\n        Specialized pruning for all-different constraint.\n        \n        Uses the pigeonhole principle and singleton propagation.\n        \"\"\"\n        changed = False\n        \n        # Collect singleton values (variables with domain size 1)\n        singleton_values = set()\n        for var in self.variables:\n            if domains.is_singleton(var):\n                singleton_values.add(domains.get_singleton_value(var))\n        \n        # Remove singleton values from other variables' domains\n        for var in self.variables:\n            if not domains.is_singleton(var):\n                for value in singleton_values:\n                    if domains.remove(var, value):\n                        changed = True\n        \n        # Check for pigeonhole principle violations\n        remaining_vars = [v for v in self.variables if not domains.is_singleton(v)]\n        if len(remaining_vars) > 0:\n            # Collect all possible values\n            all_values = set()\n            for var in remaining_vars:\n                all_values.update(domains.get(var))\n            \n            # If there are more variables than values, no solution exists\n            if len(remaining_vars) > len(all_values):\n                # Create domain wipeout\n                for var in remaining_vars:\n                    domains.set(var, set())\n                return True\n        \n        # Hidden singles: if a value appears in only one domain, assign it\n        value_to_vars = defaultdict(list)\n        for var in self.variables:\n            if not domains.is_singleton(var):\n                for value in domains.get(var):\n                    value_to_vars[value].append(var)\n        \n        for value, vars_with_value in value_to_vars.items():\n            if len(vars_with_value) == 1:\n                var = vars_with_value[0]\n                if len(domains.get(var)) > 1:\n                    domains.set(var, {value})\n                    changed = True\n        \n        return changed\n\n\nclass SamePatternConstraint(NAryConstraint):\n    \"\"\"\n    Constraint requiring variables to follow the same pattern as a reference.\n    \n    Useful for pattern matching and repetition detection in ARC puzzles.\n    \"\"\"\n    \n    def __init__(self, variables: List[VariableName], \n                 reference_pattern: List[Any],\n                 allow_rotation: bool = False,\n                 allow_reflection: bool = False,\n                 name: Optional[str] = None):\n        \"\"\"\n        Initialize same-pattern constraint.\n        \n        Args:\n            variables: Variables that should match the pattern\n            reference_pattern: The pattern to match\n            allow_rotation: Whether to allow rotated versions\n            allow_reflection: Whether to allow reflected versions\n            name: Optional name for the constraint\n        \"\"\"\n        self.reference_pattern = reference_pattern\n        self.allow_rotation = allow_rotation\n        self.allow_reflection = allow_reflection\n        \n        # Generate all valid transformations of the pattern\n        self.valid_patterns = self._generate_pattern_variants(reference_pattern)\n        \n        def pattern_predicate(assignment: Dict[VariableName, DomainValue]) -> bool:\n            if len(assignment) != len(variables):\n                return True  # Not all assigned yet\n            \n            # Extract values in order\n            values = [assignment[var] for var in variables]\n            \n            # Check if values match any valid pattern\n            return tuple(values) in self.valid_patterns\n        \n        super().__init__(variables, pattern_predicate,\n                        name or f\"SamePattern({len(variables)} vars)\")\n    \n    def _generate_pattern_variants(self, pattern: List[Any]) -> Set[Tuple[Any, ...]]:\n        \"\"\"Generate all valid transformations of the pattern.\"\"\"\n        variants = {tuple(pattern)}\n        \n        if self.allow_rotation:\n            # For 1D patterns, rotation means reversal\n            variants.add(tuple(reversed(pattern)))\n        \n        if self.allow_reflection:\n            # For 1D patterns, reflection is same as reversal\n            variants.add(tuple(reversed(pattern)))\n        \n        return variants\n\n\nclass TransformationConstraint(NAryConstraint):\n    \"\"\"\n    Constraint for grid transformations (rotation, reflection, scaling).\n    \n    Ensures that one grid region is a transformation of another.\n    \"\"\"\n    \n    def __init__(self, source_vars: List[VariableName],\n                 target_vars: List[VariableName],\n                 transformation_type: str,\n                 grid_shape: Tuple[int, int],\n                 name: Optional[str] = None):\n        \"\"\"\n        Initialize transformation constraint.\n        \n        Args:\n            source_vars: Variables in the source region\n            target_vars: Variables in the target region\n            transformation_type: Type of transformation ('rotate90', 'rotate180', \n                                'rotate270', 'fliph', 'flipv', 'transpose')\n            grid_shape: Shape of the grid (rows, cols)\n            name: Optional name for the constraint\n        \"\"\"\n        self.source_vars = source_vars\n        self.target_vars = target_vars\n        self.transformation_type = transformation_type\n        self.grid_shape = grid_shape\n        \n        # Create mapping from source to target positions\n        self.position_mapping = self._create_position_mapping()\n        \n        def transformation_predicate(assignment: Dict[VariableName, DomainValue]) -> bool:\n            # Check if source and target match according to transformation\n            for src_var, tgt_var in self.position_mapping.items():\n                if src_var in assignment and tgt_var in assignment:\n                    if assignment[src_var] != assignment[tgt_var]:\n                        return False\n            return True\n        \n        all_vars = list(set(source_vars + target_vars))\n        super().__init__(all_vars, transformation_predicate,\n                        name or f\"Transform_{transformation_type}\")\n    \n    def _create_position_mapping(self) -> Dict[VariableName, VariableName]:\n        \"\"\"Create mapping from source to target variables based on transformation.\"\"\"\n        mapping = {}\n        rows, cols = self.grid_shape\n        \n        # Parse variable positions from names (assuming format \"cell_row_col\")\n        def get_position(var_name):\n            parts = var_name.split('_')\n            if len(parts) >= 3:\n                return int(parts[-2]), int(parts[-1])\n            return None\n        \n        # Create position arrays\n        source_positions = {var: get_position(var) for var in self.source_vars}\n        target_positions = {get_position(var): var for var in self.target_vars}\n        \n        for src_var, src_pos in source_positions.items():\n            if src_pos is None:\n                continue\n            \n            row, col = src_pos\n            \n            # Apply transformation\n            if self.transformation_type == 'rotate90':\n                new_row, new_col = col, rows - 1 - row\n            elif self.transformation_type == 'rotate180':\n                new_row, new_col = rows - 1 - row, cols - 1 - col\n            elif self.transformation_type == 'rotate270':\n                new_row, new_col = cols - 1 - col, row\n            elif self.transformation_type == 'fliph':\n                new_row, new_col = row, cols - 1 - col\n            elif self.transformation_type == 'flipv':\n                new_row, new_col = rows - 1 - row, col\n            elif self.transformation_type == 'transpose':\n                new_row, new_col = col, row\n            else:\n                new_row, new_col = row, col\n            \n            # Find corresponding target variable\n            if (new_row, new_col) in target_positions:\n                mapping[src_var] = target_positions[(new_row, new_col)]\n        \n        return mapping\n    \n    def prune_domains(self, domains: Domain) -> bool:\n        \"\"\"Prune domains based on transformation mapping.\"\"\"\n        changed = False\n        \n        for src_var, tgt_var in self.position_mapping.items():\n            src_domain = domains.get(src_var)\n            tgt_domain = domains.get(tgt_var)\n            \n            # Intersect domains\n            common = src_domain.intersection(tgt_domain)\n            \n            if len(common) < len(src_domain):\n                domains.set(src_var, common)\n                changed = True\n            \n            if len(common) < len(tgt_domain):\n                domains.set(tgt_var, common)\n                changed = True\n        \n        return changed\n\n\nclass ColorMappingConstraint(NAryConstraint):\n    \"\"\"\n    Constraint for color mapping relationships.\n    \n    Ensures that colors are mapped consistently (e.g., all 1s become 2s).\n    \"\"\"\n    \n    def __init__(self, source_vars: List[VariableName],\n                 target_vars: List[VariableName],\n                 mapping: Optional[Dict[int, int]] = None,\n                 allow_permutation: bool = False,\n                 name: Optional[str] = None):\n        \"\"\"\n        Initialize color mapping constraint.\n        \n        Args:\n            source_vars: Variables in the source region\n            target_vars: Variables in the target region  \n            mapping: Fixed color mapping (None for any consistent mapping)\n            allow_permutation: Whether to allow any permutation of colors\n            name: Optional name for the constraint\n        \"\"\"\n        self.source_vars = source_vars\n        self.target_vars = target_vars\n        self.fixed_mapping = mapping\n        self.allow_permutation = allow_permutation\n        \n        def mapping_predicate(assignment: Dict[VariableName, DomainValue]) -> bool:\n            # Build mapping from assigned values\n            inferred_mapping = {}\n            \n            for src_var, tgt_var in zip(source_vars, target_vars):\n                if src_var in assignment and tgt_var in assignment:\n                    src_val = assignment[src_var]\n                    tgt_val = assignment[tgt_var]\n                    \n                    if self.fixed_mapping:\n                        # Check against fixed mapping\n                        if src_val in self.fixed_mapping:\n                            if self.fixed_mapping[src_val] != tgt_val:\n                                return False\n                    else:\n                        # Infer mapping\n                        if src_val in inferred_mapping:\n                            if inferred_mapping[src_val] != tgt_val:\n                                return False\n                        else:\n                            inferred_mapping[src_val] = tgt_val\n            \n            # Check if mapping is valid (injective if required)\n            if not self.allow_permutation:\n                # Mapping should be one-to-one\n                if len(set(inferred_mapping.values())) != len(inferred_mapping):\n                    return False\n            \n            return True\n        \n        all_vars = list(set(source_vars + target_vars))\n        super().__init__(all_vars, mapping_predicate,\n                        name or \"ColorMapping\")\n\n\nclass SymmetryConstraint(NAryConstraint):\n    \"\"\"\n    Constraint for enforcing symmetry in grids.\n    \n    Ensures that a grid region has specified symmetry properties.\n    \"\"\"\n    \n    def __init__(self, variables: List[VariableName],\n                 grid_shape: Tuple[int, int],\n                 symmetry_type: str,\n                 name: Optional[str] = None):\n        \"\"\"\n        Initialize symmetry constraint.\n        \n        Args:\n            variables: Variables in the grid region\n            grid_shape: Shape of the grid (rows, cols)\n            symmetry_type: Type of symmetry ('horizontal', 'vertical', \n                          'diagonal', 'anti-diagonal', 'rotational')\n            name: Optional name for the constraint\n        \"\"\"\n        self.grid_shape = grid_shape\n        self.symmetry_type = symmetry_type\n        \n        # Create symmetry pairs\n        self.symmetry_pairs = self._create_symmetry_pairs(variables)\n        \n        def symmetry_predicate(assignment: Dict[VariableName, DomainValue]) -> bool:\n            for var1, var2 in self.symmetry_pairs:\n                if var1 in assignment and var2 in assignment:\n                    if assignment[var1] != assignment[var2]:\n                        return False\n            return True\n        \n        super().__init__(variables, symmetry_predicate,\n                        name or f\"Symmetry_{symmetry_type}\")\n    \n    def _create_symmetry_pairs(self, variables: List[VariableName]) -> List[Tuple[VariableName, VariableName]]:\n        \"\"\"Create pairs of variables that must be equal for symmetry.\"\"\"\n        pairs = []\n        rows, cols = self.grid_shape\n        \n        # Parse positions\n        var_positions = {}\n        position_vars = {}\n        \n        for var in variables:\n            parts = var.split('_')\n            if len(parts) >= 3:\n                row, col = int(parts[-2]), int(parts[-1])\n                var_positions[var] = (row, col)\n                position_vars[(row, col)] = var\n        \n        # Create pairs based on symmetry type\n        for var, (row, col) in var_positions.items():\n            sym_row, sym_col = row, col\n            \n            if self.symmetry_type == 'horizontal':\n                sym_col = cols - 1 - col\n            elif self.symmetry_type == 'vertical':\n                sym_row = rows - 1 - row\n            elif self.symmetry_type == 'diagonal':\n                sym_row, sym_col = col, row\n            elif self.symmetry_type == 'anti-diagonal':\n                sym_row = cols - 1 - col\n                sym_col = rows - 1 - row\n            elif self.symmetry_type == 'rotational':\n                sym_row = rows - 1 - row\n                sym_col = cols - 1 - col\n            \n            if (sym_row, sym_col) in position_vars:\n                sym_var = position_vars[(sym_row, sym_col)]\n                if var != sym_var and (sym_var, var) not in pairs:\n                    pairs.append((var, sym_var))\n        \n        return pairs\n    \n    def prune_domains(self, domains: Domain) -> bool:\n        \"\"\"Prune domains based on symmetry pairs.\"\"\"\n        changed = False\n        \n        for var1, var2 in self.symmetry_pairs:\n            domain1 = domains.get(var1)\n            domain2 = domains.get(var2)\n            \n            # Intersect domains for symmetric positions\n            common = domain1.intersection(domain2)\n            \n            if len(common) < len(domain1):\n                domains.set(var1, common)\n                changed = True\n            \n            if len(common) < len(domain2):\n                domains.set(var2, common)\n                changed = True\n        \n        return changed\n\n\nclass AdjacentConstraint(BinaryConstraint):\n    \"\"\"\n    Constraint for adjacent cells in a grid.\n    \n    Ensures that adjacent cells satisfy a given relationship.\n    \"\"\"\n    \n    def __init__(self, var1: VariableName, var2: VariableName,\n                 relation_type: str = 'different',\n                 name: Optional[str] = None):\n        \"\"\"\n        Initialize adjacent constraint.\n        \n        Args:\n            var1: First variable\n            var2: Second variable (adjacent to first)\n            relation_type: Type of relation ('different', 'same', 'sum', 'product')\n            name: Optional name for the constraint\n        \"\"\"\n        if relation_type == 'different':\n            relation = lambda v1, v2: v1 != v2\n        elif relation_type == 'same':\n            relation = lambda v1, v2: v1 == v2\n        elif relation_type == 'sum':\n            # For sum constraint, values should sum to a specific target\n            # This would need to be parameterized\n            relation = lambda v1, v2: True  # Placeholder\n        elif relation_type == 'product':\n            relation = lambda v1, v2: True  # Placeholder\n        else:\n            relation = lambda v1, v2: True\n        \n        super().__init__(var1, var2, relation,\n                        name or f\"Adjacent_{relation_type}({var1},{var2})\")\n\n\nclass CountConstraint(NAryConstraint):\n    \"\"\"\n    Constraint on the count of specific values.\n    \n    Ensures that certain values appear a specific number of times.\n    \"\"\"\n    \n    def __init__(self, variables: List[VariableName],\n                 value_counts: Dict[DomainValue, int],\n                 exact: bool = True,\n                 name: Optional[str] = None):\n        \"\"\"\n        Initialize count constraint.\n        \n        Args:\n            variables: Variables to count over\n            value_counts: Required counts for each value\n            exact: Whether counts must be exact (vs minimum)\n            name: Optional name for the constraint\n        \"\"\"\n        self.value_counts = value_counts\n        self.exact = exact\n        \n        def count_predicate(assignment: Dict[VariableName, DomainValue]) -> bool:\n            # Count assigned values\n            counts = Counter(assignment.values())\n            \n            # Check against required counts\n            for value, required_count in value_counts.items():\n                actual_count = counts.get(value, 0)\n                \n                if exact:\n                    # For exact counts, check if we haven't exceeded\n                    # (final check happens when all vars assigned)\n                    if actual_count > required_count:\n                        return False\n                    \n                    # If all variables assigned, check exact match\n                    if len(assignment) == len(variables):\n                        if actual_count != required_count:\n                            return False\n                else:\n                    # For minimum counts, check when all assigned\n                    if len(assignment) == len(variables):\n                        if actual_count < required_count:\n                            return False\n            \n            return True\n        \n        super().__init__(variables, count_predicate,\n                        name or f\"Count({len(value_counts)} values)\")\n    \n    def prune_domains(self, domains: Domain) -> bool:\n        \"\"\"Prune based on counting constraints.\"\"\"\n        changed = False\n        \n        if not self.exact:\n            return super().prune_domains(domains)\n        \n        # Count current assignments\n        assigned_counts = defaultdict(int)\n        unassigned_vars = []\n        \n        for var in self.variables:\n            if domains.is_singleton(var):\n                value = domains.get_singleton_value(var)\n                assigned_counts[value] += 1\n            else:\n                unassigned_vars.append(var)\n        \n        # Check if any value has reached its limit\n        for value, max_count in self.value_counts.items():\n            if assigned_counts[value] >= max_count:\n                # Remove this value from all unassigned variables\n                for var in unassigned_vars:\n                    if domains.remove(var, value):\n                        changed = True\n        \n        # Check if any value must be assigned to remaining variables\n        for value, required_count in self.value_counts.items():\n            remaining_needed = required_count - assigned_counts[value]\n            \n            if remaining_needed > 0:\n                # Count how many unassigned variables can take this value\n                possible_vars = [v for v in unassigned_vars if value in domains.get(v)]\n                \n                if len(possible_vars) == remaining_needed:\n                    # These variables must all take this value\n                    for var in possible_vars:\n                        if len(domains.get(var)) > 1:\n                            domains.set(var, {value})\n                            changed = True\n                elif len(possible_vars) < remaining_needed:\n                    # Impossible to satisfy constraint\n                    for var in unassigned_vars:\n                        domains.set(var, set())\n                    return True\n        \n        return changed\n\n\nclass ConnectedComponentConstraint(NAryConstraint):\n    \"\"\"\n    Constraint for connected components in a grid.\n    \n    Ensures that cells of the same color form connected regions.\n    \"\"\"\n    \n    def __init__(self, variables: List[VariableName],\n                 grid_shape: Tuple[int, int],\n                 connectivity: str = '4',  # '4' or '8' connectivity\n                 name: Optional[str] = None):\n        \"\"\"\n        Initialize connected component constraint.\n        \n        Args:\n            variables: Variables in the grid\n            grid_shape: Shape of the grid (rows, cols)\n            connectivity: Type of connectivity ('4' or '8')\n            name: Optional name for the constraint\n        \"\"\"\n        self.grid_shape = grid_shape\n        self.connectivity = connectivity\n        \n        # Build adjacency structure\n        self.adjacency = self._build_adjacency(variables)\n        \n        def connected_predicate(assignment: Dict[VariableName, DomainValue]) -> bool:\n            if len(assignment) < len(variables):\n                return True  # Can't check connectivity until all assigned\n            \n            # Group variables by color\n            color_groups = defaultdict(list)\n            for var, color in assignment.items():\n                color_groups[color].append(var)\n            \n            # Check if each color group is connected\n            for color, vars_list in color_groups.items():\n                if not self._is_connected(vars_list):\n                    return False\n            \n            return True\n        \n        super().__init__(variables, connected_predicate,\n                        name or f\"ConnectedComponents_{connectivity}\")\n    \n    def _build_adjacency(self, variables: List[VariableName]) -> Dict[VariableName, List[VariableName]]:\n        \"\"\"Build adjacency list for the grid.\"\"\"\n        adjacency = defaultdict(list)\n        \n        # Parse positions\n        var_positions = {}\n        position_vars = {}\n        \n        for var in variables:\n            parts = var.split('_')\n            if len(parts) >= 3:\n                row, col = int(parts[-2]), int(parts[-1])\n                var_positions[var] = (row, col)\n                position_vars[(row, col)] = var\n        \n        # Build adjacency based on connectivity\n        for var, (row, col) in var_positions.items():\n            neighbors = []\n            \n            # 4-connectivity\n            for dr, dc in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n                new_row, new_col = row + dr, col + dc\n                if (new_row, new_col) in position_vars:\n                    neighbors.append(position_vars[(new_row, new_col)])\n            \n            # Additional neighbors for 8-connectivity\n            if self.connectivity == '8':\n                for dr, dc in [(-1, -1), (-1, 1), (1, -1), (1, 1)]:\n                    new_row, new_col = row + dr, col + dc\n                    if (new_row, new_col) in position_vars:\n                        neighbors.append(position_vars[(new_row, new_col)])\n            \n            adjacency[var] = neighbors\n        \n        return adjacency\n    \n    def _is_connected(self, variables: List[VariableName]) -> bool:\n        \"\"\"Check if a set of variables forms a connected component.\"\"\"\n        if not variables:\n            return True\n        \n        # BFS from first variable\n        visited = {variables[0]}\n        queue = [variables[0]]\n        var_set = set(variables)\n        \n        while queue:\n            current = queue.pop(0)\n            \n            for neighbor in self.adjacency[current]:\n                if neighbor in var_set and neighbor not in visited:\n                    visited.add(neighbor)\n                    queue.append(neighbor)\n        \n        return len(visited) == len(variables)\n\n\nclass RegionConstraint(NAryConstraint):\n    \"\"\"\n    Constraint for rectangular regions in a grid.\n    \n    Ensures that a rectangular region satisfies certain properties.\n    \"\"\"\n    \n    def __init__(self, variables: List[VariableName],\n                 region_property: str,\n                 target_value: Optional[Any] = None,\n                 name: Optional[str] = None):\n        \"\"\"\n        Initialize region constraint.\n        \n        Args:\n            variables: Variables in the region\n            region_property: Property to enforce ('uniform', 'sum', 'product', 'min', 'max')\n            target_value: Target value for the property\n            name: Optional name for the constraint\n        \"\"\"\n        self.region_property = region_property\n        self.target_value = target_value\n        \n        def region_predicate(assignment: Dict[VariableName, DomainValue]) -> bool:\n            if not assignment:\n                return True\n            \n            values = list(assignment.values())\n            \n            if region_property == 'uniform':\n                # All values in region must be the same\n                return len(set(values)) <= 1\n            \n            elif region_property == 'sum' and target_value is not None:\n                # Sum of values must equal target\n                if len(assignment) == len(variables):\n                    return sum(values) == target_value\n                else:\n                    # Partial sum shouldn't exceed target\n                    return sum(values) <= target_value\n            \n            elif region_property == 'product' and target_value is not None:\n                # Product of values must equal target\n                if len(assignment) == len(variables):\n                    product = 1\n                    for v in values:\n                        product *= v\n                    return product == target_value\n                return True\n            \n            elif region_property == 'min' and target_value is not None:\n                # Minimum value must equal target\n                return min(values) >= target_value\n            \n            elif region_property == 'max' and target_value is not None:\n                # Maximum value must equal target\n                return max(values) <= target_value\n            \n            return True\n        \n        super().__init__(variables, region_predicate,\n                        name or f\"Region_{region_property}\")\n\n\ndef create_sudoku_constraints(csp, grid_size: int = 9, box_size: int = 3) -> None:\n    \"\"\"\n    Add Sudoku constraints to a CSP.\n    \n    Args:\n        csp: The CSP to add constraints to\n        grid_size: Size of the Sudoku grid (usually 9)\n        box_size: Size of each box (usually 3)\n    \"\"\"\n    # Row constraints\n    for row in range(grid_size):\n        row_vars = [f\"cell_{row}_{col}\" for col in range(grid_size)]\n        csp.add_constraint(AllDifferentConstraint(row_vars, f\"Row_{row}\"))\n    \n    # Column constraints\n    for col in range(grid_size):\n        col_vars = [f\"cell_{row}_{col}\" for row in range(grid_size)]\n        csp.add_constraint(AllDifferentConstraint(col_vars, f\"Col_{col}\"))\n    \n    # Box constraints\n    for box_row in range(0, grid_size, box_size):\n        for box_col in range(0, grid_size, box_size):\n            box_vars = []\n            for r in range(box_row, box_row + box_size):\n                for c in range(box_col, box_col + box_size):\n                    box_vars.append(f\"cell_{r}_{c}\")\n            csp.add_constraint(AllDifferentConstraint(box_vars, \n                                                     f\"Box_{box_row}_{box_col}\"))","size_bytes":29795},"src/csp/core.py":{"content":"\"\"\"\nCore CSP (Constraint Satisfaction Problem) classes for ARC Prize 2025.\n\nThis module provides the fundamental building blocks for defining and solving\nconstraint satisfaction problems, optimized for grid-based puzzles with up to\n30x30 cells and 10 colors.\n\"\"\"\n\nfrom typing import Dict, List, Set, Tuple, Any, Optional, Callable, Generic, TypeVar\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nimport numpy as np\nfrom dataclasses import dataclass, field\nfrom enum import Enum\n\n# Type definitions\nT = TypeVar('T')\nVariableName = str\nDomainValue = Any\n\n\nclass ConstraintType(Enum):\n    \"\"\"Types of constraints supported by the CSP solver.\"\"\"\n    UNARY = 1\n    BINARY = 2\n    N_ARY = 3\n    GLOBAL = 4\n\n\n@dataclass\nclass Variable:\n    \"\"\"\n    Represents a variable in a CSP.\n    \n    Attributes:\n        name: Unique identifier for the variable\n        domain: Initial domain of possible values\n        position: Optional (row, col) position for grid-based problems\n        metadata: Additional metadata for the variable\n    \"\"\"\n    name: VariableName\n    domain: Set[DomainValue]\n    position: Optional[Tuple[int, int]] = None\n    metadata: Dict[str, Any] = field(default_factory=dict)\n    \n    def __hash__(self) -> int:\n        return hash(self.name)\n    \n    def __eq__(self, other) -> bool:\n        return isinstance(other, Variable) and self.name == other.name\n    \n    def __repr__(self) -> str:\n        return f\"Variable({self.name}, |domain|={len(self.domain)})\"\n    \n    def copy(self) -> 'Variable':\n        \"\"\"Create a deep copy of the variable.\"\"\"\n        return Variable(\n            name=self.name,\n            domain=self.domain.copy(),\n            position=self.position,\n            metadata=self.metadata.copy()\n        )\n\n\n@dataclass\nclass Domain:\n    \"\"\"\n    Manages domains for all variables in a CSP.\n    \n    Provides efficient domain manipulation and tracking for constraint propagation.\n    \"\"\"\n    _domains: Dict[VariableName, Set[DomainValue]] = field(default_factory=dict)\n    _initial_domains: Dict[VariableName, Set[DomainValue]] = field(default_factory=dict)\n    _domain_changes: List[Tuple[VariableName, Set[DomainValue]]] = field(default_factory=list)\n    \n    def __init__(self, variables: Optional[List[Variable]] = None):\n        \"\"\"Initialize domains from a list of variables.\"\"\"\n        self._domains = {}\n        self._initial_domains = {}\n        self._domain_changes = []\n        \n        if variables:\n            for var in variables:\n                self._domains[var.name] = var.domain.copy()\n                self._initial_domains[var.name] = var.domain.copy()\n    \n    def get(self, var_name: VariableName) -> Set[DomainValue]:\n        \"\"\"Get the current domain of a variable.\"\"\"\n        return self._domains.get(var_name, set())\n    \n    def set(self, var_name: VariableName, values: Set[DomainValue]) -> None:\n        \"\"\"Set the domain of a variable.\"\"\"\n        old_domain = self._domains.get(var_name, set()).copy()\n        self._domains[var_name] = values.copy()\n        self._domain_changes.append((var_name, old_domain))\n    \n    def remove(self, var_name: VariableName, value: DomainValue) -> bool:\n        \"\"\"\n        Remove a value from a variable's domain.\n        \n        Returns:\n            True if the value was removed, False if it wasn't in the domain\n        \"\"\"\n        if var_name in self._domains and value in self._domains[var_name]:\n            old_domain = self._domains[var_name].copy()\n            self._domains[var_name].remove(value)\n            self._domain_changes.append((var_name, old_domain))\n            return True\n        return False\n    \n    def is_empty(self, var_name: VariableName) -> bool:\n        \"\"\"Check if a variable's domain is empty.\"\"\"\n        return len(self._domains.get(var_name, set())) == 0\n    \n    def is_singleton(self, var_name: VariableName) -> bool:\n        \"\"\"Check if a variable's domain has exactly one value.\"\"\"\n        return len(self._domains.get(var_name, set())) == 1\n    \n    def get_singleton_value(self, var_name: VariableName) -> Optional[DomainValue]:\n        \"\"\"Get the single value if domain is a singleton, None otherwise.\"\"\"\n        domain = self._domains.get(var_name, set())\n        if len(domain) == 1:\n            return next(iter(domain))\n        return None\n    \n    def save_state(self) -> int:\n        \"\"\"Save the current state and return a checkpoint ID.\"\"\"\n        checkpoint = len(self._domain_changes)\n        return checkpoint\n    \n    def restore_state(self, checkpoint: int) -> None:\n        \"\"\"Restore domains to a previous checkpoint.\"\"\"\n        while len(self._domain_changes) > checkpoint:\n            var_name, old_domain = self._domain_changes.pop()\n            self._domains[var_name] = old_domain\n    \n    def copy(self) -> 'Domain':\n        \"\"\"Create a deep copy of the domain manager.\"\"\"\n        new_domain = Domain()\n        new_domain._domains = {k: v.copy() for k, v in self._domains.items()}\n        new_domain._initial_domains = {k: v.copy() for k, v in self._initial_domains.items()}\n        new_domain._domain_changes = self._domain_changes.copy()\n        return new_domain\n    \n    def reset(self) -> None:\n        \"\"\"Reset all domains to their initial values.\"\"\"\n        self._domains = {k: v.copy() for k, v in self._initial_domains.items()}\n        self._domain_changes.clear()\n\n\nclass Constraint(ABC):\n    \"\"\"\n    Abstract base class for all constraints.\n    \n    A constraint defines a relationship that must hold between variables.\n    \"\"\"\n    \n    def __init__(self, variables: List[VariableName], name: Optional[str] = None):\n        \"\"\"\n        Initialize a constraint.\n        \n        Args:\n            variables: List of variable names involved in this constraint\n            name: Optional name for the constraint\n        \"\"\"\n        self.variables = variables\n        self.name = name or f\"Constraint({','.join(variables)})\"\n    \n    @abstractmethod\n    def is_satisfied(self, assignment: Dict[VariableName, DomainValue]) -> bool:\n        \"\"\"\n        Check if the constraint is satisfied by the given assignment.\n        \n        Args:\n            assignment: Current variable assignments\n            \n        Returns:\n            True if the constraint is satisfied, False otherwise\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    def prune_domains(self, domains: Domain) -> bool:\n        \"\"\"\n        Prune inconsistent values from variable domains.\n        \n        Args:\n            domains: Current domains of all variables\n            \n        Returns:\n            True if any domain was changed, False otherwise\n        \"\"\"\n        pass\n    \n    def get_scope(self) -> List[VariableName]:\n        \"\"\"Get the variables involved in this constraint.\"\"\"\n        return self.variables\n    \n    def __repr__(self) -> str:\n        return self.name\n\n\nclass UnaryConstraint(Constraint):\n    \"\"\"\n    A constraint involving a single variable.\n    \n    Useful for imposing restrictions on individual cells in ARC puzzles.\n    \"\"\"\n    \n    def __init__(self, variable: VariableName, predicate: Callable[[DomainValue], bool],\n                 name: Optional[str] = None):\n        \"\"\"\n        Initialize a unary constraint.\n        \n        Args:\n            variable: The variable this constraint applies to\n            predicate: Function that returns True for valid values\n            name: Optional name for the constraint\n        \"\"\"\n        super().__init__([variable], name)\n        self.predicate = predicate\n    \n    def is_satisfied(self, assignment: Dict[VariableName, DomainValue]) -> bool:\n        \"\"\"Check if the constraint is satisfied.\"\"\"\n        if self.variables[0] not in assignment:\n            return True  # Unassigned variables don't violate constraints\n        return self.predicate(assignment[self.variables[0]])\n    \n    def prune_domains(self, domains: Domain) -> bool:\n        \"\"\"Remove values that don't satisfy the predicate.\"\"\"\n        changed = False\n        var_name = self.variables[0]\n        current_domain = domains.get(var_name).copy()\n        \n        for value in current_domain:\n            if not self.predicate(value):\n                domains.remove(var_name, value)\n                changed = True\n        \n        return changed\n\n\nclass BinaryConstraint(Constraint):\n    \"\"\"\n    A constraint involving exactly two variables.\n    \n    Most common constraint type in CSPs, efficient for arc consistency.\n    \"\"\"\n    \n    def __init__(self, var1: VariableName, var2: VariableName,\n                 relation: Callable[[DomainValue, DomainValue], bool],\n                 name: Optional[str] = None):\n        \"\"\"\n        Initialize a binary constraint.\n        \n        Args:\n            var1: First variable\n            var2: Second variable\n            relation: Function that returns True for valid value pairs\n            name: Optional name for the constraint\n        \"\"\"\n        super().__init__([var1, var2], name)\n        self.relation = relation\n    \n    def is_satisfied(self, assignment: Dict[VariableName, DomainValue]) -> bool:\n        \"\"\"Check if the constraint is satisfied.\"\"\"\n        var1, var2 = self.variables\n        \n        # If either variable is unassigned, the constraint is not violated\n        if var1 not in assignment or var2 not in assignment:\n            return True\n        \n        return self.relation(assignment[var1], assignment[var2])\n    \n    def prune_domains(self, domains: Domain) -> bool:\n        \"\"\"Prune domains using arc consistency.\"\"\"\n        changed = False\n        var1, var2 = self.variables\n        \n        # Prune var1's domain\n        domain1 = domains.get(var1).copy()\n        domain2 = domains.get(var2)\n        \n        for val1 in domain1:\n            has_support = False\n            for val2 in domain2:\n                if self.relation(val1, val2):\n                    has_support = True\n                    break\n            \n            if not has_support:\n                domains.remove(var1, val1)\n                changed = True\n        \n        # Prune var2's domain\n        domain1 = domains.get(var1)\n        domain2 = domains.get(var2).copy()\n        \n        for val2 in domain2:\n            has_support = False\n            for val1 in domain1:\n                if self.relation(val1, val2):\n                    has_support = True\n                    break\n            \n            if not has_support:\n                domains.remove(var2, val2)\n                changed = True\n        \n        return changed\n    \n    def has_support(self, value: DomainValue, var_idx: int, \n                    other_domain: Set[DomainValue]) -> bool:\n        \"\"\"\n        Check if a value has support in the other variable's domain.\n        \n        Args:\n            value: The value to check\n            var_idx: Index of the variable (0 or 1)\n            other_domain: Domain of the other variable\n            \n        Returns:\n            True if the value has at least one supporting value\n        \"\"\"\n        for other_value in other_domain:\n            if var_idx == 0:\n                if self.relation(value, other_value):\n                    return True\n            else:\n                if self.relation(other_value, value):\n                    return True\n        return False\n\n\nclass NAryConstraint(Constraint):\n    \"\"\"\n    A constraint involving multiple variables (more than 2).\n    \n    Used for complex relationships in ARC puzzles.\n    \"\"\"\n    \n    def __init__(self, variables: List[VariableName],\n                 predicate: Callable[[Dict[VariableName, DomainValue]], bool],\n                 name: Optional[str] = None):\n        \"\"\"\n        Initialize an n-ary constraint.\n        \n        Args:\n            variables: List of variables involved\n            predicate: Function that validates assignments\n            name: Optional name for the constraint\n        \"\"\"\n        super().__init__(variables, name)\n        self.predicate = predicate\n    \n    def is_satisfied(self, assignment: Dict[VariableName, DomainValue]) -> bool:\n        \"\"\"Check if the constraint is satisfied.\"\"\"\n        # Extract relevant assignments\n        relevant = {var: assignment[var] for var in self.variables if var in assignment}\n        \n        # If not all variables are assigned, we can't violate the constraint yet\n        if len(relevant) < len(self.variables):\n            return True\n        \n        return self.predicate(relevant)\n    \n    def prune_domains(self, domains: Domain) -> bool:\n        \"\"\"\n        Prune domains using generalized arc consistency.\n        \n        This is more expensive than binary constraint propagation.\n        \"\"\"\n        changed = False\n        \n        # For each variable in the constraint\n        for i, var in enumerate(self.variables):\n            current_domain = domains.get(var).copy()\n            other_vars = [v for j, v in enumerate(self.variables) if j != i]\n            \n            # Check each value in the current variable's domain\n            for value in current_domain:\n                has_support = self._has_support(var, value, other_vars, domains)\n                \n                if not has_support:\n                    domains.remove(var, value)\n                    changed = True\n        \n        return changed\n    \n    def _has_support(self, var: VariableName, value: DomainValue,\n                     other_vars: List[VariableName], domains: Domain) -> bool:\n        \"\"\"\n        Check if a value has support (recursive backtracking).\n        \n        This performs a mini-search to see if there exists an assignment\n        to other variables that satisfies the constraint.\n        \"\"\"\n        # Create partial assignment\n        assignment = {var: value}\n        return self._check_support_recursive(assignment, other_vars, 0, domains)\n    \n    def _check_support_recursive(self, assignment: Dict[VariableName, DomainValue],\n                                other_vars: List[VariableName], idx: int,\n                                domains: Domain) -> bool:\n        \"\"\"Recursive helper for checking support.\"\"\"\n        if idx == len(other_vars):\n            # All variables assigned, check constraint\n            return self.predicate(assignment)\n        \n        var = other_vars[idx]\n        for val in domains.get(var):\n            assignment[var] = val\n            if self._check_support_recursive(assignment, other_vars, idx + 1, domains):\n                del assignment[var]\n                return True\n            del assignment[var]\n        \n        return False\n\n\nclass CSP:\n    \"\"\"\n    Represents a Constraint Satisfaction Problem.\n    \n    Manages variables, domains, and constraints for ARC puzzle solving.\n    \"\"\"\n    \n    def __init__(self, name: str = \"CSP\"):\n        \"\"\"Initialize an empty CSP.\"\"\"\n        self.name = name\n        self.variables: Dict[VariableName, Variable] = {}\n        self.constraints: List[Constraint] = []\n        self.domains: Domain = Domain()\n        \n        # Constraint indexing for efficient lookup\n        self.constraints_by_var: Dict[VariableName, List[Constraint]] = defaultdict(list)\n        self.binary_constraints: Dict[Tuple[VariableName, VariableName], List[BinaryConstraint]] = defaultdict(list)\n        \n        # Statistics\n        self.stats = {\n            'nodes_explored': 0,\n            'backtracks': 0,\n            'domain_reductions': 0,\n            'constraint_checks': 0\n        }\n    \n    def add_variable(self, variable: Variable) -> None:\n        \"\"\"Add a variable to the CSP.\"\"\"\n        self.variables[variable.name] = variable\n        self.domains.set(variable.name, variable.domain)\n    \n    def add_variables(self, variables: List[Variable]) -> None:\n        \"\"\"Add multiple variables to the CSP.\"\"\"\n        for var in variables:\n            self.add_variable(var)\n    \n    def add_constraint(self, constraint: Constraint) -> None:\n        \"\"\"Add a constraint to the CSP.\"\"\"\n        self.constraints.append(constraint)\n        \n        # Update indices\n        for var in constraint.get_scope():\n            self.constraints_by_var[var].append(constraint)\n        \n        # Special indexing for binary constraints\n        if isinstance(constraint, BinaryConstraint) and len(constraint.variables) == 2:\n            var1, var2 = constraint.variables\n            self.binary_constraints[(var1, var2)].append(constraint)\n            self.binary_constraints[(var2, var1)].append(constraint)\n    \n    def add_constraints(self, constraints: List[Constraint]) -> None:\n        \"\"\"Add multiple constraints to the CSP.\"\"\"\n        for constraint in constraints:\n            self.add_constraint(constraint)\n    \n    def get_constraints_for_variable(self, var_name: VariableName) -> List[Constraint]:\n        \"\"\"Get all constraints involving a specific variable.\"\"\"\n        return self.constraints_by_var.get(var_name, [])\n    \n    def get_binary_constraints(self, var1: VariableName, var2: VariableName) -> List[BinaryConstraint]:\n        \"\"\"Get binary constraints between two specific variables.\"\"\"\n        return self.binary_constraints.get((var1, var2), [])\n    \n    def is_consistent(self, assignment: Dict[VariableName, DomainValue]) -> bool:\n        \"\"\"\n        Check if an assignment is consistent with all constraints.\n        \n        Args:\n            assignment: Current variable assignments\n            \n        Returns:\n            True if all constraints are satisfied\n        \"\"\"\n        for constraint in self.constraints:\n            self.stats['constraint_checks'] += 1\n            if not constraint.is_satisfied(assignment):\n                return False\n        return True\n    \n    def is_complete(self, assignment: Dict[VariableName, DomainValue]) -> bool:\n        \"\"\"Check if an assignment is complete (all variables assigned).\"\"\"\n        return len(assignment) == len(self.variables)\n    \n    def get_unassigned_variables(self, assignment: Dict[VariableName, DomainValue]) -> List[VariableName]:\n        \"\"\"Get list of unassigned variables.\"\"\"\n        return [var for var in self.variables if var not in assignment]\n    \n    def copy(self) -> 'CSP':\n        \"\"\"Create a deep copy of the CSP.\"\"\"\n        new_csp = CSP(self.name)\n        \n        # Copy variables\n        for var in self.variables.values():\n            new_csp.add_variable(var.copy())\n        \n        # Copy constraints (note: constraints themselves are not deep copied)\n        new_csp.constraints = self.constraints.copy()\n        new_csp.constraints_by_var = defaultdict(list)\n        for var, constraints in self.constraints_by_var.items():\n            new_csp.constraints_by_var[var] = constraints.copy()\n        \n        new_csp.binary_constraints = defaultdict(list)\n        for key, constraints in self.binary_constraints.items():\n            new_csp.binary_constraints[key] = constraints.copy()\n        \n        # Copy domains\n        new_csp.domains = self.domains.copy()\n        \n        # Copy stats\n        new_csp.stats = self.stats.copy()\n        \n        return new_csp\n    \n    def reset_stats(self) -> None:\n        \"\"\"Reset statistics counters.\"\"\"\n        self.stats = {\n            'nodes_explored': 0,\n            'backtracks': 0,\n            'domain_reductions': 0,\n            'constraint_checks': 0\n        }\n    \n    def __repr__(self) -> str:\n        return (f\"CSP(name={self.name}, \"\n                f\"variables={len(self.variables)}, \"\n                f\"constraints={len(self.constraints)})\")\n\n\ndef create_grid_csp(rows: int, cols: int, colors: Set[int],\n                    name: str = \"GridCSP\") -> CSP:\n    \"\"\"\n    Create a CSP for a grid-based problem.\n    \n    Args:\n        rows: Number of rows in the grid\n        cols: Number of columns in the grid\n        colors: Set of possible colors (usually 0-9 for ARC)\n        name: Name for the CSP\n        \n    Returns:\n        A CSP with variables for each grid cell\n    \"\"\"\n    csp = CSP(name)\n    \n    # Create a variable for each grid cell\n    for row in range(rows):\n        for col in range(cols):\n            var_name = f\"cell_{row}_{col}\"\n            var = Variable(\n                name=var_name,\n                domain=colors.copy(),\n                position=(row, col)\n            )\n            csp.add_variable(var)\n    \n    return csp\n\n\ndef create_pattern_csp(pattern: np.ndarray, name: str = \"PatternCSP\") -> CSP:\n    \"\"\"\n    Create a CSP from a pattern with some unknown values.\n    \n    Args:\n        pattern: NumPy array where -1 indicates unknown values\n        name: Name for the CSP\n        \n    Returns:\n        A CSP representing the pattern completion problem\n    \"\"\"\n    rows, cols = pattern.shape\n    \n    # Determine the set of colors from known values\n    known_values = pattern[pattern >= 0]\n    if len(known_values) > 0:\n        colors = set(np.unique(known_values))\n        # Add a few more colors in case new ones are needed\n        max_color = max(colors) if colors else 0\n        for i in range(max_color + 1, min(max_color + 3, 10)):\n            colors.add(i)\n    else:\n        colors = set(range(10))  # Default ARC colors\n    \n    csp = create_grid_csp(rows, cols, colors, name)\n    \n    # Add unary constraints for known values\n    for row in range(rows):\n        for col in range(cols):\n            if pattern[row, col] >= 0:\n                var_name = f\"cell_{row}_{col}\"\n                value = int(pattern[row, col])\n                constraint = UnaryConstraint(\n                    var_name,\n                    lambda v, val=value: v == val,\n                    name=f\"Fixed_{row}_{col}={value}\"\n                )\n                csp.add_constraint(constraint)\n    \n    return csp","size_bytes":21608},"src/csp/search.py":{"content":"\"\"\"\nSearch algorithms for CSP solving in ARC Prize 2025.\n\nThis module implements various search strategies for finding solutions\nto constraint satisfaction problems.\n\"\"\"\n\nfrom typing import Dict, List, Optional, Tuple, Set, Callable\nfrom collections import defaultdict\nimport random\nimport time\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nfrom .core import (\n    CSP, Variable, Domain, Constraint,\n    VariableName, DomainValue\n)\nfrom .arc_consistency import MAC, AC3, achieve_arc_consistency\n\n\nclass VariableOrdering(Enum):\n    \"\"\"Variable ordering heuristics for search.\"\"\"\n    DEFAULT = \"default\"  # Lexicographic ordering\n    MRV = \"mrv\"  # Minimum Remaining Values (Most Constrained Variable)\n    DEGREE = \"degree\"  # Degree heuristic\n    MRV_DEGREE = \"mrv_degree\"  # MRV with degree as tiebreaker\n    RANDOM = \"random\"  # Random ordering\n\n\nclass ValueOrdering(Enum):\n    \"\"\"Value ordering heuristics for search.\"\"\"\n    DEFAULT = \"default\"  # Natural ordering\n    LCV = \"lcv\"  # Least Constraining Value\n    RANDOM = \"random\"  # Random ordering\n    MIN = \"min\"  # Minimum value first\n    MAX = \"max\"  # Maximum value first\n\n\n@dataclass\nclass SearchStats:\n    \"\"\"Statistics for search algorithms.\"\"\"\n    nodes_explored: int = 0\n    backtracks: int = 0\n    solutions_found: int = 0\n    time_elapsed: float = 0.0\n    constraint_checks: int = 0\n    domain_reductions: int = 0\n    \n    def reset(self):\n        \"\"\"Reset all statistics.\"\"\"\n        self.nodes_explored = 0\n        self.backtracks = 0\n        self.solutions_found = 0\n        self.time_elapsed = 0.0\n        self.constraint_checks = 0\n        self.domain_reductions = 0\n\n\nclass BacktrackingSearch:\n    \"\"\"\n    Standard backtracking search algorithm for CSPs.\n    \n    Systematically explores the search space using depth-first search\n    with constraint checking.\n    \"\"\"\n    \n    def __init__(self, csp: CSP,\n                 variable_ordering: VariableOrdering = VariableOrdering.MRV_DEGREE,\n                 value_ordering: ValueOrdering = ValueOrdering.LCV):\n        \"\"\"\n        Initialize backtracking search.\n        \n        Args:\n            csp: The CSP to solve\n            variable_ordering: Heuristic for selecting next variable\n            value_ordering: Heuristic for ordering domain values\n        \"\"\"\n        self.csp = csp\n        self.variable_ordering = variable_ordering\n        self.value_ordering = value_ordering\n        self.stats = SearchStats()\n    \n    def solve(self, assignment: Optional[Dict[VariableName, DomainValue]] = None,\n              domains: Optional[Domain] = None) -> Optional[Dict[VariableName, DomainValue]]:\n        \"\"\"\n        Find a solution to the CSP using backtracking.\n        \n        Args:\n            assignment: Initial partial assignment\n            domains: Initial domains (uses CSP's domains if not provided)\n            \n        Returns:\n            Complete assignment if solution found, None otherwise\n        \"\"\"\n        start_time = time.time()\n        self.stats.reset()\n        \n        if assignment is None:\n            assignment = {}\n        \n        if domains is None:\n            domains = self.csp.domains.copy()\n        \n        result = self._backtrack(assignment, domains)\n        \n        self.stats.time_elapsed = time.time() - start_time\n        return result\n    \n    def _backtrack(self, assignment: Dict[VariableName, DomainValue],\n                   domains: Domain) -> Optional[Dict[VariableName, DomainValue]]:\n        \"\"\"\n        Recursive backtracking algorithm.\n        \n        Args:\n            assignment: Current partial assignment\n            domains: Current domains\n            \n        Returns:\n            Complete assignment if found, None otherwise\n        \"\"\"\n        self.stats.nodes_explored += 1\n        \n        # Check if assignment is complete\n        if self.csp.is_complete(assignment):\n            if self.csp.is_consistent(assignment):\n                self.stats.solutions_found += 1\n                return assignment.copy()\n            return None\n        \n        # Select next variable to assign\n        var = self._select_unassigned_variable(assignment, domains)\n        if var is None:\n            return None\n        \n        # Try each value in the domain\n        for value in self._order_domain_values(var, assignment, domains):\n            # Check if value is consistent with current assignment\n            assignment[var] = value\n            self.stats.constraint_checks += 1\n            \n            if self._is_consistent_assignment(var, value, assignment):\n                # Save current state\n                checkpoint = domains.save_state()\n                \n                # Apply inference (constraint propagation)\n                inference_result = self._inference(var, value, assignment, domains)\n                \n                if inference_result:\n                    # Recurse with updated assignment\n                    result = self._backtrack(assignment, domains)\n                    \n                    if result is not None:\n                        return result\n                \n                # Restore domains\n                domains.restore_state(checkpoint)\n            \n            # Remove assignment\n            del assignment[var]\n            self.stats.backtracks += 1\n        \n        return None\n    \n    def _select_unassigned_variable(self, assignment: Dict[VariableName, DomainValue],\n                                   domains: Domain) -> Optional[VariableName]:\n        \"\"\"\n        Select the next variable to assign using the specified heuristic.\n        \n        Args:\n            assignment: Current assignment\n            domains: Current domains\n            \n        Returns:\n            Name of the selected variable, or None if all assigned\n        \"\"\"\n        unassigned = self.csp.get_unassigned_variables(assignment)\n        \n        if not unassigned:\n            return None\n        \n        if self.variable_ordering == VariableOrdering.MRV:\n            # Minimum Remaining Values\n            return min(unassigned, key=lambda v: len(domains.get(v)))\n        \n        elif self.variable_ordering == VariableOrdering.DEGREE:\n            # Degree heuristic - most constrained variable\n            return max(unassigned, key=lambda v: self._get_degree(v, assignment))\n        \n        elif self.variable_ordering == VariableOrdering.MRV_DEGREE:\n            # MRV with degree as tiebreaker\n            min_domain_size = min(len(domains.get(v)) for v in unassigned)\n            candidates = [v for v in unassigned if len(domains.get(v)) == min_domain_size]\n            \n            if len(candidates) == 1:\n                return candidates[0]\n            \n            # Use degree as tiebreaker\n            return max(candidates, key=lambda v: self._get_degree(v, assignment))\n        \n        elif self.variable_ordering == VariableOrdering.RANDOM:\n            return random.choice(unassigned)\n        \n        else:  # DEFAULT - lexicographic\n            return unassigned[0]\n    \n    def _get_degree(self, var: VariableName,\n                   assignment: Dict[VariableName, DomainValue]) -> int:\n        \"\"\"\n        Get the degree of a variable (number of constraints with unassigned variables).\n        \n        Args:\n            var: Variable to check\n            assignment: Current assignment\n            \n        Returns:\n            Number of constraints with unassigned variables\n        \"\"\"\n        degree = 0\n        for constraint in self.csp.get_constraints_for_variable(var):\n            for other_var in constraint.get_scope():\n                if other_var != var and other_var not in assignment:\n                    degree += 1\n                    break\n        return degree\n    \n    def _order_domain_values(self, var: VariableName,\n                           assignment: Dict[VariableName, DomainValue],\n                           domains: Domain) -> List[DomainValue]:\n        \"\"\"\n        Order domain values using the specified heuristic.\n        \n        Args:\n            var: Variable whose values to order\n            assignment: Current assignment\n            domains: Current domains\n            \n        Returns:\n            Ordered list of domain values\n        \"\"\"\n        values = list(domains.get(var))\n        \n        if self.value_ordering == ValueOrdering.LCV:\n            # Least Constraining Value - choose value that rules out fewest choices\n            def count_conflicts(value):\n                conflicts = 0\n                for constraint in self.csp.get_constraints_for_variable(var):\n                    for other_var in constraint.get_scope():\n                        if other_var != var and other_var not in assignment:\n                            for other_val in domains.get(other_var):\n                                test_assignment = assignment.copy()\n                                test_assignment[var] = value\n                                test_assignment[other_var] = other_val\n                                if not constraint.is_satisfied(test_assignment):\n                                    conflicts += 1\n                return conflicts\n            \n            values.sort(key=count_conflicts)\n        \n        elif self.value_ordering == ValueOrdering.RANDOM:\n            random.shuffle(values)\n        \n        elif self.value_ordering == ValueOrdering.MIN:\n            values.sort()\n        \n        elif self.value_ordering == ValueOrdering.MAX:\n            values.sort(reverse=True)\n        \n        # DEFAULT - natural ordering\n        return values\n    \n    def _is_consistent_assignment(self, var: VariableName, value: DomainValue,\n                                 assignment: Dict[VariableName, DomainValue]) -> bool:\n        \"\"\"\n        Check if assigning var=value is consistent with current assignment.\n        \n        Args:\n            var: Variable being assigned\n            value: Value being assigned\n            assignment: Current assignment (including var=value)\n            \n        Returns:\n            True if consistent, False otherwise\n        \"\"\"\n        for constraint in self.csp.get_constraints_for_variable(var):\n            # Check only if all variables in constraint are assigned\n            all_assigned = all(v in assignment for v in constraint.get_scope())\n            \n            if all_assigned:\n                if not constraint.is_satisfied(assignment):\n                    return False\n        \n        return True\n    \n    def _inference(self, var: VariableName, value: DomainValue,\n                  assignment: Dict[VariableName, DomainValue],\n                  domains: Domain) -> bool:\n        \"\"\"\n        Basic inference - no additional constraint propagation.\n        \n        Override in subclasses for more sophisticated inference.\n        \n        Args:\n            var: Variable that was just assigned\n            value: Value that was assigned\n            assignment: Current assignment\n            domains: Current domains\n            \n        Returns:\n            True if inference succeeded, False if inconsistency detected\n        \"\"\"\n        # Reduce domain of assigned variable to single value\n        domains.set(var, {value})\n        return True\n    \n    def get_stats(self) -> SearchStats:\n        \"\"\"Get search statistics.\"\"\"\n        return self.stats\n\n\nclass ForwardCheckingSearch(BacktrackingSearch):\n    \"\"\"\n    Backtracking search with forward checking.\n    \n    Maintains arc consistency for the assigned variable after each assignment.\n    \"\"\"\n    \n    def _inference(self, var: VariableName, value: DomainValue,\n                  assignment: Dict[VariableName, DomainValue],\n                  domains: Domain) -> bool:\n        \"\"\"\n        Forward checking - remove inconsistent values from unassigned variables.\n        \n        Args:\n            var: Variable that was just assigned\n            value: Value that was assigned\n            assignment: Current assignment\n            domains: Current domains\n            \n        Returns:\n            True if inference succeeded, False if domain wipeout detected\n        \"\"\"\n        # First reduce domain of assigned variable\n        domains.set(var, {value})\n        \n        # Forward check: remove inconsistent values from neighbors\n        for constraint in self.csp.get_constraints_for_variable(var):\n            for other_var in constraint.get_scope():\n                if other_var != var and other_var not in assignment:\n                    # Remove values from other_var that are inconsistent with var=value\n                    other_domain = domains.get(other_var).copy()\n                    \n                    for other_val in other_domain:\n                        test_assignment = {var: value, other_var: other_val}\n                        \n                        # Add any other assigned variables in the constraint\n                        for v in constraint.get_scope():\n                            if v in assignment:\n                                test_assignment[v] = assignment[v]\n                        \n                        if not constraint.is_satisfied(test_assignment):\n                            domains.remove(other_var, other_val)\n                            self.stats.domain_reductions += 1\n                    \n                    # Check for domain wipeout\n                    if domains.is_empty(other_var):\n                        return False\n        \n        return True\n\n\nclass MACSearch(BacktrackingSearch):\n    \"\"\"\n    Backtracking search with Maintaining Arc Consistency (MAC).\n    \n    Maintains full arc consistency after each assignment.\n    \"\"\"\n    \n    def __init__(self, csp: CSP,\n                 variable_ordering: VariableOrdering = VariableOrdering.MRV_DEGREE,\n                 value_ordering: ValueOrdering = ValueOrdering.LCV,\n                 ac_algorithm: str = 'ac3'):\n        \"\"\"\n        Initialize MAC search.\n        \n        Args:\n            csp: The CSP to solve\n            variable_ordering: Heuristic for selecting next variable\n            value_ordering: Heuristic for ordering domain values\n            ac_algorithm: Arc consistency algorithm to use ('ac3' or 'ac4')\n        \"\"\"\n        super().__init__(csp, variable_ordering, value_ordering)\n        self.mac = MAC(csp, ac_algorithm)\n    \n    def solve(self, assignment: Optional[Dict[VariableName, DomainValue]] = None,\n              domains: Optional[Domain] = None) -> Optional[Dict[VariableName, DomainValue]]:\n        \"\"\"\n        Find a solution using MAC.\n        \n        Args:\n            assignment: Initial partial assignment\n            domains: Initial domains\n            \n        Returns:\n            Complete assignment if solution found, None otherwise\n        \"\"\"\n        if domains is None:\n            domains = self.csp.domains.copy()\n        \n        # Initial arc consistency\n        if not achieve_arc_consistency(self.csp, domains=domains):\n            return None  # Problem is inconsistent\n        \n        return super().solve(assignment, domains)\n    \n    def _inference(self, var: VariableName, value: DomainValue,\n                  assignment: Dict[VariableName, DomainValue],\n                  domains: Domain) -> bool:\n        \"\"\"\n        MAC inference - maintain arc consistency after assignment.\n        \n        Args:\n            var: Variable that was just assigned\n            value: Value that was assigned\n            assignment: Current assignment\n            domains: Current domains\n            \n        Returns:\n            True if arc consistency maintained, False if inconsistency detected\n        \"\"\"\n        return self.mac.maintain_arc_consistency(var, value, assignment, domains)\n\n\nclass MinConflicts:\n    \"\"\"\n    Min-conflicts local search algorithm.\n    \n    Starts with a complete (possibly inconsistent) assignment and\n    iteratively improves it by minimizing conflicts.\n    \"\"\"\n    \n    def __init__(self, csp: CSP, max_steps: int = 10000):\n        \"\"\"\n        Initialize min-conflicts search.\n        \n        Args:\n            csp: The CSP to solve\n            max_steps: Maximum number of improvement steps\n        \"\"\"\n        self.csp = csp\n        self.max_steps = max_steps\n        self.stats = SearchStats()\n    \n    def solve(self, initial_assignment: Optional[Dict[VariableName, DomainValue]] = None) -> Optional[Dict[VariableName, DomainValue]]:\n        \"\"\"\n        Find a solution using min-conflicts local search.\n        \n        Args:\n            initial_assignment: Initial complete assignment (random if not provided)\n            \n        Returns:\n            Solution if found within max_steps, None otherwise\n        \"\"\"\n        start_time = time.time()\n        self.stats.reset()\n        \n        # Generate initial complete assignment\n        if initial_assignment is None:\n            assignment = self._random_assignment()\n        else:\n            assignment = initial_assignment.copy()\n            # Complete any missing variables\n            for var in self.csp.variables:\n                if var not in assignment:\n                    domain = list(self.csp.domains.get(var))\n                    if domain:\n                        assignment[var] = random.choice(domain)\n        \n        # Main loop\n        for step in range(self.max_steps):\n            self.stats.nodes_explored += 1\n            \n            # Check if current assignment is a solution\n            conflicts = self._get_conflicted_variables(assignment)\n            \n            if not conflicts:\n                self.stats.solutions_found += 1\n                self.stats.time_elapsed = time.time() - start_time\n                return assignment\n            \n            # Select a random conflicted variable\n            var = random.choice(conflicts)\n            \n            # Find value that minimizes conflicts\n            min_conflicts_value = self._min_conflicts_value(var, assignment)\n            \n            if min_conflicts_value is not None:\n                assignment[var] = min_conflicts_value\n        \n        self.stats.time_elapsed = time.time() - start_time\n        return None  # No solution found within max_steps\n    \n    def _random_assignment(self) -> Dict[VariableName, DomainValue]:\n        \"\"\"Generate a random complete assignment.\"\"\"\n        assignment = {}\n        \n        for var in self.csp.variables:\n            domain = list(self.csp.domains.get(var))\n            if domain:\n                assignment[var] = random.choice(domain)\n        \n        return assignment\n    \n    def _get_conflicted_variables(self, assignment: Dict[VariableName, DomainValue]) -> List[VariableName]:\n        \"\"\"\n        Get list of variables involved in constraint violations.\n        \n        Args:\n            assignment: Current assignment\n            \n        Returns:\n            List of conflicted variable names\n        \"\"\"\n        conflicted = set()\n        \n        for constraint in self.csp.constraints:\n            self.stats.constraint_checks += 1\n            \n            # Check if all variables in constraint are assigned\n            if all(var in assignment for var in constraint.get_scope()):\n                if not constraint.is_satisfied(assignment):\n                    # Add all variables in this constraint to conflicted set\n                    for var in constraint.get_scope():\n                        conflicted.add(var)\n        \n        return list(conflicted)\n    \n    def _min_conflicts_value(self, var: VariableName,\n                            assignment: Dict[VariableName, DomainValue]) -> Optional[DomainValue]:\n        \"\"\"\n        Find the value that minimizes conflicts for the given variable.\n        \n        Args:\n            var: Variable to find value for\n            assignment: Current assignment\n            \n        Returns:\n            Value that minimizes conflicts, or None if no improvement possible\n        \"\"\"\n        min_conflicts = float('inf')\n        best_values = []\n        \n        for value in self.csp.domains.get(var):\n            # Count conflicts if we assign var=value\n            test_assignment = assignment.copy()\n            test_assignment[var] = value\n            \n            conflicts = 0\n            for constraint in self.csp.get_constraints_for_variable(var):\n                self.stats.constraint_checks += 1\n                \n                # Check if all variables in constraint are assigned\n                if all(v in test_assignment for v in constraint.get_scope()):\n                    if not constraint.is_satisfied(test_assignment):\n                        conflicts += 1\n            \n            if conflicts < min_conflicts:\n                min_conflicts = conflicts\n                best_values = [value]\n            elif conflicts == min_conflicts:\n                best_values.append(value)\n        \n        if best_values:\n            # Return random choice among best values\n            return random.choice(best_values)\n        \n        return None\n    \n    def get_stats(self) -> SearchStats:\n        \"\"\"Get search statistics.\"\"\"\n        return self.stats\n\n\nclass HybridSearch:\n    \"\"\"\n    Hybrid search combining systematic and local search.\n    \n    Uses backtracking with constraint propagation, but switches to\n    local search when stuck.\n    \"\"\"\n    \n    def __init__(self, csp: CSP,\n                 systematic_timeout: float = 10.0,\n                 local_search_steps: int = 1000):\n        \"\"\"\n        Initialize hybrid search.\n        \n        Args:\n            csp: The CSP to solve\n            systematic_timeout: Time limit for systematic search (seconds)\n            local_search_steps: Max steps for local search phase\n        \"\"\"\n        self.csp = csp\n        self.systematic_timeout = systematic_timeout\n        self.local_search_steps = local_search_steps\n        self.stats = SearchStats()\n    \n    def solve(self) -> Optional[Dict[VariableName, DomainValue]]:\n        \"\"\"\n        Find a solution using hybrid approach.\n        \n        Returns:\n            Solution if found, None otherwise\n        \"\"\"\n        start_time = time.time()\n        self.stats.reset()\n        \n        # Phase 1: Try systematic search with MAC\n        mac_search = MACSearch(self.csp)\n        \n        # Set up timeout mechanism\n        def timeout_checker():\n            return time.time() - start_time < self.systematic_timeout\n        \n        # Modified backtrack with timeout\n        assignment = self._backtrack_with_timeout(mac_search, {}, \n                                                 self.csp.domains.copy(),\n                                                 timeout_checker)\n        \n        if assignment is not None:\n            self.stats.solutions_found += 1\n            self.stats.time_elapsed = time.time() - start_time\n            return assignment\n        \n        # Phase 2: Switch to local search\n        # Use partial assignment from systematic search as starting point\n        partial = self._get_best_partial_assignment()\n        \n        min_conflicts = MinConflicts(self.csp, self.local_search_steps)\n        solution = min_conflicts.solve(partial)\n        \n        if solution is not None:\n            self.stats.solutions_found += 1\n        \n        self.stats.time_elapsed = time.time() - start_time\n        \n        # Combine stats\n        self.stats.nodes_explored += min_conflicts.stats.nodes_explored\n        self.stats.constraint_checks += min_conflicts.stats.constraint_checks\n        \n        return solution\n    \n    def _backtrack_with_timeout(self, search: MACSearch,\n                               assignment: Dict[VariableName, DomainValue],\n                               domains: Domain,\n                               timeout_checker: Callable[[], bool]) -> Optional[Dict[VariableName, DomainValue]]:\n        \"\"\"\n        Backtracking with timeout check.\n        \n        Args:\n            search: Search algorithm instance\n            assignment: Current assignment\n            domains: Current domains\n            timeout_checker: Function that returns False when timeout reached\n            \n        Returns:\n            Solution if found before timeout, None otherwise\n        \"\"\"\n        if not timeout_checker():\n            return None\n        \n        self.stats.nodes_explored += 1\n        \n        if self.csp.is_complete(assignment):\n            if self.csp.is_consistent(assignment):\n                return assignment.copy()\n            return None\n        \n        var = search._select_unassigned_variable(assignment, domains)\n        if var is None:\n            return None\n        \n        for value in search._order_domain_values(var, assignment, domains):\n            if not timeout_checker():\n                return None\n            \n            assignment[var] = value\n            \n            if search._is_consistent_assignment(var, value, assignment):\n                checkpoint = domains.save_state()\n                \n                if search._inference(var, value, assignment, domains):\n                    result = self._backtrack_with_timeout(search, assignment, \n                                                         domains, timeout_checker)\n                    \n                    if result is not None:\n                        return result\n                \n                domains.restore_state(checkpoint)\n            \n            del assignment[var]\n            self.stats.backtracks += 1\n        \n        return None\n    \n    def _get_best_partial_assignment(self) -> Dict[VariableName, DomainValue]:\n        \"\"\"\n        Get the best partial assignment found so far.\n        \n        This is a heuristic - we'll use domain reduction information.\n        \n        Returns:\n            Partial assignment based on singleton domains\n        \"\"\"\n        assignment = {}\n        \n        for var in self.csp.variables:\n            domain = self.csp.domains.get(var)\n            if len(domain) == 1:\n                # Variable has been determined\n                assignment[var] = next(iter(domain))\n            elif len(domain) > 0:\n                # Use most constrained value heuristic\n                # Choose value that appears in most constraints\n                value_counts = defaultdict(int)\n                \n                for constraint in self.csp.get_constraints_for_variable(var):\n                    for val in domain:\n                        test_assignment = {var: val}\n                        # Count how many other values this is compatible with\n                        for other_var in constraint.get_scope():\n                            if other_var != var:\n                                for other_val in self.csp.domains.get(other_var):\n                                    test_assignment[other_var] = other_val\n                                    if constraint.is_satisfied(test_assignment):\n                                        value_counts[val] += 1\n                                    del test_assignment[other_var]\n                \n                if value_counts:\n                    # Choose value with most compatibility\n                    best_val = max(value_counts, key=value_counts.get)\n                    assignment[var] = best_val\n        \n        return assignment\n    \n    def get_stats(self) -> SearchStats:\n        \"\"\"Get search statistics.\"\"\"\n        return self.stats\n\n\ndef solve_csp(csp: CSP, \n              algorithm: str = 'mac',\n              **kwargs) -> Optional[Dict[VariableName, DomainValue]]:\n    \"\"\"\n    Solve a CSP using the specified algorithm.\n    \n    Args:\n        csp: The CSP to solve\n        algorithm: Algorithm to use ('backtrack', 'forward', 'mac', 'min_conflicts', 'hybrid')\n        **kwargs: Additional arguments for the specific algorithm\n        \n    Returns:\n        Solution if found, None otherwise\n    \"\"\"\n    if algorithm == 'backtrack':\n        search = BacktrackingSearch(csp, **kwargs)\n        return search.solve()\n    \n    elif algorithm == 'forward':\n        search = ForwardCheckingSearch(csp, **kwargs)\n        return search.solve()\n    \n    elif algorithm == 'mac':\n        search = MACSearch(csp, **kwargs)\n        return search.solve()\n    \n    elif algorithm == 'min_conflicts':\n        search = MinConflicts(csp, **kwargs)\n        return search.solve()\n    \n    elif algorithm == 'hybrid':\n        search = HybridSearch(csp, **kwargs)\n        return search.solve()\n    \n    else:\n        raise ValueError(f\"Unknown algorithm: {algorithm}\")\n\n\ndef find_all_solutions(csp: CSP,\n                      algorithm: str = 'backtrack',\n                      max_solutions: int = 100) -> List[Dict[VariableName, DomainValue]]:\n    \"\"\"\n    Find all solutions to a CSP (up to max_solutions).\n    \n    Args:\n        csp: The CSP to solve\n        algorithm: Algorithm to use\n        max_solutions: Maximum number of solutions to find\n        \n    Returns:\n        List of all solutions found\n    \"\"\"\n    solutions = []\n    \n    if algorithm == 'min_conflicts':\n        # Min-conflicts can't systematically find all solutions\n        # Just run it multiple times with different random starts\n        search = MinConflicts(csp)\n        \n        for _ in range(max_solutions):\n            solution = search.solve()\n            if solution and solution not in solutions:\n                solutions.append(solution)\n            \n            if len(solutions) >= max_solutions:\n                break\n    else:\n        # Use systematic search with solution collection\n        if algorithm == 'forward':\n            search = ForwardCheckingSearch(csp)\n        elif algorithm == 'mac':\n            search = MACSearch(csp)\n        else:\n            search = BacktrackingSearch(csp)\n        \n        # Modified backtrack that collects all solutions\n        def collect_solutions(assignment, domains):\n            nonlocal solutions\n            \n            if len(solutions) >= max_solutions:\n                return\n            \n            if csp.is_complete(assignment):\n                if csp.is_consistent(assignment):\n                    solutions.append(assignment.copy())\n                return\n            \n            var = search._select_unassigned_variable(assignment, domains)\n            if var is None:\n                return\n            \n            for value in search._order_domain_values(var, assignment, domains):\n                assignment[var] = value\n                \n                if search._is_consistent_assignment(var, value, assignment):\n                    checkpoint = domains.save_state()\n                    \n                    if search._inference(var, value, assignment, domains):\n                        collect_solutions(assignment, domains)\n                    \n                    domains.restore_state(checkpoint)\n                \n                del assignment[var]\n        \n        collect_solutions({}, csp.domains.copy())\n    \n    return solutions","size_bytes":30846},"src/solvers/__init__.py":{"content":"\"\"\"Main solver implementations for ARC puzzles\"\"\"","size_bytes":49},"src/solvers/csp_solver.py":{"content":"\"\"\"\nCSP-based Solver for ARC Prize 2025\nUses Constraint Satisfaction Problems to solve ARC tasks\n\"\"\"\n\nimport numpy as np\nfrom typing import List, Dict, Optional, Tuple, Set, Any, Callable\nfrom dataclasses import dataclass, field\nfrom collections import deque\nimport copy\n\nfrom ..arc.grid_operations import Grid\nfrom ..arc.pattern_detector import PatternDetector\nfrom ..csp.core import CSP, Variable, Domain, Constraint\nfrom ..csp.constraints import AllDifferentConstraint, FunctionConstraint\nfrom ..csp.arc_consistency import AC3\nfrom ..csp.search import BacktrackingSearch, VariableOrdering, ValueOrdering\n\n\n@dataclass\nclass GridVariable(Variable):\n    \"\"\"Variable representing a cell or region in the grid\"\"\"\n    position: Tuple[int, int] = (0, 0)\n    region_id: Optional[int] = None\n    color_constraint: Optional[Set[int]] = None\n    \n    def __post_init__(self):\n        super().__init__(self.name, self.domain)\n        \n    def is_neighbor(self, other: 'GridVariable') -> bool:\n        \"\"\"Check if two variables are neighbors\"\"\"\n        if not isinstance(other, GridVariable):\n            return False\n        \n        y1, x1 = self.position\n        y2, x2 = other.position\n        \n        # Adjacent cells (4-connected)\n        return abs(y1 - y2) + abs(x1 - x2) == 1\n\n\n@dataclass\nclass ColorConstraint(Constraint):\n    \"\"\"Constraint on cell colors\"\"\"\n    allowed_colors: Set[int]\n    \n    def is_satisfied(self, assignment: Dict[str, Any]) -> bool:\n        for var in self.variables:\n            if var in assignment:\n                if assignment[var] not in self.allowed_colors:\n                    return False\n        return True\n\n\n@dataclass\nclass PatternConstraint(Constraint):\n    \"\"\"Constraint enforcing pattern relationships\"\"\"\n    pattern_type: str  # 'repeat', 'mirror', 'rotate', etc.\n    source_vars: List[str]\n    target_vars: List[str]\n    \n    def is_satisfied(self, assignment: Dict[str, Any]) -> bool:\n        # Check if all variables are assigned\n        all_assigned = all(v in assignment for v in self.variables)\n        if not all_assigned:\n            return True  # Can't check partial assignment\n        \n        if self.pattern_type == 'repeat':\n            # Target should repeat source\n            source_values = [assignment[v] for v in self.source_vars]\n            target_values = [assignment[v] for v in self.target_vars]\n            \n            if len(target_values) >= len(source_values):\n                # Check if target starts with source pattern\n                for i in range(0, len(target_values), len(source_values)):\n                    chunk = target_values[i:i+len(source_values)]\n                    if chunk != source_values[:len(chunk)]:\n                        return False\n            return True\n        \n        elif self.pattern_type == 'mirror':\n            # Target should be mirror of source\n            source_values = [assignment[v] for v in self.source_vars]\n            target_values = [assignment[v] for v in self.target_vars]\n            return target_values == source_values[::-1]\n        \n        elif self.pattern_type == 'equal':\n            # All variables should have same value\n            values = [assignment[v] for v in self.variables]\n            return len(set(values)) == 1\n        \n        return True\n\n\n@dataclass\nclass SpatialConstraint(Constraint):\n    \"\"\"Constraint on spatial relationships between cells\"\"\"\n    constraint_type: str  # 'connected', 'separated', 'aligned', etc.\n    \n    def is_satisfied(self, assignment: Dict[str, Any]) -> bool:\n        if self.constraint_type == 'connected':\n            # Check if assigned cells form connected component\n            return self._check_connectivity(assignment)\n        \n        elif self.constraint_type == 'separated':\n            # Check if regions are separated\n            return self._check_separation(assignment)\n        \n        elif self.constraint_type == 'aligned':\n            # Check if cells are aligned (row/column)\n            return self._check_alignment(assignment)\n        \n        return True\n    \n    def _check_connectivity(self, assignment: Dict[str, Any]) -> bool:\n        \"\"\"Check if assigned cells form connected component\"\"\"\n        # Implementation would check 4-connectivity\n        return True\n    \n    def _check_separation(self, assignment: Dict[str, Any]) -> bool:\n        \"\"\"Check if regions are properly separated\"\"\"\n        return True\n    \n    def _check_alignment(self, assignment: Dict[str, Any]) -> bool:\n        \"\"\"Check if cells are aligned\"\"\"\n        return True\n\n\nclass ARCtoCSPConverter:\n    \"\"\"Converts ARC tasks to CSP problems\"\"\"\n    \n    def __init__(self):\n        self.variables = []\n        self.constraints = []\n        self.grid_shape = None\n    \n    def convert_task(self, \n                     input_grid: Grid,\n                     output_grid: Optional[Grid] = None,\n                     examples: Optional[List[Tuple[Grid, Grid]]] = None) -> CSP:\n        \"\"\"Convert ARC task to CSP problem\"\"\"\n        \n        self.grid_shape = output_grid.shape if output_grid else input_grid.shape\n        \n        # Create variables for each cell\n        self._create_cell_variables(input_grid, output_grid)\n        \n        # Add constraints based on patterns\n        if examples:\n            self._infer_constraints_from_examples(examples)\n        else:\n            self._infer_constraints_from_pair(input_grid, output_grid)\n        \n        # Create and return CSP\n        return CSP(self.variables, self.constraints)\n    \n    def _create_cell_variables(self, \n                              input_grid: Grid,\n                              output_grid: Optional[Grid] = None):\n        \"\"\"Create variables for each cell in the grid\"\"\"\n        self.variables = []\n        \n        height, width = self.grid_shape\n        \n        for y in range(height):\n            for x in range(width):\n                var_name = f\"cell_{y}_{x}\"\n                \n                # Determine domain based on input and output\n                if output_grid and y < output_grid.height and x < output_grid.width:\n                    # If we know the output, domain is just that value\n                    domain = {int(output_grid.data[y, x])}\n                else:\n                    # Otherwise, domain is all possible colors\n                    domain = set(range(10))\n                \n                var = GridVariable(\n                    name=var_name,\n                    domain=domain,\n                    position=(y, x)\n                )\n                \n                self.variables.append(var)\n    \n    def _infer_constraints_from_examples(self, \n                                        examples: List[Tuple[Grid, Grid]]):\n        \"\"\"Infer constraints from training examples\"\"\"\n        \n        # Analyze patterns across examples\n        for input_grid, output_grid in examples:\n            # Detect patterns in output\n            detector = PatternDetector(output_grid)\n            \n            # Check for symmetries\n            symmetries = detector.get_symmetries()\n            if symmetries['horizontal']:\n                self._add_symmetry_constraint('horizontal')\n            if symmetries['vertical']:\n                self._add_symmetry_constraint('vertical')\n            \n            # Check for repeating patterns\n            patterns = detector.find_repeating_patterns()\n            if patterns:\n                self._add_pattern_constraints(patterns)\n            \n            # Check for color relationships\n            self._add_color_constraints(input_grid, output_grid)\n            \n            # Check for spatial relationships\n            self._add_spatial_constraints(input_grid, output_grid)\n    \n    def _infer_constraints_from_pair(self, \n                                    input_grid: Grid,\n                                    output_grid: Grid):\n        \"\"\"Infer constraints from single input/output pair\"\"\"\n        \n        # Basic color mapping constraint\n        input_colors = input_grid.unique_colors\n        output_colors = output_grid.unique_colors\n        \n        # If colors are preserved, add color preservation constraint\n        if input_colors == output_colors:\n            for y in range(min(input_grid.height, output_grid.height)):\n                for x in range(min(input_grid.width, output_grid.width)):\n                    if input_grid.data[y, x] == output_grid.data[y, x]:\n                        # This cell preserves its color\n                        var_name = f\"cell_{y}_{x}\"\n                        var = next((v for v in self.variables if v.name == var_name), None)\n                        if var:\n                            var.domain = Domain([input_grid.data[y, x]])\n    \n    def _add_symmetry_constraint(self, symmetry_type: str):\n        \"\"\"Add symmetry constraint\"\"\"\n        height, width = self.grid_shape\n        \n        if symmetry_type == 'horizontal':\n            # Left-right symmetry\n            for y in range(height):\n                for x in range(width // 2):\n                    left_var = f\"cell_{y}_{x}\"\n                    right_var = f\"cell_{y}_{width - 1 - x}\"\n                    \n                    constraint = PatternConstraint(\n                        variables=[left_var, right_var],\n                        pattern_type='equal',\n                        source_vars=[left_var],\n                        target_vars=[right_var]\n                    )\n                    self.constraints.append(constraint)\n        \n        elif symmetry_type == 'vertical':\n            # Top-bottom symmetry\n            for y in range(height // 2):\n                for x in range(width):\n                    top_var = f\"cell_{y}_{x}\"\n                    bottom_var = f\"cell_{height - 1 - y}_{x}\"\n                    \n                    constraint = PatternConstraint(\n                        variables=[top_var, bottom_var],\n                        pattern_type='equal',\n                        source_vars=[top_var],\n                        target_vars=[bottom_var]\n                    )\n                    self.constraints.append(constraint)\n    \n    def _add_pattern_constraints(self, patterns: List[Tuple[Grid, int]]):\n        \"\"\"Add constraints for repeating patterns\"\"\"\n        if not patterns:\n            return\n        \n        # Take the most frequent pattern\n        pattern_grid, frequency = patterns[0]\n        pattern_h, pattern_w = pattern_grid.shape\n        \n        height, width = self.grid_shape\n        \n        # Add constraints for pattern repetition\n        for y in range(0, height - pattern_h + 1, pattern_h):\n            for x in range(0, width - pattern_w + 1, pattern_w):\n                # Create constraint for this pattern instance\n                pattern_vars = []\n                for py in range(pattern_h):\n                    for px in range(pattern_w):\n                        var_name = f\"cell_{y + py}_{x + px}\"\n                        pattern_vars.append(var_name)\n                \n                # Constraint that enforces the pattern\n                def pattern_checker(assignment, pvars=pattern_vars, pgrid=pattern_grid):\n                    for i, var in enumerate(pvars):\n                        if var in assignment:\n                            py = i // pgrid.width\n                            px = i % pgrid.width\n                            if assignment[var] != pgrid.data[py, px]:\n                                return False\n                    return True\n                \n                constraint = FunctionConstraint(\n                    variables=pattern_vars,\n                    function=pattern_checker\n                )\n                self.constraints.append(constraint)\n    \n    def _add_color_constraints(self, input_grid: Grid, output_grid: Grid):\n        \"\"\"Add color-based constraints\"\"\"\n        input_colors = input_grid.unique_colors\n        output_colors = output_grid.unique_colors\n        \n        # If certain colors are never used, exclude them from domains\n        unused_colors = set(range(10)) - output_colors\n        \n        for var in self.variables:\n            if isinstance(var, GridVariable):\n                var.domain.values = [v for v in var.domain.values \n                                    if v not in unused_colors]\n        \n        # Add color mapping constraints if there's a clear mapping\n        color_map = self._infer_color_mapping(input_grid, output_grid)\n        if color_map:\n            for y in range(min(input_grid.height, output_grid.height)):\n                for x in range(min(input_grid.width, output_grid.width)):\n                    input_color = input_grid.data[y, x]\n                    if input_color in color_map:\n                        var_name = f\"cell_{y}_{x}\"\n                        var = next((v for v in self.variables if v.name == var_name), None)\n                        if var:\n                            var.domain = Domain([color_map[input_color]])\n    \n    def _infer_color_mapping(self, \n                           input_grid: Grid,\n                           output_grid: Grid) -> Optional[Dict[int, int]]:\n        \"\"\"Infer color mapping between input and output\"\"\"\n        if input_grid.shape != output_grid.shape:\n            return None\n        \n        color_map = {}\n        \n        for color in input_grid.unique_colors:\n            input_positions = set(input_grid.get_color_positions(color))\n            \n            # Find which color these positions map to in output\n            output_colors_at_positions = set()\n            for y, x in input_positions:\n                if y < output_grid.height and x < output_grid.width:\n                    output_colors_at_positions.add(output_grid.data[y, x])\n            \n            # If all map to same color, we have a mapping\n            if len(output_colors_at_positions) == 1:\n                color_map[color] = output_colors_at_positions.pop()\n        \n        return color_map if color_map else None\n    \n    def _add_spatial_constraints(self, input_grid: Grid, output_grid: Grid):\n        \"\"\"Add spatial relationship constraints\"\"\"\n        \n        # Check for connected components\n        detector = PatternDetector(output_grid)\n        components = detector.get_connected_components()\n        \n        # Add connectivity constraints for each component\n        for component in components:\n            if component['color'] == 0:  # Skip background\n                continue\n            \n            component_vars = []\n            for y, x in component['pixels']:\n                var_name = f\"cell_{y}_{x}\"\n                component_vars.append(var_name)\n            \n            if len(component_vars) > 1:\n                constraint = SpatialConstraint(\n                    variables=component_vars,\n                    constraint_type='connected'\n                )\n                self.constraints.append(constraint)\n\n\nclass CSPSolver:\n    \"\"\"CSP-based solver for ARC tasks\"\"\"\n    \n    def __init__(self, config: Optional[Dict[str, Any]] = None):\n        config = config or {}\n        \n        self.converter = ARCtoCSPConverter()\n        # BacktrackingSearch will be initialized with a CSP in solve method\n        self.search_config = {\n            'variable_ordering': VariableOrdering.MRV_DEGREE,\n            'value_ordering': ValueOrdering.LCV\n        }\n        \n        self.max_iterations = config.get('max_iterations', 10000)\n        self.timeout = config.get('timeout', 5.0)\n    \n    def solve(self,\n             train_inputs: List[np.ndarray],\n             train_outputs: List[np.ndarray],\n             test_input: np.ndarray) -> Optional[np.ndarray]:\n        \"\"\"Solve ARC task using CSP approach\"\"\"\n        \n        # Convert to Grid objects\n        train_input_grids = [Grid(inp) for inp in train_inputs]\n        train_output_grids = [Grid(out) for out in train_outputs]\n        test_input_grid = Grid(test_input)\n        \n        # Convert to CSP\n        examples = list(zip(train_input_grids, train_output_grids))\n        \n        # Determine output shape (assume same as input for now)\n        output_shape = self._infer_output_shape(\n            train_input_grids, \n            train_output_grids,\n            test_input_grid\n        )\n        \n        # Create template output grid\n        template_output = Grid(np.zeros(output_shape, dtype=np.int8))\n        \n        # Convert to CSP\n        csp = self.converter.convert_task(\n            test_input_grid,\n            template_output,\n            examples\n        )\n        \n        # Apply arc consistency\n        ac3 = AC3(csp)\n        ac3.run()\n        \n        # Create search with the CSP\n        search = BacktrackingSearch(\n            csp,\n            variable_ordering=self.search_config['variable_ordering'],\n            value_ordering=self.search_config['value_ordering']\n        )\n        \n        # Search for solution\n        solution = search.solve()\n        \n        if solution:\n            # Convert solution to grid\n            return self._solution_to_grid(solution, output_shape)\n        \n        return None\n    \n    def _infer_output_shape(self,\n                          train_inputs: List[Grid],\n                          train_outputs: List[Grid],\n                          test_input: Grid) -> Tuple[int, int]:\n        \"\"\"Infer the output shape based on examples\"\"\"\n        \n        # Check if output shape is consistent across examples\n        output_shapes = [out.shape for out in train_outputs]\n        \n        if len(set(output_shapes)) == 1:\n            # All outputs have same shape\n            output_shape = output_shapes[0]\n            \n            # Check if it's a fixed shape or relative to input\n            input_shapes = [inp.shape for inp in train_inputs]\n            \n            if len(set(input_shapes)) == 1 and input_shapes[0] == output_shape:\n                # Same shape as input\n                return test_input.shape\n            else:\n                # Fixed output shape\n                return output_shape\n        \n        # Try to infer scaling relationship\n        scale_factors = []\n        for inp, out in zip(train_inputs, train_outputs):\n            if out.height % inp.height == 0 and out.width % inp.width == 0:\n                h_scale = out.height // inp.height\n                w_scale = out.width // inp.width\n                if h_scale == w_scale:\n                    scale_factors.append(h_scale)\n        \n        if scale_factors and len(set(scale_factors)) == 1:\n            # Consistent scaling\n            scale = scale_factors[0]\n            return (test_input.height * scale, test_input.width * scale)\n        \n        # Default: same as input\n        return test_input.shape\n    \n    def _solution_to_grid(self, \n                        solution: Dict[str, Any],\n                        shape: Tuple[int, int]) -> np.ndarray:\n        \"\"\"Convert CSP solution to grid\"\"\"\n        height, width = shape\n        grid = np.zeros((height, width), dtype=np.int8)\n        \n        for y in range(height):\n            for x in range(width):\n                var_name = f\"cell_{y}_{x}\"\n                if var_name in solution:\n                    grid[y, x] = solution[var_name]\n        \n        return grid\n    \n    def solve_with_constraints(self,\n                              input_grid: Grid,\n                              constraints: List[Constraint]) -> Optional[Grid]:\n        \"\"\"Solve with explicit constraints\"\"\"\n        \n        # Create CSP with given constraints\n        height, width = input_grid.shape\n        variables = []\n        \n        for y in range(height):\n            for x in range(width):\n                var = GridVariable(\n                    name=f\"cell_{y}_{x}\",\n                    domain=Domain(list(range(10))),\n                    position=(y, x)\n                )\n                variables.append(var)\n        \n        csp = CSP(variables, constraints)\n        \n        # Apply arc consistency\n        ac3 = AC3(csp)\n        ac3.run()\n        \n        # Create search with the CSP\n        search = BacktrackingSearch(\n            csp,\n            variable_ordering=self.search_config['variable_ordering'],\n            value_ordering=self.search_config['value_ordering']\n        )\n        \n        # Search for solution\n        solution = search.solve()\n        \n        if solution:\n            # Convert to grid\n            result_data = self._solution_to_grid(solution, input_grid.shape)\n            return Grid(result_data)\n        \n        return None","size_bytes":20466},"src/solvers/ensemble_solver.py":{"content":"\"\"\"\nEnsemble Solver for ARC Prize 2025\nCombines multiple solving strategies for robust solutions\n\"\"\"\n\nimport numpy as np\nfrom typing import List, Dict, Optional, Tuple, Any, Callable\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nimport concurrent.futures\nimport time\nfrom collections import Counter\n\nfrom ..arc.grid_operations import Grid\nfrom .program_synthesis import ProgramSynthesisEngine\nfrom .csp_solver import CSPSolver\nfrom .pattern_solver import PatternSolver\n\n\nclass SolverType(Enum):\n    \"\"\"Types of solvers in the ensemble\"\"\"\n    PROGRAM_SYNTHESIS = \"program_synthesis\"\n    CSP = \"csp\"\n    PATTERN = \"pattern\"\n    HYBRID = \"hybrid\"\n\n\n@dataclass\nclass SolverResult:\n    \"\"\"Result from a single solver\"\"\"\n    solver_type: SolverType\n    solution: Optional[np.ndarray]\n    confidence: float\n    execution_time: float\n    metadata: Dict[str, Any] = field(default_factory=dict)\n\n\n@dataclass\nclass EnsembleResult:\n    \"\"\"Combined result from ensemble\"\"\"\n    final_solution: Optional[np.ndarray]\n    individual_results: List[SolverResult]\n    consensus_score: float\n    total_time: float\n    strategy_used: str\n\n\nclass SolverWrapper:\n    \"\"\"Wrapper for individual solvers with timeout and error handling\"\"\"\n    \n    def __init__(self, \n                 solver_type: SolverType,\n                 solver_instance: Any,\n                 timeout: float = 10.0):\n        self.solver_type = solver_type\n        self.solver = solver_instance\n        self.timeout = timeout\n    \n    def solve(self,\n             train_inputs: List[np.ndarray],\n             train_outputs: List[np.ndarray],\n             test_input: np.ndarray) -> SolverResult:\n        \"\"\"Solve with timeout and error handling\"\"\"\n        \n        start_time = time.time()\n        \n        try:\n            # Execute solver with timeout\n            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n                future = executor.submit(\n                    self.solver.solve,\n                    train_inputs,\n                    train_outputs,\n                    test_input\n                )\n                \n                try:\n                    solution = future.result(timeout=self.timeout)\n                    execution_time = time.time() - start_time\n                    \n                    if solution is not None:\n                        confidence = self._calculate_confidence(\n                            solution, train_inputs, train_outputs\n                        )\n                    else:\n                        confidence = 0.0\n                    \n                    return SolverResult(\n                        solver_type=self.solver_type,\n                        solution=solution,\n                        confidence=confidence,\n                        execution_time=execution_time\n                    )\n                    \n                except concurrent.futures.TimeoutError:\n                    return SolverResult(\n                        solver_type=self.solver_type,\n                        solution=None,\n                        confidence=0.0,\n                        execution_time=self.timeout,\n                        metadata={'error': 'timeout'}\n                    )\n                    \n        except Exception as e:\n            return SolverResult(\n                solver_type=self.solver_type,\n                solution=None,\n                confidence=0.0,\n                execution_time=time.time() - start_time,\n                metadata={'error': str(e)}\n            )\n    \n    def _calculate_confidence(self,\n                            solution: np.ndarray,\n                            train_inputs: List[np.ndarray],\n                            train_outputs: List[np.ndarray]) -> float:\n        \"\"\"Calculate confidence score for solution\"\"\"\n        \n        # Basic confidence based on solver type\n        base_confidence = {\n            SolverType.PROGRAM_SYNTHESIS: 0.8,\n            SolverType.CSP: 0.7,\n            SolverType.PATTERN: 0.75,\n            SolverType.HYBRID: 0.85\n        }.get(self.solver_type, 0.5)\n        \n        # Adjust based on solution characteristics\n        if solution is not None:\n            # Check if solution size is reasonable\n            avg_output_size = np.mean([o.size for o in train_outputs])\n            size_ratio = solution.size / avg_output_size if avg_output_size > 0 else 1.0\n            \n            if 0.5 <= size_ratio <= 2.0:\n                base_confidence *= 1.1\n            else:\n                base_confidence *= 0.9\n            \n            # Check if colors are reasonable\n            solution_colors = set(np.unique(solution))\n            train_colors = set()\n            for out in train_outputs:\n                train_colors.update(np.unique(out))\n            \n            if solution_colors.issubset(train_colors):\n                base_confidence *= 1.1\n            else:\n                base_confidence *= 0.8\n        \n        return min(1.0, base_confidence)\n\n\nclass VotingMechanism:\n    \"\"\"Voting mechanism for solution selection\"\"\"\n    \n    def __init__(self, voting_type: str = \"weighted\"):\n        self.voting_type = voting_type\n    \n    def vote(self, results: List[SolverResult]) -> Optional[np.ndarray]:\n        \"\"\"Select solution through voting\"\"\"\n        \n        if not results:\n            return None\n        \n        # Filter out None solutions\n        valid_results = [r for r in results if r.solution is not None]\n        \n        if not valid_results:\n            return None\n        \n        if len(valid_results) == 1:\n            return valid_results[0].solution\n        \n        if self.voting_type == \"weighted\":\n            return self._weighted_vote(valid_results)\n        elif self.voting_type == \"majority\":\n            return self._majority_vote(valid_results)\n        elif self.voting_type == \"confidence\":\n            return self._confidence_vote(valid_results)\n        else:\n            return valid_results[0].solution\n    \n    def _weighted_vote(self, results: List[SolverResult]) -> np.ndarray:\n        \"\"\"Weighted voting based on confidence scores\"\"\"\n        \n        # Group solutions by similarity\n        solution_groups = self._group_similar_solutions(results)\n        \n        # Calculate weighted score for each group\n        group_scores = {}\n        for group_id, group_results in solution_groups.items():\n            total_weight = sum(r.confidence for r in group_results)\n            group_scores[group_id] = total_weight\n        \n        # Select group with highest score\n        best_group = max(group_scores, key=group_scores.get)\n        best_results = solution_groups[best_group]\n        \n        # Return solution with highest confidence in best group\n        return max(best_results, key=lambda r: r.confidence).solution\n    \n    def _majority_vote(self, results: List[SolverResult]) -> np.ndarray:\n        \"\"\"Simple majority voting\"\"\"\n        \n        # Group identical solutions\n        solution_counts = Counter()\n        solution_map = {}\n        \n        for result in results:\n            # Create hashable representation\n            solution_key = tuple(result.solution.flatten())\n            solution_counts[solution_key] += 1\n            solution_map[solution_key] = result.solution\n        \n        # Return most common solution\n        most_common = solution_counts.most_common(1)[0][0]\n        return solution_map[most_common]\n    \n    def _confidence_vote(self, results: List[SolverResult]) -> np.ndarray:\n        \"\"\"Select solution with highest confidence\"\"\"\n        return max(results, key=lambda r: r.confidence).solution\n    \n    def _group_similar_solutions(self, \n                                results: List[SolverResult]) -> Dict[int, List[SolverResult]]:\n        \"\"\"Group solutions by similarity\"\"\"\n        \n        groups = {}\n        group_id = 0\n        \n        for result in results:\n            found_group = False\n            \n            for gid, group_results in groups.items():\n                # Check if solution is similar to group\n                representative = group_results[0].solution\n                \n                if self._solutions_similar(result.solution, representative):\n                    groups[gid].append(result)\n                    found_group = True\n                    break\n            \n            if not found_group:\n                groups[group_id] = [result]\n                group_id += 1\n        \n        return groups\n    \n    def _solutions_similar(self, sol1: np.ndarray, sol2: np.ndarray) -> bool:\n        \"\"\"Check if two solutions are similar\"\"\"\n        \n        # Same shape\n        if sol1.shape != sol2.shape:\n            return False\n        \n        # Calculate similarity\n        matches = np.sum(sol1 == sol2)\n        total = sol1.size\n        similarity = matches / total if total > 0 else 0\n        \n        return similarity > 0.9\n\n\nclass HybridSolver:\n    \"\"\"Hybrid solver combining multiple strategies\"\"\"\n    \n    def __init__(self):\n        self.program_synthesis = ProgramSynthesisEngine()\n        self.pattern_solver = PatternSolver()\n    \n    def solve(self,\n             train_inputs: List[np.ndarray],\n             train_outputs: List[np.ndarray],\n             test_input: np.ndarray) -> Optional[np.ndarray]:\n        \"\"\"Solve using hybrid approach\"\"\"\n        \n        # Try pattern detection first (faster)\n        pattern_solution = self.pattern_solver.solve(\n            train_inputs, train_outputs, test_input\n        )\n        \n        if pattern_solution is not None:\n            return pattern_solution\n        \n        # Fall back to program synthesis\n        return self.program_synthesis.solve(\n            train_inputs, train_outputs, test_input\n        )\n\n\nclass EnsembleSolver:\n    \"\"\"Main ensemble solver combining multiple strategies\"\"\"\n    \n    def __init__(self, config: Optional[Dict[str, Any]] = None):\n        config = config or {}\n        \n        # Initialize component solvers\n        self.solvers = self._initialize_solvers(config)\n        \n        # Voting mechanism\n        self.voting = VotingMechanism(\n            voting_type=config.get('voting_type', 'weighted')\n        )\n        \n        # Configuration\n        self.parallel = config.get('parallel', True)\n        self.timeout_per_solver = config.get('timeout_per_solver', 10.0)\n        self.min_consensus = config.get('min_consensus', 0.5)\n        self.adaptive = config.get('adaptive', True)\n    \n    def _initialize_solvers(self, config: Dict[str, Any]) -> List[SolverWrapper]:\n        \"\"\"Initialize component solvers\"\"\"\n        solvers = []\n        \n        # Program Synthesis Solver\n        if config.get('use_program_synthesis', True):\n            solver = ProgramSynthesisEngine(\n                config.get('program_synthesis_config', {})\n            )\n            wrapper = SolverWrapper(\n                SolverType.PROGRAM_SYNTHESIS,\n                solver,\n                config.get('timeout_per_solver', 10.0)\n            )\n            solvers.append(wrapper)\n        \n        # CSP Solver\n        if config.get('use_csp', True):\n            solver = CSPSolver(\n                config.get('csp_config', {})\n            )\n            wrapper = SolverWrapper(\n                SolverType.CSP,\n                solver,\n                config.get('timeout_per_solver', 10.0)\n            )\n            solvers.append(wrapper)\n        \n        # Pattern Solver\n        if config.get('use_pattern', True):\n            solver = PatternSolver(\n                config.get('pattern_config', {})\n            )\n            wrapper = SolverWrapper(\n                SolverType.PATTERN,\n                solver,\n                config.get('timeout_per_solver', 10.0)\n            )\n            solvers.append(wrapper)\n        \n        # Hybrid Solver\n        if config.get('use_hybrid', True):\n            solver = HybridSolver()\n            wrapper = SolverWrapper(\n                SolverType.HYBRID,\n                solver,\n                config.get('timeout_per_solver', 15.0)\n            )\n            solvers.append(wrapper)\n        \n        return solvers\n    \n    def solve(self,\n             train_inputs: List[np.ndarray],\n             train_outputs: List[np.ndarray],\n             test_input: np.ndarray) -> EnsembleResult:\n        \"\"\"Solve using ensemble approach\"\"\"\n        \n        start_time = time.time()\n        \n        # Select solvers based on task characteristics\n        if self.adaptive:\n            selected_solvers = self._select_solvers(\n                train_inputs, train_outputs, test_input\n            )\n        else:\n            selected_solvers = self.solvers\n        \n        # Execute solvers\n        if self.parallel:\n            results = self._solve_parallel(\n                selected_solvers, train_inputs, train_outputs, test_input\n            )\n        else:\n            results = self._solve_sequential(\n                selected_solvers, train_inputs, train_outputs, test_input\n            )\n        \n        # Vote on solution\n        final_solution = self.voting.vote(results)\n        \n        # Calculate consensus score\n        consensus_score = self._calculate_consensus(results)\n        \n        # Determine strategy used\n        if final_solution is not None:\n            winning_result = next(\n                (r for r in results if r.solution is not None and \n                 np.array_equal(r.solution, final_solution)), None\n            )\n            strategy_used = winning_result.solver_type.value if winning_result else \"unknown\"\n        else:\n            strategy_used = \"none\"\n        \n        total_time = time.time() - start_time\n        \n        return EnsembleResult(\n            final_solution=final_solution,\n            individual_results=results,\n            consensus_score=consensus_score,\n            total_time=total_time,\n            strategy_used=strategy_used\n        )\n    \n    def _select_solvers(self,\n                       train_inputs: List[np.ndarray],\n                       train_outputs: List[np.ndarray],\n                       test_input: np.ndarray) -> List[SolverWrapper]:\n        \"\"\"Select appropriate solvers based on task characteristics\"\"\"\n        \n        selected = []\n        \n        # Analyze task characteristics\n        characteristics = self._analyze_task(train_inputs, train_outputs, test_input)\n        \n        # Select solvers based on characteristics\n        for solver in self.solvers:\n            if self._should_use_solver(solver.solver_type, characteristics):\n                selected.append(solver)\n        \n        # Ensure at least one solver is selected\n        if not selected:\n            selected = [self.solvers[0]]\n        \n        return selected\n    \n    def _analyze_task(self,\n                     train_inputs: List[np.ndarray],\n                     train_outputs: List[np.ndarray],\n                     test_input: np.ndarray) -> Dict[str, Any]:\n        \"\"\"Analyze task characteristics\"\"\"\n        \n        characteristics = {\n            'size_change': False,\n            'color_change': False,\n            'pattern_repetition': False,\n            'symmetry': False,\n            'complexity': 'low'\n        }\n        \n        # Check for size changes\n        for inp, out in zip(train_inputs, train_outputs):\n            if inp.shape != out.shape:\n                characteristics['size_change'] = True\n                break\n        \n        # Check for color changes\n        for inp, out in zip(train_inputs, train_outputs):\n            if set(np.unique(inp)) != set(np.unique(out)):\n                characteristics['color_change'] = True\n                break\n        \n        # Check for patterns\n        for out in train_outputs:\n            grid = Grid(out)\n            from ..arc.pattern_detector import PatternDetector\n            detector = PatternDetector(grid)\n            \n            if detector.find_repeating_patterns():\n                characteristics['pattern_repetition'] = True\n            \n            symmetries = detector.get_symmetries()\n            if any(symmetries.values()):\n                characteristics['symmetry'] = True\n        \n        # Estimate complexity\n        avg_size = np.mean([o.size for o in train_outputs])\n        if avg_size > 100:\n            characteristics['complexity'] = 'high'\n        elif avg_size > 25:\n            characteristics['complexity'] = 'medium'\n        \n        return characteristics\n    \n    def _should_use_solver(self,\n                         solver_type: SolverType,\n                         characteristics: Dict[str, Any]) -> bool:\n        \"\"\"Determine if solver should be used for task\"\"\"\n        \n        if solver_type == SolverType.PATTERN:\n            # Good for pattern-based tasks\n            return (characteristics['pattern_repetition'] or \n                   characteristics['symmetry'] or\n                   characteristics['size_change'])\n        \n        elif solver_type == SolverType.PROGRAM_SYNTHESIS:\n            # Good for complex transformations\n            return (characteristics['complexity'] in ['medium', 'high'] or\n                   characteristics['color_change'])\n        \n        elif solver_type == SolverType.CSP:\n            # Good for constraint-based tasks\n            return (characteristics['color_change'] or\n                   not characteristics['size_change'])\n        \n        elif solver_type == SolverType.HYBRID:\n            # Always useful as fallback\n            return True\n        \n        return True\n    \n    def _solve_parallel(self,\n                       solvers: List[SolverWrapper],\n                       train_inputs: List[np.ndarray],\n                       train_outputs: List[np.ndarray],\n                       test_input: np.ndarray) -> List[SolverResult]:\n        \"\"\"Execute solvers in parallel\"\"\"\n        \n        results = []\n        \n        with concurrent.futures.ThreadPoolExecutor(max_workers=len(solvers)) as executor:\n            futures = []\n            \n            for solver in solvers:\n                future = executor.submit(\n                    solver.solve,\n                    train_inputs,\n                    train_outputs,\n                    test_input\n                )\n                futures.append((solver.solver_type, future))\n            \n            for solver_type, future in futures:\n                try:\n                    result = future.result(timeout=self.timeout_per_solver * 2)\n                    results.append(result)\n                except Exception as e:\n                    results.append(SolverResult(\n                        solver_type=solver_type,\n                        solution=None,\n                        confidence=0.0,\n                        execution_time=0.0,\n                        metadata={'error': str(e)}\n                    ))\n        \n        return results\n    \n    def _solve_sequential(self,\n                        solvers: List[SolverWrapper],\n                        train_inputs: List[np.ndarray],\n                        train_outputs: List[np.ndarray],\n                        test_input: np.ndarray) -> List[SolverResult]:\n        \"\"\"Execute solvers sequentially\"\"\"\n        \n        results = []\n        \n        for solver in solvers:\n            result = solver.solve(train_inputs, train_outputs, test_input)\n            results.append(result)\n            \n            # Early stopping if high confidence solution found\n            if result.confidence > 0.95:\n                break\n        \n        return results\n    \n    def _calculate_consensus(self, results: List[SolverResult]) -> float:\n        \"\"\"Calculate consensus score among solvers\"\"\"\n        \n        valid_results = [r for r in results if r.solution is not None]\n        \n        if len(valid_results) <= 1:\n            return 0.0 if not valid_results else 1.0\n        \n        # Group similar solutions\n        groups = {}\n        for result in valid_results:\n            found_group = False\n            for group_id, group_results in groups.items():\n                if self._solutions_match(result.solution, group_results[0].solution):\n                    groups[group_id].append(result)\n                    found_group = True\n                    break\n            \n            if not found_group:\n                groups[len(groups)] = [result]\n        \n        # Calculate consensus as ratio of largest group\n        largest_group_size = max(len(g) for g in groups.values())\n        consensus = largest_group_size / len(valid_results)\n        \n        return consensus\n    \n    def _solutions_match(self, sol1: np.ndarray, sol2: np.ndarray) -> bool:\n        \"\"\"Check if two solutions match exactly\"\"\"\n        return sol1.shape == sol2.shape and np.array_equal(sol1, sol2)\n    \n    def get_solution(self,\n                    train_inputs: List[np.ndarray],\n                    train_outputs: List[np.ndarray],\n                    test_input: np.ndarray) -> Optional[np.ndarray]:\n        \"\"\"Get solution (simplified interface)\"\"\"\n        \n        result = self.solve(train_inputs, train_outputs, test_input)\n        return result.final_solution","size_bytes":21081},"src/solvers/pattern_solver.py":{"content":"\"\"\"\nPattern-based Solver for ARC Prize 2025\nSolves ARC tasks by detecting and applying common patterns\n\"\"\"\n\nimport numpy as np\nfrom typing import List, Dict, Optional, Tuple, Set, Any, Callable\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nfrom ..arc.grid_operations import Grid\nfrom ..arc.pattern_detector import PatternDetector\n\n\nclass PatternType(Enum):\n    \"\"\"Types of patterns commonly found in ARC tasks\"\"\"\n    TILING = \"tiling\"\n    SCALING = \"scaling\"\n    SYMMETRY = \"symmetry\"\n    ROTATION = \"rotation\"\n    COLOR_MAPPING = \"color_mapping\"\n    REPETITION = \"repetition\"\n    MIRRORING = \"mirroring\"\n    OVERLAY = \"overlay\"\n    EXTRACTION = \"extraction\"\n    FILLING = \"filling\"\n    CONNECTIVITY = \"connectivity\"\n    COUNTING = \"counting\"\n\n\n@dataclass\nclass DetectedPattern:\n    \"\"\"Represents a detected pattern in the task\"\"\"\n    pattern_type: PatternType\n    confidence: float\n    parameters: Dict[str, Any]\n    transformation: Optional[Callable] = None\n\n\nclass PatternSolver:\n    \"\"\"Solves ARC tasks by detecting and applying patterns\"\"\"\n    \n    def __init__(self, config: Optional[Dict[str, Any]] = None):\n        config = config or {}\n        self.min_confidence = config.get('min_confidence', 0.7)\n        self.max_patterns = config.get('max_patterns', 10)\n    \n    def solve(self,\n             train_inputs: List[np.ndarray],\n             train_outputs: List[np.ndarray],\n             test_input: np.ndarray) -> Optional[np.ndarray]:\n        \"\"\"Solve ARC task using pattern detection\"\"\"\n        \n        # Convert to Grid objects\n        train_input_grids = [Grid(inp) for inp in train_inputs]\n        train_output_grids = [Grid(out) for out in train_outputs]\n        test_input_grid = Grid(test_input)\n        \n        # Detect patterns from training examples\n        patterns = self._detect_patterns(train_input_grids, train_output_grids)\n        \n        if not patterns:\n            return None\n        \n        # Sort patterns by confidence\n        patterns.sort(key=lambda p: p.confidence, reverse=True)\n        \n        # Try applying patterns\n        for pattern in patterns[:self.max_patterns]:\n            if pattern.confidence < self.min_confidence:\n                break\n            \n            try:\n                result = self._apply_pattern(test_input_grid, pattern)\n                \n                # Validate result on training examples\n                if self._validate_pattern(pattern, train_input_grids, train_output_grids):\n                    return result.data\n            except Exception:\n                continue\n        \n        return None\n    \n    def _detect_patterns(self, \n                        inputs: List[Grid],\n                        outputs: List[Grid]) -> List[DetectedPattern]:\n        \"\"\"Detect patterns from training examples\"\"\"\n        patterns = []\n        \n        # Check each type of pattern\n        patterns.extend(self._detect_tiling_patterns(inputs, outputs))\n        patterns.extend(self._detect_scaling_patterns(inputs, outputs))\n        patterns.extend(self._detect_symmetry_patterns(inputs, outputs))\n        patterns.extend(self._detect_color_patterns(inputs, outputs))\n        patterns.extend(self._detect_repetition_patterns(inputs, outputs))\n        patterns.extend(self._detect_mirroring_patterns(inputs, outputs))\n        patterns.extend(self._detect_extraction_patterns(inputs, outputs))\n        patterns.extend(self._detect_filling_patterns(inputs, outputs))\n        patterns.extend(self._detect_connectivity_patterns(inputs, outputs))\n        \n        return patterns\n    \n    def _detect_tiling_patterns(self, \n                               inputs: List[Grid],\n                               outputs: List[Grid]) -> List[DetectedPattern]:\n        \"\"\"Detect tiling patterns\"\"\"\n        patterns = []\n        \n        for inp, out in zip(inputs, outputs):\n            # Check if output is tiled version of input\n            if out.height % inp.height == 0 and out.width % inp.width == 0:\n                rows = out.height // inp.height\n                cols = out.width // inp.width\n                \n                # Verify tiling\n                expected = inp.tile(rows, cols)\n                if expected == out:\n                    pattern = DetectedPattern(\n                        pattern_type=PatternType.TILING,\n                        confidence=1.0,\n                        parameters={'rows': rows, 'cols': cols}\n                    )\n                    patterns.append(pattern)\n                    break\n            \n            # Check if output contains repeated subpattern\n            detector = PatternDetector(out)\n            repeating = detector.find_repeating_patterns()\n            \n            if repeating:\n                pattern_grid, count = repeating[0]\n                if count > 2:\n                    # Check if input matches the pattern\n                    if pattern_grid.data.shape == inp.data.shape:\n                        if np.array_equal(pattern_grid.data, inp.data):\n                            pattern = DetectedPattern(\n                                pattern_type=PatternType.TILING,\n                                confidence=0.9,\n                                parameters={'pattern': pattern_grid, 'count': count}\n                            )\n                            patterns.append(pattern)\n        \n        return patterns\n    \n    def _detect_scaling_patterns(self,\n                                inputs: List[Grid],\n                                outputs: List[Grid]) -> List[DetectedPattern]:\n        \"\"\"Detect scaling patterns\"\"\"\n        patterns = []\n        \n        scale_factors = []\n        for inp, out in zip(inputs, outputs):\n            # Check for uniform scaling\n            if out.height % inp.height == 0 and out.width % inp.width == 0:\n                h_scale = out.height // inp.height\n                w_scale = out.width // inp.width\n                \n                if h_scale == w_scale:\n                    # Verify scaling\n                    expected = inp.scale(h_scale)\n                    if expected == out:\n                        scale_factors.append(h_scale)\n        \n        if scale_factors and len(set(scale_factors)) == 1:\n            pattern = DetectedPattern(\n                pattern_type=PatternType.SCALING,\n                confidence=1.0,\n                parameters={'factor': scale_factors[0]}\n            )\n            patterns.append(pattern)\n        \n        return patterns\n    \n    def _detect_symmetry_patterns(self,\n                                 inputs: List[Grid],\n                                 outputs: List[Grid]) -> List[DetectedPattern]:\n        \"\"\"Detect symmetry-based patterns\"\"\"\n        patterns = []\n        \n        for inp, out in zip(inputs, outputs):\n            detector_out = PatternDetector(out)\n            symmetries = detector_out.get_symmetries()\n            \n            # Check if output is symmetric version of input\n            if symmetries['horizontal']:\n                # Check if it's horizontally mirrored input\n                mirrored = inp.mirror('horizontal')\n                if mirrored == out:\n                    pattern = DetectedPattern(\n                        pattern_type=PatternType.SYMMETRY,\n                        confidence=1.0,\n                        parameters={'type': 'horizontal_mirror'}\n                    )\n                    patterns.append(pattern)\n                    break\n            \n            if symmetries['vertical']:\n                # Check if it's vertically mirrored input\n                mirrored = inp.mirror('vertical')\n                if mirrored == out:\n                    pattern = DetectedPattern(\n                        pattern_type=PatternType.SYMMETRY,\n                        confidence=1.0,\n                        parameters={'type': 'vertical_mirror'}\n                    )\n                    patterns.append(pattern)\n                    break\n            \n            # Check for rotation\n            for degrees in [90, 180, 270]:\n                rotated = inp.rotate(degrees)\n                if rotated == out:\n                    pattern = DetectedPattern(\n                        pattern_type=PatternType.ROTATION,\n                        confidence=1.0,\n                        parameters={'degrees': degrees}\n                    )\n                    patterns.append(pattern)\n                    break\n        \n        return patterns\n    \n    def _detect_color_patterns(self,\n                              inputs: List[Grid],\n                              outputs: List[Grid]) -> List[DetectedPattern]:\n        \"\"\"Detect color mapping patterns\"\"\"\n        patterns = []\n        \n        # Collect all color mappings\n        all_mappings = []\n        \n        for inp, out in zip(inputs, outputs):\n            if inp.shape != out.shape:\n                continue\n            \n            # Build color mapping\n            color_map = {}\n            for color in inp.unique_colors:\n                positions = inp.get_color_positions(color)\n                if positions:\n                    # Get colors at these positions in output\n                    out_colors = set()\n                    for y, x in positions:\n                        if y < out.height and x < out.width:\n                            out_colors.add(out.data[y, x])\n                    \n                    # If all map to same color\n                    if len(out_colors) == 1:\n                        color_map[color] = out_colors.pop()\n            \n            if color_map:\n                all_mappings.append(color_map)\n        \n        # Check for consistent mapping\n        if all_mappings and all(m == all_mappings[0] for m in all_mappings):\n            pattern = DetectedPattern(\n                pattern_type=PatternType.COLOR_MAPPING,\n                confidence=1.0,\n                parameters={'color_map': all_mappings[0]}\n            )\n            patterns.append(pattern)\n        \n        # Check for color inversion\n        for inp, out in zip(inputs, outputs):\n            if inp.shape == out.shape:\n                # Check if colors are inverted/swapped\n                inp_colors = sorted(inp.unique_colors - {0})\n                out_colors = sorted(out.unique_colors - {0})\n                \n                if inp_colors == out_colors:\n                    # Check for systematic inversion\n                    is_inverted = True\n                    for i, color in enumerate(inp_colors):\n                        expected_color = out_colors[-(i+1)]\n                        positions = inp.get_color_positions(color)\n                        \n                        for y, x in positions:\n                            if out.data[y, x] != expected_color:\n                                is_inverted = False\n                                break\n                        \n                        if not is_inverted:\n                            break\n                    \n                    if is_inverted:\n                        pattern = DetectedPattern(\n                            pattern_type=PatternType.COLOR_MAPPING,\n                            confidence=0.9,\n                            parameters={'type': 'inversion'}\n                        )\n                        patterns.append(pattern)\n                        break\n        \n        return patterns\n    \n    def _detect_repetition_patterns(self,\n                                  inputs: List[Grid],\n                                  outputs: List[Grid]) -> List[DetectedPattern]:\n        \"\"\"Detect repetition patterns\"\"\"\n        patterns = []\n        \n        for inp, out in zip(inputs, outputs):\n            detector = PatternDetector(out)\n            \n            # Check for periodic patterns\n            periodicity = detector.detect_periodicity()\n            \n            if periodicity['horizontal']:\n                period = periodicity['horizontal']\n                # Check if input matches the period\n                if inp.width == period:\n                    pattern = DetectedPattern(\n                        pattern_type=PatternType.REPETITION,\n                        confidence=0.9,\n                        parameters={'direction': 'horizontal', 'period': period}\n                    )\n                    patterns.append(pattern)\n            \n            if periodicity['vertical']:\n                period = periodicity['vertical']\n                # Check if input matches the period\n                if inp.height == period:\n                    pattern = DetectedPattern(\n                        pattern_type=PatternType.REPETITION,\n                        confidence=0.9,\n                        parameters={'direction': 'vertical', 'period': period}\n                    )\n                    patterns.append(pattern)\n        \n        return patterns\n    \n    def _detect_mirroring_patterns(self,\n                                  inputs: List[Grid],\n                                  outputs: List[Grid]) -> List[DetectedPattern]:\n        \"\"\"Detect mirroring patterns\"\"\"\n        patterns = []\n        \n        for inp, out in zip(inputs, outputs):\n            # Check horizontal mirroring\n            if out.width == inp.width * 2:\n                left_half = out.extract_subgrid(0, 0, out.height, inp.width)\n                right_half = out.extract_subgrid(0, inp.width, out.height, inp.width)\n                \n                if left_half == inp and right_half == inp.flip_horizontal():\n                    pattern = DetectedPattern(\n                        pattern_type=PatternType.MIRRORING,\n                        confidence=1.0,\n                        parameters={'axis': 'horizontal'}\n                    )\n                    patterns.append(pattern)\n                    break\n            \n            # Check vertical mirroring\n            if out.height == inp.height * 2:\n                top_half = out.extract_subgrid(0, 0, inp.height, out.width)\n                bottom_half = out.extract_subgrid(inp.height, 0, inp.height, out.width)\n                \n                if top_half == inp and bottom_half == inp.flip_vertical():\n                    pattern = DetectedPattern(\n                        pattern_type=PatternType.MIRRORING,\n                        confidence=1.0,\n                        parameters={'axis': 'vertical'}\n                    )\n                    patterns.append(pattern)\n                    break\n        \n        return patterns\n    \n    def _detect_extraction_patterns(self,\n                                  inputs: List[Grid],\n                                  outputs: List[Grid]) -> List[DetectedPattern]:\n        \"\"\"Detect extraction patterns\"\"\"\n        patterns = []\n        \n        for inp, out in zip(inputs, outputs):\n            # Check if output is a subgrid of input\n            if out.height <= inp.height and out.width <= inp.width:\n                found = False\n                for y in range(inp.height - out.height + 1):\n                    for x in range(inp.width - out.width + 1):\n                        subgrid = inp.extract_subgrid(y, x, out.height, out.width)\n                        if subgrid == out:\n                            pattern = DetectedPattern(\n                                pattern_type=PatternType.EXTRACTION,\n                                confidence=0.8,\n                                parameters={'y': y, 'x': x, \n                                          'height': out.height, 'width': out.width}\n                            )\n                            patterns.append(pattern)\n                            found = True\n                            break\n                    if found:\n                        break\n            \n            # Check if output is largest/smallest component\n            detector = PatternDetector(inp)\n            components = detector.get_connected_components()\n            if components:\n                largest = max(components, key=lambda c: c['area'])\n                smallest = min(components, key=lambda c: c['area'])\n                \n                # Check if output matches largest component\n                if self._component_matches_grid(largest, out):\n                    pattern = DetectedPattern(\n                        pattern_type=PatternType.EXTRACTION,\n                        confidence=0.9,\n                        parameters={'type': 'largest_component'}\n                    )\n                    patterns.append(pattern)\n                \n                # Check if output matches smallest component\n                elif self._component_matches_grid(smallest, out):\n                    pattern = DetectedPattern(\n                        pattern_type=PatternType.EXTRACTION,\n                        confidence=0.9,\n                        parameters={'type': 'smallest_component'}\n                    )\n                    patterns.append(pattern)\n        \n        return patterns\n    \n    def _detect_filling_patterns(self,\n                                inputs: List[Grid],\n                                outputs: List[Grid]) -> List[DetectedPattern]:\n        \"\"\"Detect filling patterns\"\"\"\n        patterns = []\n        \n        for inp, out in zip(inputs, outputs):\n            if inp.shape != out.shape:\n                continue\n            \n            # Check if background is filled\n            if 0 in inp.unique_colors and 0 not in out.unique_colors:\n                # Find what color replaced background\n                bg_positions = inp.get_color_positions(0)\n                if bg_positions:\n                    fill_colors = set()\n                    for y, x in bg_positions:\n                        fill_colors.add(out.data[y, x])\n                    \n                    if len(fill_colors) == 1:\n                        pattern = DetectedPattern(\n                            pattern_type=PatternType.FILLING,\n                            confidence=0.9,\n                            parameters={'fill_color': fill_colors.pop()}\n                        )\n                        patterns.append(pattern)\n            \n            # Check for flood fill patterns\n            detector = PatternDetector(inp)\n            components = detector.get_connected_components(include_background=True)\n            \n            for component in components:\n                if component['color'] == 0:  # Background component\n                    # Check if this region is filled in output\n                    region_colors = set()\n                    for y, x in component['pixels']:\n                        region_colors.add(out.data[y, x])\n                    \n                    if len(region_colors) == 1 and 0 not in region_colors:\n                        pattern = DetectedPattern(\n                            pattern_type=PatternType.FILLING,\n                            confidence=0.8,\n                            parameters={'type': 'flood_fill', \n                                      'fill_color': region_colors.pop()}\n                        )\n                        patterns.append(pattern)\n                        break\n        \n        return patterns\n    \n    def _detect_connectivity_patterns(self,\n                                    inputs: List[Grid],\n                                    outputs: List[Grid]) -> List[DetectedPattern]:\n        \"\"\"Detect connectivity-based patterns\"\"\"\n        patterns = []\n        \n        for inp, out in zip(inputs, outputs):\n            # Check if output shows connected components\n            inp_detector = PatternDetector(inp)\n            inp_components = inp_detector.get_connected_components()\n            \n            out_detector = PatternDetector(out)\n            out_components = out_detector.get_connected_components()\n            \n            # Check if number of components matches\n            if len(inp_components) == len(out_components):\n                # Check if components are transformed consistently\n                component_transforms = []\n                \n                for inp_comp in inp_components:\n                    # Find matching component in output\n                    for out_comp in out_components:\n                        if self._components_match(inp_comp, out_comp):\n                            component_transforms.append({\n                                'input': inp_comp,\n                                'output': out_comp\n                            })\n                            break\n                \n                if len(component_transforms) == len(inp_components):\n                    pattern = DetectedPattern(\n                        pattern_type=PatternType.CONNECTIVITY,\n                        confidence=0.8,\n                        parameters={'type': 'component_transform',\n                                  'transforms': component_transforms}\n                    )\n                    patterns.append(pattern)\n        \n        return patterns\n    \n    def _apply_pattern(self, \n                      grid: Grid,\n                      pattern: DetectedPattern) -> Grid:\n        \"\"\"Apply detected pattern to grid\"\"\"\n        \n        if pattern.pattern_type == PatternType.TILING:\n            params = pattern.parameters\n            if 'rows' in params and 'cols' in params:\n                return grid.tile(params['rows'], params['cols'])\n            elif 'pattern' in params:\n                # Tile the pattern\n                return params['pattern'].tile(3, 3)  # Default tiling\n        \n        elif pattern.pattern_type == PatternType.SCALING:\n            return grid.scale(pattern.parameters['factor'])\n        \n        elif pattern.pattern_type == PatternType.SYMMETRY:\n            if pattern.parameters['type'] == 'horizontal_mirror':\n                return grid.mirror('horizontal')\n            elif pattern.parameters['type'] == 'vertical_mirror':\n                return grid.mirror('vertical')\n        \n        elif pattern.pattern_type == PatternType.ROTATION:\n            return grid.rotate(pattern.parameters['degrees'])\n        \n        elif pattern.pattern_type == PatternType.COLOR_MAPPING:\n            if 'color_map' in pattern.parameters:\n                return grid.map_colors(pattern.parameters['color_map'])\n            elif pattern.parameters.get('type') == 'inversion':\n                return self._invert_colors(grid)\n        \n        elif pattern.pattern_type == PatternType.REPETITION:\n            direction = pattern.parameters['direction']\n            if direction == 'horizontal':\n                return grid.tile(1, 3)  # Repeat horizontally\n            else:\n                return grid.tile(3, 1)  # Repeat vertically\n        \n        elif pattern.pattern_type == PatternType.MIRRORING:\n            return grid.mirror(pattern.parameters['axis'])\n        \n        elif pattern.pattern_type == PatternType.EXTRACTION:\n            if pattern.parameters.get('type') == 'largest_component':\n                return grid.extract_largest_component()\n            elif pattern.parameters.get('type') == 'smallest_component':\n                return grid.extract_smallest_component()\n            else:\n                # Extract subgrid\n                return grid.extract_subgrid(\n                    pattern.parameters['y'],\n                    pattern.parameters['x'],\n                    pattern.parameters['height'],\n                    pattern.parameters['width']\n                )\n        \n        elif pattern.pattern_type == PatternType.FILLING:\n            if pattern.parameters.get('type') == 'flood_fill':\n                # Flood fill background\n                result = grid.copy()\n                result.data[result.data == 0] = pattern.parameters['fill_color']\n                return result\n            else:\n                # Fill background\n                result = grid.copy()\n                result.data[result.data == 0] = pattern.parameters['fill_color']\n                return result\n        \n        elif pattern.pattern_type == PatternType.CONNECTIVITY:\n            # Apply component transformations\n            return self._apply_component_transforms(grid, pattern.parameters)\n        \n        return grid\n    \n    def _validate_pattern(self,\n                        pattern: DetectedPattern,\n                        inputs: List[Grid],\n                        outputs: List[Grid]) -> bool:\n        \"\"\"Validate pattern on training examples\"\"\"\n        \n        correct = 0\n        total = len(inputs)\n        \n        for inp, expected_out in zip(inputs, outputs):\n            try:\n                result = self._apply_pattern(inp, pattern)\n                if result == expected_out:\n                    correct += 1\n            except Exception:\n                continue\n        \n        # Pattern should work on most examples\n        return correct / total >= 0.8\n    \n    def _invert_colors(self, grid: Grid) -> Grid:\n        \"\"\"Invert non-background colors\"\"\"\n        colors = sorted(grid.unique_colors - {0})\n        if not colors:\n            return grid\n        \n        color_map = {}\n        for i, color in enumerate(colors):\n            color_map[color] = colors[-(i+1)]\n        \n        return grid.map_colors(color_map)\n    \n    def _component_matches_grid(self, \n                               component: Dict[str, Any],\n                               grid: Grid) -> bool:\n        \"\"\"Check if a component matches a grid\"\"\"\n        # Get bounding box of component\n        pixels = component['pixels']\n        if not pixels:\n            return False\n        \n        ys = [p[0] for p in pixels]\n        xs = [p[1] for p in pixels]\n        \n        min_y, max_y = min(ys), max(ys)\n        min_x, max_x = min(xs), max(xs)\n        \n        # Check if grid size matches bounding box\n        if grid.height != (max_y - min_y + 1) or grid.width != (max_x - min_x + 1):\n            return False\n        \n        # Check if pixels match\n        for y, x in pixels:\n            grid_y = y - min_y\n            grid_x = x - min_x\n            if grid.data[grid_y, grid_x] != component['color']:\n                return False\n        \n        return True\n    \n    def _components_match(self,\n                        comp1: Dict[str, Any],\n                        comp2: Dict[str, Any]) -> bool:\n        \"\"\"Check if two components match (same shape/structure)\"\"\"\n        if len(comp1['pixels']) != len(comp2['pixels']):\n            return False\n        \n        # Normalize positions\n        pixels1 = comp1['pixels']\n        pixels2 = comp2['pixels']\n        \n        if not pixels1 or not pixels2:\n            return False\n        \n        # Get relative positions\n        min_y1 = min(p[0] for p in pixels1)\n        min_x1 = min(p[1] for p in pixels1)\n        \n        min_y2 = min(p[0] for p in pixels2)\n        min_x2 = min(p[1] for p in pixels2)\n        \n        norm_pixels1 = set((y - min_y1, x - min_x1) for y, x in pixels1)\n        norm_pixels2 = set((y - min_y2, x - min_x2) for y, x in pixels2)\n        \n        return norm_pixels1 == norm_pixels2\n    \n    def _apply_component_transforms(self,\n                                   grid: Grid,\n                                   parameters: Dict) -> Grid:\n        \"\"\"Apply transformations to components\"\"\"\n        # This would apply component-specific transformations\n        # For now, return the grid as-is\n        return grid","size_bytes":27251},"src/solvers/program_synthesis.py":{"content":"\"\"\"\nProgram Synthesis Engine for ARC Prize 2025\nImplements program synthesis with DSL, beam search, and program optimization\n\"\"\"\n\nimport numpy as np\nfrom typing import List, Dict, Optional, Tuple, Any, Callable, Set\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nimport heapq\nfrom itertools import product, combinations\nimport time\nimport copy\n\nfrom ..arc.grid_operations import Grid\nfrom ..arc.pattern_detector import PatternDetector\nfrom ..arc.transformation_rules import TransformationType, TransformationRule\n\n\nclass DSLOperation(Enum):\n    \"\"\"Domain-Specific Language operations for grid transformations\"\"\"\n    # Geometric operations\n    ROTATE_90 = \"rotate_90\"\n    ROTATE_180 = \"rotate_180\"\n    ROTATE_270 = \"rotate_270\"\n    FLIP_H = \"flip_h\"\n    FLIP_V = \"flip_v\"\n    TRANSPOSE = \"transpose\"\n    \n    # Scaling operations\n    SCALE_2X = \"scale_2x\"\n    SCALE_3X = \"scale_3x\"\n    DOWNSCALE_2X = \"downscale_2x\"\n    \n    # Tiling operations\n    TILE_2X2 = \"tile_2x2\"\n    TILE_3X3 = \"tile_3x3\"\n    MIRROR_H = \"mirror_h\"\n    MIRROR_V = \"mirror_v\"\n    \n    # Color operations\n    REPLACE_COLOR = \"replace_color\"\n    FILTER_COLOR = \"filter_color\"\n    INVERT_COLORS = \"invert_colors\"\n    MAP_COLORS = \"map_colors\"\n    \n    # Pattern operations\n    EXTRACT_PATTERN = \"extract_pattern\"\n    REPEAT_PATTERN = \"repeat_pattern\"\n    OVERLAY = \"overlay\"\n    MASK = \"mask\"\n    \n    # Structural operations\n    CROP_TO_CONTENT = \"crop_to_content\"\n    PAD = \"pad\"\n    EXTRACT_SUBGRID = \"extract_subgrid\"\n    FILL_BACKGROUND = \"fill_background\"\n    \n    # Connected components\n    EXTRACT_LARGEST = \"extract_largest\"\n    EXTRACT_SMALLEST = \"extract_smallest\"\n    EXTRACT_BY_COLOR = \"extract_by_color\"\n    \n    # Conditional operations\n    IF_SYMMETRIC = \"if_symmetric\"\n    IF_CONTAINS_COLOR = \"if_contains_color\"\n    IF_SIZE = \"if_size\"\n    \n    # Composite operations\n    COMPOSE = \"compose\"\n    LOOP = \"loop\"\n    APPLY_TO_EACH = \"apply_to_each\"\n\n\n@dataclass\nclass DSLInstruction:\n    \"\"\"Single instruction in the DSL program\"\"\"\n    operation: DSLOperation\n    parameters: Dict[str, Any] = field(default_factory=dict)\n    \n    def __hash__(self):\n        param_tuple = tuple(sorted(self.parameters.items()))\n        return hash((self.operation, param_tuple))\n\n\n@dataclass\nclass Program:\n    \"\"\"Represents a transformation program\"\"\"\n    instructions: List[DSLInstruction]\n    score: float = 0.0\n    complexity: int = 0\n    metadata: Dict[str, Any] = field(default_factory=dict)\n    \n    def __post_init__(self):\n        self.complexity = len(self.instructions)\n        for inst in self.instructions:\n            # Add complexity for parameterized operations\n            if inst.parameters:\n                self.complexity += len(inst.parameters)\n    \n    def execute(self, grid: Grid) -> Grid:\n        \"\"\"Execute the program on a grid\"\"\"\n        result = grid.copy()\n        \n        for instruction in self.instructions:\n            try:\n                result = self._execute_instruction(result, instruction)\n            except Exception as e:\n                # Program execution failed\n                raise ProgramExecutionError(f\"Failed at {instruction.operation}: {str(e)}\")\n        \n        return result\n    \n    def _execute_instruction(self, grid: Grid, instruction: DSLInstruction) -> Grid:\n        \"\"\"Execute a single instruction\"\"\"\n        op = instruction.operation\n        params = instruction.parameters\n        \n        # Geometric operations\n        if op == DSLOperation.ROTATE_90:\n            return grid.rotate(90)\n        elif op == DSLOperation.ROTATE_180:\n            return grid.rotate(180)\n        elif op == DSLOperation.ROTATE_270:\n            return grid.rotate(270)\n        elif op == DSLOperation.FLIP_H:\n            return grid.flip_horizontal()\n        elif op == DSLOperation.FLIP_V:\n            return grid.flip_vertical()\n        elif op == DSLOperation.TRANSPOSE:\n            return grid.transpose()\n        \n        # Scaling operations\n        elif op == DSLOperation.SCALE_2X:\n            return grid.scale(2)\n        elif op == DSLOperation.SCALE_3X:\n            return grid.scale(3)\n        elif op == DSLOperation.DOWNSCALE_2X:\n            return self._downscale(grid, 2)\n        \n        # Tiling operations\n        elif op == DSLOperation.TILE_2X2:\n            return grid.tile(2, 2)\n        elif op == DSLOperation.TILE_3X3:\n            return grid.tile(3, 3)\n        elif op == DSLOperation.MIRROR_H:\n            return grid.mirror('horizontal')\n        elif op == DSLOperation.MIRROR_V:\n            return grid.mirror('vertical')\n        \n        # Color operations\n        elif op == DSLOperation.REPLACE_COLOR:\n            return grid.replace_color(params['old_color'], params['new_color'])\n        elif op == DSLOperation.FILTER_COLOR:\n            return grid.filter_color(params['color'], params.get('background', 0))\n        elif op == DSLOperation.INVERT_COLORS:\n            return self._invert_colors(grid)\n        elif op == DSLOperation.MAP_COLORS:\n            return grid.map_colors(params['color_map'])\n        \n        # Pattern operations\n        elif op == DSLOperation.EXTRACT_PATTERN:\n            return self._extract_pattern(grid, params)\n        elif op == DSLOperation.REPEAT_PATTERN:\n            return self._repeat_pattern(grid, params)\n        elif op == DSLOperation.OVERLAY:\n            other = params['other']\n            return grid.overlay(other, params.get('x', 0), params.get('y', 0))\n        elif op == DSLOperation.MASK:\n            return grid.mask(params['mask'], params.get('mask_value', 0))\n        \n        # Structural operations\n        elif op == DSLOperation.CROP_TO_CONTENT:\n            return grid.crop_to_content(params.get('background', 0))\n        elif op == DSLOperation.PAD:\n            return grid.pad(params.get('padding', 1), params.get('value', 0))\n        elif op == DSLOperation.EXTRACT_SUBGRID:\n            return grid.extract_subgrid(\n                params['y'], params['x'], \n                params['height'], params['width']\n            )\n        elif op == DSLOperation.FILL_BACKGROUND:\n            return self._fill_background(grid, params.get('color', 0))\n        \n        # Connected components\n        elif op == DSLOperation.EXTRACT_LARGEST:\n            return grid.extract_largest_component(params.get('background', 0))\n        elif op == DSLOperation.EXTRACT_SMALLEST:\n            return grid.extract_smallest_component(params.get('background', 0))\n        elif op == DSLOperation.EXTRACT_BY_COLOR:\n            return grid.filter_color(params['color'])\n        \n        # Conditional operations\n        elif op == DSLOperation.IF_SYMMETRIC:\n            if self._check_symmetry(grid, params.get('type', 'horizontal')):\n                return self._execute_instruction(grid, params['then'])\n            elif 'else' in params:\n                return self._execute_instruction(grid, params['else'])\n            return grid\n        \n        # Composite operations\n        elif op == DSLOperation.COMPOSE:\n            result = grid\n            for sub_inst in params['instructions']:\n                result = self._execute_instruction(result, sub_inst)\n            return result\n        \n        elif op == DSLOperation.LOOP:\n            result = grid\n            for _ in range(params.get('times', 1)):\n                result = self._execute_instruction(result, params['instruction'])\n            return result\n        \n        else:\n            raise ValueError(f\"Unknown operation: {op}\")\n    \n    def _downscale(self, grid: Grid, factor: int) -> Grid:\n        \"\"\"Downscale grid by taking every nth pixel\"\"\"\n        data = grid.data[::factor, ::factor]\n        return Grid(data)\n    \n    def _invert_colors(self, grid: Grid) -> Grid:\n        \"\"\"Invert colors (swap non-zero colors)\"\"\"\n        color_map = {}\n        unique_colors = sorted(grid.unique_colors)\n        if 0 in unique_colors:\n            unique_colors.remove(0)\n        \n        # Create inversion mapping\n        for i, color in enumerate(unique_colors):\n            color_map[color] = unique_colors[-(i+1)]\n        \n        return grid.map_colors(color_map)\n    \n    def _extract_pattern(self, grid: Grid, params: Dict) -> Grid:\n        \"\"\"Extract a repeating pattern from the grid\"\"\"\n        detector = PatternDetector(grid)\n        patterns = detector.find_repeating_patterns(\n            min_size=params.get('min_size', 2),\n            max_size=params.get('max_size', None)\n        )\n        \n        if patterns:\n            return patterns[0][0]  # Return most frequent pattern\n        return grid\n    \n    def _repeat_pattern(self, grid: Grid, params: Dict) -> Grid:\n        \"\"\"Repeat a pattern to fill a larger grid\"\"\"\n        rows = params.get('rows', 2)\n        cols = params.get('cols', 2)\n        return grid.tile(rows, cols)\n    \n    def _fill_background(self, grid: Grid, color: int) -> Grid:\n        \"\"\"Fill background (0) with specified color\"\"\"\n        result = grid.copy()\n        result.data[result.data == 0] = color\n        return result\n    \n    def _check_symmetry(self, grid: Grid, sym_type: str) -> bool:\n        \"\"\"Check if grid has specified symmetry\"\"\"\n        detector = PatternDetector(grid)\n        if sym_type == 'horizontal':\n            return detector.has_horizontal_symmetry()\n        elif sym_type == 'vertical':\n            return detector.has_vertical_symmetry()\n        elif sym_type == 'diagonal':\n            return detector.has_diagonal_symmetry()\n        return False\n    \n    def to_string(self) -> str:\n        \"\"\"Convert program to string representation\"\"\"\n        lines = []\n        for inst in self.instructions:\n            if inst.parameters:\n                param_str = ', '.join(f\"{k}={v}\" for k, v in inst.parameters.items())\n                lines.append(f\"{inst.operation.value}({param_str})\")\n            else:\n                lines.append(inst.operation.value)\n        return '\\n'.join(lines)\n\n\nclass ProgramExecutionError(Exception):\n    \"\"\"Raised when program execution fails\"\"\"\n    pass\n\n\nclass ProgramGenerator:\n    \"\"\"Generates candidate programs for solving ARC tasks\"\"\"\n    \n    def __init__(self, max_length: int = 5, allow_composite: bool = True):\n        self.max_length = max_length\n        self.allow_composite = allow_composite\n        self.operation_templates = self._create_operation_templates()\n    \n    def _create_operation_templates(self) -> Dict[DSLOperation, List[Dict]]:\n        \"\"\"Create templates for parameterized operations\"\"\"\n        templates = {\n            # Simple geometric operations (no parameters)\n            DSLOperation.ROTATE_90: [{}],\n            DSLOperation.ROTATE_180: [{}],\n            DSLOperation.ROTATE_270: [{}],\n            DSLOperation.FLIP_H: [{}],\n            DSLOperation.FLIP_V: [{}],\n            DSLOperation.TRANSPOSE: [{}],\n            \n            # Scaling operations\n            DSLOperation.SCALE_2X: [{}],\n            DSLOperation.SCALE_3X: [{}],\n            DSLOperation.DOWNSCALE_2X: [{}],\n            \n            # Tiling operations\n            DSLOperation.TILE_2X2: [{}],\n            DSLOperation.TILE_3X3: [{}],\n            DSLOperation.MIRROR_H: [{}],\n            DSLOperation.MIRROR_V: [{}],\n            \n            # Color operations (need parameters)\n            DSLOperation.REPLACE_COLOR: [\n                {'old_color': old, 'new_color': new}\n                for old in range(10) for new in range(10) if old != new\n            ][:20],  # Limit to avoid explosion\n            \n            DSLOperation.FILTER_COLOR: [\n                {'color': c, 'background': 0} for c in range(1, 10)\n            ],\n            \n            DSLOperation.INVERT_COLORS: [{}],\n            \n            # Structural operations\n            DSLOperation.CROP_TO_CONTENT: [{'background': 0}],\n            DSLOperation.PAD: [{'padding': p, 'value': 0} for p in [1, 2]],\n            DSLOperation.FILL_BACKGROUND: [{'color': c} for c in range(1, 10)],\n            \n            # Component operations\n            DSLOperation.EXTRACT_LARGEST: [{'background': 0}],\n            DSLOperation.EXTRACT_SMALLEST: [{'background': 0}],\n        }\n        \n        return templates\n    \n    def generate_atomic_programs(self, input_grid: Grid, output_grid: Grid) -> List[Program]:\n        \"\"\"Generate single-instruction programs\"\"\"\n        programs = []\n        \n        # Analyze grids to guide generation\n        input_colors = input_grid.unique_colors\n        output_colors = output_grid.unique_colors\n        \n        for op, param_templates in self.operation_templates.items():\n            for params in param_templates:\n                # Filter color operations based on actual colors\n                if op in [DSLOperation.REPLACE_COLOR, DSLOperation.FILTER_COLOR]:\n                    if 'old_color' in params and params['old_color'] not in input_colors:\n                        continue\n                    if 'color' in params and params['color'] not in input_colors:\n                        continue\n                \n                instruction = DSLInstruction(op, params)\n                program = Program([instruction])\n                programs.append(program)\n        \n        return programs\n    \n    def generate_composite_programs(self, \n                                  atomic_programs: List[Program],\n                                  max_length: int = None) -> List[Program]:\n        \"\"\"Generate composite programs by combining atomic programs\"\"\"\n        if max_length is None:\n            max_length = self.max_length\n        \n        composite_programs = []\n        \n        # Generate programs of length 2\n        for prog1 in atomic_programs:\n            for prog2 in atomic_programs:\n                if len(prog1.instructions) + len(prog2.instructions) <= max_length:\n                    combined = Program(prog1.instructions + prog2.instructions)\n                    composite_programs.append(combined)\n        \n        # Generate programs of length 3+ (limited to avoid explosion)\n        if max_length >= 3:\n            for prog in composite_programs[:100]:  # Limit for efficiency\n                for atomic in atomic_programs[:20]:\n                    if len(prog.instructions) + len(atomic.instructions) <= max_length:\n                        extended = Program(prog.instructions + atomic.instructions)\n                        composite_programs.append(extended)\n        \n        return composite_programs\n    \n    def generate_from_examples(self, \n                              train_inputs: List[Grid],\n                              train_outputs: List[Grid]) -> List[Program]:\n        \"\"\"Generate programs based on training examples\"\"\"\n        all_programs = []\n        \n        # Generate for each training pair\n        for input_grid, output_grid in zip(train_inputs, train_outputs):\n            # Analyze transformation characteristics\n            size_changed = input_grid.shape != output_grid.shape\n            colors_changed = input_grid.unique_colors != output_grid.unique_colors\n            \n            # Generate atomic programs\n            atomic = self.generate_atomic_programs(input_grid, output_grid)\n            all_programs.extend(atomic)\n            \n            # Generate composite programs if needed\n            if self.allow_composite:\n                composite = self.generate_composite_programs(atomic, self.max_length)\n                all_programs.extend(composite)\n        \n        # Remove duplicates\n        unique_programs = []\n        seen = set()\n        for prog in all_programs:\n            prog_str = prog.to_string()\n            if prog_str not in seen:\n                seen.add(prog_str)\n                unique_programs.append(prog)\n        \n        return unique_programs\n\n\nclass ProgramEvaluator:\n    \"\"\"Evaluates programs on ARC examples\"\"\"\n    \n    def __init__(self, timeout: float = 1.0):\n        self.timeout = timeout\n    \n    def evaluate(self, \n                program: Program,\n                input_grids: List[Grid],\n                output_grids: List[Grid]) -> float:\n        \"\"\"Evaluate program on examples, return score\"\"\"\n        if not input_grids or not output_grids:\n            return 0.0\n        \n        total_score = 0.0\n        \n        for input_grid, expected_output in zip(input_grids, output_grids):\n            try:\n                # Execute program with timeout\n                result = self._execute_with_timeout(program, input_grid)\n                \n                # Calculate similarity score\n                score = self._calculate_similarity(result, expected_output)\n                total_score += score\n                \n            except (ProgramExecutionError, TimeoutError, Exception):\n                # Program failed on this example\n                total_score += 0.0\n        \n        # Average score across examples\n        avg_score = total_score / len(input_grids)\n        \n        # Apply complexity penalty\n        complexity_penalty = 1.0 / (1.0 + 0.1 * program.complexity)\n        \n        return avg_score * complexity_penalty\n    \n    def _execute_with_timeout(self, program: Program, grid: Grid) -> Grid:\n        \"\"\"Execute program with timeout (simplified version)\"\"\"\n        # In production, use proper timeout mechanism\n        start_time = time.time()\n        result = program.execute(grid)\n        \n        if time.time() - start_time > self.timeout:\n            raise TimeoutError(\"Program execution timeout\")\n        \n        return result\n    \n    def _calculate_similarity(self, predicted: Grid, expected: Grid) -> float:\n        \"\"\"Calculate similarity between predicted and expected grids\"\"\"\n        # Exact match\n        if predicted == expected:\n            return 1.0\n        \n        # Shape match bonus\n        shape_score = 1.0 if predicted.shape == expected.shape else 0.5\n        \n        # If shapes don't match, can't compare directly\n        if predicted.shape != expected.shape:\n            # Check if one is scaled version of other\n            if self._is_scaled_version(predicted, expected):\n                return 0.7 * shape_score\n            return 0.0\n        \n        # Pixel-wise accuracy\n        matches = np.sum(predicted.data == expected.data)\n        total = predicted.data.size\n        pixel_accuracy = matches / total\n        \n        # Color distribution similarity\n        pred_colors = predicted.count_colors()\n        exp_colors = expected.count_colors()\n        color_similarity = self._color_distribution_similarity(pred_colors, exp_colors)\n        \n        # Weighted combination\n        return shape_score * (0.7 * pixel_accuracy + 0.3 * color_similarity)\n    \n    def _is_scaled_version(self, grid1: Grid, grid2: Grid) -> bool:\n        \"\"\"Check if one grid is a scaled version of the other\"\"\"\n        h1, w1 = grid1.shape\n        h2, w2 = grid2.shape\n        \n        # Check for integer scaling\n        if h1 % h2 == 0 and w1 % w2 == 0:\n            scale = h1 // h2\n            if w1 // w2 == scale:\n                # Check if grid1 is scaled version of grid2\n                scaled = grid2.scale(scale)\n                return scaled == grid1\n        \n        if h2 % h1 == 0 and w2 % w1 == 0:\n            scale = h2 // h1\n            if w2 // w1 == scale:\n                # Check if grid2 is scaled version of grid1\n                scaled = grid1.scale(scale)\n                return scaled == grid2\n        \n        return False\n    \n    def _color_distribution_similarity(self, \n                                     dist1: Dict[int, int],\n                                     dist2: Dict[int, int]) -> float:\n        \"\"\"Calculate similarity between color distributions\"\"\"\n        all_colors = set(dist1.keys()) | set(dist2.keys())\n        \n        if not all_colors:\n            return 1.0\n        \n        total_diff = 0\n        total_count = sum(dist1.values()) + sum(dist2.values())\n        \n        for color in all_colors:\n            count1 = dist1.get(color, 0)\n            count2 = dist2.get(color, 0)\n            total_diff += abs(count1 - count2)\n        \n        if total_count == 0:\n            return 1.0\n        \n        return 1.0 - (total_diff / total_count)\n\n\nclass BeamSearchSynthesizer:\n    \"\"\"Beam search for program synthesis\"\"\"\n    \n    def __init__(self, \n                 beam_width: int = 10,\n                 max_iterations: int = 100,\n                 generator: Optional[ProgramGenerator] = None,\n                 evaluator: Optional[ProgramEvaluator] = None):\n        self.beam_width = beam_width\n        self.max_iterations = max_iterations\n        self.generator = generator or ProgramGenerator()\n        self.evaluator = evaluator or ProgramEvaluator()\n    \n    def synthesize(self,\n                  train_inputs: List[Grid],\n                  train_outputs: List[Grid],\n                  test_input: Optional[Grid] = None) -> Optional[Program]:\n        \"\"\"Synthesize program using beam search\"\"\"\n        \n        # Generate initial candidate programs\n        candidates = self.generator.generate_from_examples(train_inputs, train_outputs)\n        \n        if not candidates:\n            return None\n        \n        # Evaluate and score candidates\n        scored_programs = []\n        for program in candidates:\n            score = self.evaluator.evaluate(program, train_inputs, train_outputs)\n            program.score = score\n            scored_programs.append(program)\n        \n        # Sort by score and keep top beam_width\n        scored_programs.sort(key=lambda p: p.score, reverse=True)\n        beam = scored_programs[:self.beam_width]\n        \n        # Check if we found perfect solution\n        for program in beam:\n            if program.score >= 0.99:\n                return program\n        \n        # Beam search iterations\n        for iteration in range(self.max_iterations):\n            new_beam = []\n            \n            for program in beam:\n                # Generate variations of this program\n                variations = self._generate_variations(program, train_inputs[0], train_outputs[0])\n                \n                for variant in variations:\n                    score = self.evaluator.evaluate(variant, train_inputs, train_outputs)\n                    variant.score = score\n                    new_beam.append(variant)\n            \n            # Combine old beam and new candidates\n            all_candidates = beam + new_beam\n            all_candidates.sort(key=lambda p: p.score, reverse=True)\n            \n            # Keep top beam_width\n            beam = all_candidates[:self.beam_width]\n            \n            # Check for perfect solution\n            if beam[0].score >= 0.99:\n                return beam[0]\n            \n            # Check for convergence\n            if iteration > 0 and len(new_beam) == 0:\n                break\n        \n        # Return best program found\n        return beam[0] if beam else None\n    \n    def _generate_variations(self, \n                           program: Program,\n                           input_grid: Grid,\n                           output_grid: Grid) -> List[Program]:\n        \"\"\"Generate variations of a program\"\"\"\n        variations = []\n        \n        # Add single instruction\n        atomic = self.generator.generate_atomic_programs(input_grid, output_grid)\n        for atomic_prog in atomic[:10]:  # Limit for efficiency\n            if len(program.instructions) + 1 <= self.generator.max_length:\n                # Prepend\n                new_prog = Program(atomic_prog.instructions + program.instructions)\n                variations.append(new_prog)\n                \n                # Append\n                new_prog = Program(program.instructions + atomic_prog.instructions)\n                variations.append(new_prog)\n        \n        # Remove instruction\n        if len(program.instructions) > 1:\n            for i in range(len(program.instructions)):\n                new_instructions = (program.instructions[:i] + \n                                  program.instructions[i+1:])\n                variations.append(Program(new_instructions))\n        \n        # Modify parameters\n        for i, instruction in enumerate(program.instructions):\n            if instruction.parameters:\n                # Try different parameter values\n                param_variations = self._generate_parameter_variations(instruction)\n                for variant_inst in param_variations[:5]:  # Limit\n                    new_instructions = program.instructions.copy()\n                    new_instructions[i] = variant_inst\n                    variations.append(Program(new_instructions))\n        \n        return variations\n    \n    def _generate_parameter_variations(self, \n                                      instruction: DSLInstruction) -> List[DSLInstruction]:\n        \"\"\"Generate variations of instruction parameters\"\"\"\n        variations = []\n        \n        if instruction.operation == DSLOperation.REPLACE_COLOR:\n            # Try different color mappings\n            for old_color in range(10):\n                for new_color in range(10):\n                    if old_color != new_color:\n                        params = {'old_color': old_color, 'new_color': new_color}\n                        variations.append(DSLInstruction(instruction.operation, params))\n        \n        elif instruction.operation == DSLOperation.PAD:\n            # Try different padding sizes\n            for padding in [1, 2, 3]:\n                for value in [0, 1]:\n                    params = {'padding': padding, 'value': value}\n                    variations.append(DSLInstruction(instruction.operation, params))\n        \n        # Add more parameter variations for other operations...\n        \n        return variations\n\n\nclass ProgramSynthesisEngine:\n    \"\"\"Main engine for program synthesis-based solving\"\"\"\n    \n    def __init__(self, config: Optional[Dict[str, Any]] = None):\n        config = config or {}\n        \n        self.generator = ProgramGenerator(\n            max_length=config.get('max_program_length', 5),\n            allow_composite=config.get('allow_composite', True)\n        )\n        \n        self.evaluator = ProgramEvaluator(\n            timeout=config.get('execution_timeout', 1.0)\n        )\n        \n        self.synthesizer = BeamSearchSynthesizer(\n            beam_width=config.get('beam_width', 10),\n            max_iterations=config.get('max_iterations', 100),\n            generator=self.generator,\n            evaluator=self.evaluator\n        )\n    \n    def solve(self, \n             train_inputs: List[np.ndarray],\n             train_outputs: List[np.ndarray],\n             test_input: np.ndarray) -> Optional[np.ndarray]:\n        \"\"\"Solve ARC task using program synthesis\"\"\"\n        \n        # Convert to Grid objects\n        train_input_grids = [Grid(inp) for inp in train_inputs]\n        train_output_grids = [Grid(out) for out in train_outputs]\n        test_input_grid = Grid(test_input)\n        \n        # Synthesize program\n        program = self.synthesizer.synthesize(\n            train_input_grids,\n            train_output_grids,\n            test_input_grid\n        )\n        \n        if program is None:\n            return None\n        \n        try:\n            # Apply program to test input\n            result_grid = program.execute(test_input_grid)\n            return result_grid.data\n        except Exception:\n            return None\n    \n    def get_program(self,\n                   train_inputs: List[np.ndarray],\n                   train_outputs: List[np.ndarray]) -> Optional[Program]:\n        \"\"\"Get the synthesized program without applying to test\"\"\"\n        \n        # Convert to Grid objects\n        train_input_grids = [Grid(inp) for inp in train_inputs]\n        train_output_grids = [Grid(out) for out in train_outputs]\n        \n        # Synthesize program\n        return self.synthesizer.synthesize(\n            train_input_grids,\n            train_output_grids\n        )","size_bytes":27875},"src/strategies/strategy_selector.py":{"content":"\"\"\"\nStrategy Selection Module for ARC Prize 2025\nAnalyzes tasks and selects appropriate solving strategies\n\"\"\"\n\nimport numpy as np\nfrom typing import List, Dict, Optional, Tuple, Set, Any\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nimport statistics\n\nfrom ..arc.grid_operations import Grid\nfrom ..arc.pattern_detector import PatternDetector\n\n\nclass TaskCategory(Enum):\n    \"\"\"Categories of ARC tasks\"\"\"\n    GEOMETRIC = \"geometric\"\n    COLOR_MANIPULATION = \"color_manipulation\"\n    PATTERN_COMPLETION = \"pattern_completion\"\n    COUNTING = \"counting\"\n    SYMMETRY = \"symmetry\"\n    OBJECT_MANIPULATION = \"object_manipulation\"\n    LOGICAL = \"logical\"\n    SPATIAL_REASONING = \"spatial_reasoning\"\n    UNKNOWN = \"unknown\"\n\n\nclass ComplexityLevel(Enum):\n    \"\"\"Task complexity levels\"\"\"\n    TRIVIAL = 1\n    SIMPLE = 2\n    MODERATE = 3\n    COMPLEX = 4\n    VERY_COMPLEX = 5\n\n\n@dataclass\nclass TaskFeatures:\n    \"\"\"Features extracted from ARC task\"\"\"\n    # Size features\n    input_sizes: List[Tuple[int, int]]\n    output_sizes: List[Tuple[int, int]]\n    size_change_ratio: float\n    consistent_size_change: bool\n    \n    # Color features\n    input_color_counts: List[Dict[int, int]]\n    output_color_counts: List[Dict[int, int]]\n    unique_colors_input: Set[int]\n    unique_colors_output: Set[int]\n    color_mapping_exists: bool\n    color_reduction: bool\n    \n    # Pattern features\n    has_repetition: bool\n    has_symmetry: bool\n    symmetry_types: List[str]\n    has_periodicity: bool\n    pattern_count: int\n    \n    # Structural features\n    has_connected_components: bool\n    component_count: int\n    has_grid_structure: bool\n    has_boundaries: bool\n    sparsity: float\n    \n    # Transformation features\n    is_rotation: bool\n    is_reflection: bool\n    is_scaling: bool\n    is_translation: bool\n    is_color_swap: bool\n    \n    # Complexity metrics\n    pixel_accuracy_variance: float\n    transformation_consistency: float\n    rule_complexity: int\n    \n    # Additional metadata\n    num_examples: int\n    avg_input_size: float\n    avg_output_size: float\n\n\n@dataclass\nclass StrategyRecommendation:\n    \"\"\"Recommended solving strategy\"\"\"\n    primary_strategy: str\n    alternative_strategies: List[str]\n    confidence: float\n    reasoning: str\n    expected_difficulty: ComplexityLevel\n    suggested_timeout: float\n\n\nclass FeatureExtractor:\n    \"\"\"Extracts features from ARC tasks\"\"\"\n    \n    def extract_features(self,\n                        train_inputs: List[np.ndarray],\n                        train_outputs: List[np.ndarray]) -> TaskFeatures:\n        \"\"\"Extract comprehensive features from training examples\"\"\"\n        \n        # Convert to Grid objects\n        input_grids = [Grid(inp) for inp in train_inputs]\n        output_grids = [Grid(out) for out in train_outputs]\n        \n        # Size features\n        input_sizes = [g.shape for g in input_grids]\n        output_sizes = [g.shape for g in output_grids]\n        size_changes = self._analyze_size_changes(input_sizes, output_sizes)\n        \n        # Color features\n        color_features = self._analyze_colors(input_grids, output_grids)\n        \n        # Pattern features\n        pattern_features = self._analyze_patterns(input_grids, output_grids)\n        \n        # Structural features\n        structural_features = self._analyze_structure(input_grids, output_grids)\n        \n        # Transformation features\n        transformation_features = self._analyze_transformations(input_grids, output_grids)\n        \n        # Complexity metrics\n        complexity_metrics = self._analyze_complexity(input_grids, output_grids)\n        \n        return TaskFeatures(\n            # Size features\n            input_sizes=input_sizes,\n            output_sizes=output_sizes,\n            size_change_ratio=size_changes['ratio'],\n            consistent_size_change=size_changes['consistent'],\n            \n            # Color features\n            input_color_counts=color_features['input_counts'],\n            output_color_counts=color_features['output_counts'],\n            unique_colors_input=color_features['unique_input'],\n            unique_colors_output=color_features['unique_output'],\n            color_mapping_exists=color_features['mapping_exists'],\n            color_reduction=color_features['reduction'],\n            \n            # Pattern features\n            has_repetition=pattern_features['repetition'],\n            has_symmetry=pattern_features['symmetry'],\n            symmetry_types=pattern_features['symmetry_types'],\n            has_periodicity=pattern_features['periodicity'],\n            pattern_count=pattern_features['pattern_count'],\n            \n            # Structural features\n            has_connected_components=structural_features['has_components'],\n            component_count=structural_features['component_count'],\n            has_grid_structure=structural_features['grid_structure'],\n            has_boundaries=structural_features['boundaries'],\n            sparsity=structural_features['sparsity'],\n            \n            # Transformation features\n            is_rotation=transformation_features['rotation'],\n            is_reflection=transformation_features['reflection'],\n            is_scaling=transformation_features['scaling'],\n            is_translation=transformation_features['translation'],\n            is_color_swap=transformation_features['color_swap'],\n            \n            # Complexity metrics\n            pixel_accuracy_variance=complexity_metrics['accuracy_variance'],\n            transformation_consistency=complexity_metrics['consistency'],\n            rule_complexity=complexity_metrics['rule_complexity'],\n            \n            # Metadata\n            num_examples=len(train_inputs),\n            avg_input_size=np.mean([g.data.size for g in input_grids]),\n            avg_output_size=np.mean([g.data.size for g in output_grids])\n        )\n    \n    def _analyze_size_changes(self,\n                            input_sizes: List[Tuple[int, int]],\n                            output_sizes: List[Tuple[int, int]]) -> Dict[str, Any]:\n        \"\"\"Analyze size changes between input and output\"\"\"\n        \n        ratios = []\n        for (ih, iw), (oh, ow) in zip(input_sizes, output_sizes):\n            if ih > 0 and iw > 0:\n                h_ratio = oh / ih\n                w_ratio = ow / iw\n                ratios.append((h_ratio, w_ratio))\n        \n        # Check consistency\n        consistent = len(set(ratios)) == 1 if ratios else False\n        \n        # Average ratio\n        avg_ratio = 1.0\n        if ratios:\n            avg_h = np.mean([r[0] for r in ratios])\n            avg_w = np.mean([r[1] for r in ratios])\n            avg_ratio = (avg_h + avg_w) / 2\n        \n        return {\n            'ratio': avg_ratio,\n            'consistent': consistent,\n            'ratios': ratios\n        }\n    \n    def _analyze_colors(self,\n                       input_grids: List[Grid],\n                       output_grids: List[Grid]) -> Dict[str, Any]:\n        \"\"\"Analyze color usage and transformations\"\"\"\n        \n        input_counts = [g.count_colors() for g in input_grids]\n        output_counts = [g.count_colors() for g in output_grids]\n        \n        unique_input = set()\n        unique_output = set()\n        \n        for g in input_grids:\n            unique_input.update(g.unique_colors)\n        for g in output_grids:\n            unique_output.update(g.unique_colors)\n        \n        # Check for color mapping\n        mapping_exists = self._check_color_mapping(input_grids, output_grids)\n        \n        # Check for color reduction\n        reduction = len(unique_output) < len(unique_input)\n        \n        return {\n            'input_counts': input_counts,\n            'output_counts': output_counts,\n            'unique_input': unique_input,\n            'unique_output': unique_output,\n            'mapping_exists': mapping_exists,\n            'reduction': reduction\n        }\n    \n    def _check_color_mapping(self,\n                           input_grids: List[Grid],\n                           output_grids: List[Grid]) -> bool:\n        \"\"\"Check if there's a consistent color mapping\"\"\"\n        \n        if len(input_grids) != len(output_grids):\n            return False\n        \n        mappings = []\n        for inp, out in zip(input_grids, output_grids):\n            if inp.shape != out.shape:\n                continue\n            \n            mapping = {}\n            for color in inp.unique_colors:\n                positions = inp.get_color_positions(color)\n                if positions:\n                    out_colors = set()\n                    for y, x in positions:\n                        if y < out.height and x < out.width:\n                            out_colors.add(out.data[y, x])\n                    \n                    if len(out_colors) == 1:\n                        mapping[color] = out_colors.pop()\n            \n            if mapping:\n                mappings.append(mapping)\n        \n        # Check consistency\n        if mappings and all(m == mappings[0] for m in mappings):\n            return True\n        \n        return False\n    \n    def _analyze_patterns(self,\n                        input_grids: List[Grid],\n                        output_grids: List[Grid]) -> Dict[str, Any]:\n        \"\"\"Analyze pattern-related features\"\"\"\n        \n        has_repetition = False\n        has_symmetry = False\n        symmetry_types = set()\n        has_periodicity = False\n        total_patterns = 0\n        \n        for grid in output_grids:\n            detector = PatternDetector(grid)\n            \n            # Check repetition\n            patterns = detector.find_repeating_patterns()\n            if patterns:\n                has_repetition = True\n                total_patterns += len(patterns)\n            \n            # Check symmetry\n            symmetries = detector.get_symmetries()\n            if any(symmetries.values()):\n                has_symmetry = True\n                for sym_type, present in symmetries.items():\n                    if present:\n                        symmetry_types.add(sym_type)\n            \n            # Check periodicity\n            periodicity = detector.detect_periodicity()\n            if periodicity['horizontal'] or periodicity['vertical']:\n                has_periodicity = True\n        \n        return {\n            'repetition': has_repetition,\n            'symmetry': has_symmetry,\n            'symmetry_types': list(symmetry_types),\n            'periodicity': has_periodicity,\n            'pattern_count': total_patterns\n        }\n    \n    def _analyze_structure(self,\n                         input_grids: List[Grid],\n                         output_grids: List[Grid]) -> Dict[str, Any]:\n        \"\"\"Analyze structural features\"\"\"\n        \n        has_components = False\n        total_components = 0\n        has_grid = False\n        has_boundaries = False\n        sparsity_values = []\n        \n        for grid in output_grids:\n            # Check connected components\n            components = grid.find_connected_components()\n            if components:\n                has_components = True\n                total_components += len(components)\n            \n            # Check grid structure (regular spacing)\n            detector = PatternDetector(grid)\n            lines = detector.find_lines()\n            if lines:\n                # Check for regular grid pattern\n                h_lines = [l for l in lines if l['type'] == 'horizontal']\n                v_lines = [l for l in lines if l['type'] == 'vertical']\n                \n                if len(h_lines) > 2 and len(v_lines) > 2:\n                    has_grid = True\n            \n            # Check boundaries\n            edges = detector.find_edges()\n            if edges:\n                has_boundaries = True\n            \n            # Calculate sparsity\n            non_zero = np.count_nonzero(grid.data)\n            total = grid.data.size\n            sparsity = 1.0 - (non_zero / total) if total > 0 else 0.0\n            sparsity_values.append(sparsity)\n        \n        avg_sparsity = np.mean(sparsity_values) if sparsity_values else 0.0\n        \n        return {\n            'has_components': has_components,\n            'component_count': total_components,\n            'grid_structure': has_grid,\n            'boundaries': has_boundaries,\n            'sparsity': avg_sparsity\n        }\n    \n    def _analyze_transformations(self,\n                                input_grids: List[Grid],\n                                output_grids: List[Grid]) -> Dict[str, Any]:\n        \"\"\"Analyze transformation types\"\"\"\n        \n        is_rotation = False\n        is_reflection = False\n        is_scaling = False\n        is_translation = False\n        is_color_swap = False\n        \n        for inp, out in zip(input_grids, output_grids):\n            # Check rotation\n            for degrees in [90, 180, 270]:\n                if inp.rotate(degrees) == out:\n                    is_rotation = True\n                    break\n            \n            # Check reflection\n            if inp.flip_horizontal() == out or inp.flip_vertical() == out:\n                is_reflection = True\n            \n            # Check scaling\n            for factor in [2, 3]:\n                if inp.scale(factor) == out:\n                    is_scaling = True\n                    break\n            \n            # Check translation (simplified)\n            if inp.shape == out.shape:\n                # Check if pattern is shifted\n                inp_data = inp.data\n                out_data = out.data\n                \n                for dy in range(-2, 3):\n                    for dx in range(-2, 3):\n                        if dy == 0 and dx == 0:\n                            continue\n                        \n                        shifted = np.roll(inp_data, (dy, dx), axis=(0, 1))\n                        if np.array_equal(shifted, out_data):\n                            is_translation = True\n                            break\n            \n            # Check color swap\n            if inp.shape == out.shape:\n                inp_colors = inp.unique_colors\n                out_colors = out.unique_colors\n                \n                if inp_colors == out_colors and len(inp_colors) > 1:\n                    # Check if colors are systematically swapped\n                    color_map = {}\n                    for color in inp_colors:\n                        inp_positions = set(inp.get_color_positions(color))\n                        \n                        for out_color in out_colors:\n                            out_positions = set(out.get_color_positions(out_color))\n                            \n                            if inp_positions == out_positions:\n                                color_map[color] = out_color\n                                break\n                    \n                    if len(color_map) == len(inp_colors) and any(k != v for k, v in color_map.items()):\n                        is_color_swap = True\n        \n        return {\n            'rotation': is_rotation,\n            'reflection': is_reflection,\n            'scaling': is_scaling,\n            'translation': is_translation,\n            'color_swap': is_color_swap\n        }\n    \n    def _analyze_complexity(self,\n                          input_grids: List[Grid],\n                          output_grids: List[Grid]) -> Dict[str, Any]:\n        \"\"\"Analyze task complexity\"\"\"\n        \n        # Calculate pixel accuracy variance\n        accuracies = []\n        for inp, out in zip(input_grids, output_grids):\n            if inp.shape == out.shape:\n                matches = np.sum(inp.data == out.data)\n                total = inp.data.size\n                accuracy = matches / total if total > 0 else 0.0\n                accuracies.append(accuracy)\n        \n        accuracy_variance = statistics.variance(accuracies) if len(accuracies) > 1 else 0.0\n        \n        # Transformation consistency\n        consistency = 1.0\n        if len(output_grids) > 1:\n            # Check if outputs follow similar patterns\n            pattern_similarities = []\n            for i in range(len(output_grids) - 1):\n                for j in range(i + 1, len(output_grids)):\n                    sim = self._pattern_similarity(output_grids[i], output_grids[j])\n                    pattern_similarities.append(sim)\n            \n            if pattern_similarities:\n                consistency = np.mean(pattern_similarities)\n        \n        # Rule complexity (heuristic)\n        rule_complexity = 1\n        \n        # Increase complexity for various factors\n        if len(input_grids[0].unique_colors) > 3:\n            rule_complexity += 1\n        if input_grids[0].shape != output_grids[0].shape:\n            rule_complexity += 1\n        if not self._check_color_mapping(input_grids, output_grids):\n            rule_complexity += 1\n        if accuracy_variance > 0.2:\n            rule_complexity += 1\n        \n        return {\n            'accuracy_variance': accuracy_variance,\n            'consistency': consistency,\n            'rule_complexity': rule_complexity\n        }\n    \n    def _pattern_similarity(self, grid1: Grid, grid2: Grid) -> float:\n        \"\"\"Calculate pattern similarity between two grids\"\"\"\n        \n        detector1 = PatternDetector(grid1)\n        detector2 = PatternDetector(grid2)\n        \n        # Compare symmetries\n        sym1 = detector1.get_symmetries()\n        sym2 = detector2.get_symmetries()\n        \n        sym_matches = sum(1 for k in sym1 if sym1[k] == sym2.get(k, False))\n        sym_similarity = sym_matches / len(sym1) if sym1 else 0.0\n        \n        # Compare color distribution\n        colors1 = grid1.count_colors()\n        colors2 = grid2.count_colors()\n        \n        all_colors = set(colors1.keys()) | set(colors2.keys())\n        if all_colors:\n            color_diff = 0\n            for color in all_colors:\n                count1 = colors1.get(color, 0)\n                count2 = colors2.get(color, 0)\n                color_diff += abs(count1 - count2)\n            \n            total_pixels = grid1.data.size + grid2.data.size\n            color_similarity = 1.0 - (color_diff / total_pixels) if total_pixels > 0 else 0.0\n        else:\n            color_similarity = 1.0\n        \n        return (sym_similarity + color_similarity) / 2\n\n\nclass TaskClassifier:\n    \"\"\"Classifies ARC tasks into categories\"\"\"\n    \n    def classify(self, features: TaskFeatures) -> TaskCategory:\n        \"\"\"Classify task based on features\"\"\"\n        \n        scores = {\n            TaskCategory.GEOMETRIC: self._score_geometric(features),\n            TaskCategory.COLOR_MANIPULATION: self._score_color_manipulation(features),\n            TaskCategory.PATTERN_COMPLETION: self._score_pattern_completion(features),\n            TaskCategory.COUNTING: self._score_counting(features),\n            TaskCategory.SYMMETRY: self._score_symmetry(features),\n            TaskCategory.OBJECT_MANIPULATION: self._score_object_manipulation(features),\n            TaskCategory.LOGICAL: self._score_logical(features),\n            TaskCategory.SPATIAL_REASONING: self._score_spatial_reasoning(features)\n        }\n        \n        # Return category with highest score\n        best_category = max(scores, key=scores.get)\n        \n        if scores[best_category] < 0.3:\n            return TaskCategory.UNKNOWN\n        \n        return best_category\n    \n    def _score_geometric(self, features: TaskFeatures) -> float:\n        \"\"\"Score for geometric transformation tasks\"\"\"\n        score = 0.0\n        \n        if features.is_rotation:\n            score += 0.4\n        if features.is_reflection:\n            score += 0.4\n        if features.is_scaling:\n            score += 0.3\n        if features.is_translation:\n            score += 0.3\n        if features.consistent_size_change:\n            score += 0.2\n        \n        return min(1.0, score)\n    \n    def _score_color_manipulation(self, features: TaskFeatures) -> float:\n        \"\"\"Score for color manipulation tasks\"\"\"\n        score = 0.0\n        \n        if features.color_mapping_exists:\n            score += 0.5\n        if features.is_color_swap:\n            score += 0.4\n        if features.color_reduction:\n            score += 0.3\n        if len(features.unique_colors_output) != len(features.unique_colors_input):\n            score += 0.2\n        \n        return min(1.0, score)\n    \n    def _score_pattern_completion(self, features: TaskFeatures) -> float:\n        \"\"\"Score for pattern completion tasks\"\"\"\n        score = 0.0\n        \n        if features.has_repetition:\n            score += 0.4\n        if features.has_periodicity:\n            score += 0.4\n        if features.pattern_count > 2:\n            score += 0.3\n        if features.has_grid_structure:\n            score += 0.2\n        \n        return min(1.0, score)\n    \n    def _score_counting(self, features: TaskFeatures) -> float:\n        \"\"\"Score for counting tasks\"\"\"\n        score = 0.0\n        \n        # Check if output size correlates with component count\n        if features.component_count > 0:\n            score += 0.3\n        \n        # Check if output is much smaller than input (possible count encoding)\n        if features.avg_output_size < features.avg_input_size * 0.1:\n            score += 0.4\n        \n        return min(1.0, score)\n    \n    def _score_symmetry(self, features: TaskFeatures) -> float:\n        \"\"\"Score for symmetry-based tasks\"\"\"\n        score = 0.0\n        \n        if features.has_symmetry:\n            score += 0.5\n        if len(features.symmetry_types) > 1:\n            score += 0.3\n        if features.is_reflection:\n            score += 0.3\n        \n        return min(1.0, score)\n    \n    def _score_object_manipulation(self, features: TaskFeatures) -> float:\n        \"\"\"Score for object manipulation tasks\"\"\"\n        score = 0.0\n        \n        if features.has_connected_components:\n            score += 0.4\n        if features.component_count > 1:\n            score += 0.3\n        if features.has_boundaries:\n            score += 0.2\n        \n        return min(1.0, score)\n    \n    def _score_logical(self, features: TaskFeatures) -> float:\n        \"\"\"Score for logical reasoning tasks\"\"\"\n        score = 0.0\n        \n        if features.rule_complexity > 3:\n            score += 0.4\n        if features.transformation_consistency < 0.7:\n            score += 0.3\n        \n        return min(1.0, score)\n    \n    def _score_spatial_reasoning(self, features: TaskFeatures) -> float:\n        \"\"\"Score for spatial reasoning tasks\"\"\"\n        score = 0.0\n        \n        if features.is_translation:\n            score += 0.3\n        if features.has_boundaries:\n            score += 0.2\n        if features.sparsity > 0.5:\n            score += 0.2\n        \n        return min(1.0, score)\n\n\nclass StrategySelector:\n    \"\"\"Selects optimal solving strategy for ARC tasks\"\"\"\n    \n    def __init__(self):\n        self.feature_extractor = FeatureExtractor()\n        self.task_classifier = TaskClassifier()\n    \n    def select_strategy(self,\n                       train_inputs: List[np.ndarray],\n                       train_outputs: List[np.ndarray]) -> StrategyRecommendation:\n        \"\"\"Select optimal strategy for the task\"\"\"\n        \n        # Extract features\n        features = self.feature_extractor.extract_features(train_inputs, train_outputs)\n        \n        # Classify task\n        category = self.task_classifier.classify(features)\n        \n        # Determine complexity\n        complexity = self._assess_complexity(features)\n        \n        # Select strategy based on category and features\n        strategy = self._determine_strategy(category, features, complexity)\n        \n        return strategy\n    \n    def _assess_complexity(self, features: TaskFeatures) -> ComplexityLevel:\n        \"\"\"Assess task complexity\"\"\"\n        \n        score = 0\n        \n        # Size complexity\n        if features.avg_output_size > 100:\n            score += 2\n        elif features.avg_output_size > 25:\n            score += 1\n        \n        # Color complexity\n        if len(features.unique_colors_output) > 5:\n            score += 1\n        \n        # Pattern complexity\n        if features.pattern_count > 5:\n            score += 1\n        \n        # Rule complexity\n        score += min(2, features.rule_complexity // 2)\n        \n        # Transformation complexity\n        if features.pixel_accuracy_variance > 0.3:\n            score += 1\n        \n        # Map score to complexity level\n        if score <= 1:\n            return ComplexityLevel.TRIVIAL\n        elif score <= 3:\n            return ComplexityLevel.SIMPLE\n        elif score <= 5:\n            return ComplexityLevel.MODERATE\n        elif score <= 7:\n            return ComplexityLevel.COMPLEX\n        else:\n            return ComplexityLevel.VERY_COMPLEX\n    \n    def _determine_strategy(self,\n                          category: TaskCategory,\n                          features: TaskFeatures,\n                          complexity: ComplexityLevel) -> StrategyRecommendation:\n        \"\"\"Determine solving strategy based on analysis\"\"\"\n        \n        # Strategy mappings for each category\n        strategy_map = {\n            TaskCategory.GEOMETRIC: {\n                'primary': 'pattern_solver',\n                'alternatives': ['program_synthesis'],\n                'reasoning': 'Geometric transformations detected'\n            },\n            TaskCategory.COLOR_MANIPULATION: {\n                'primary': 'csp_solver',\n                'alternatives': ['pattern_solver', 'program_synthesis'],\n                'reasoning': 'Color mapping patterns detected'\n            },\n            TaskCategory.PATTERN_COMPLETION: {\n                'primary': 'pattern_solver',\n                'alternatives': ['program_synthesis'],\n                'reasoning': 'Repeating patterns detected'\n            },\n            TaskCategory.COUNTING: {\n                'primary': 'program_synthesis',\n                'alternatives': ['csp_solver'],\n                'reasoning': 'Counting or enumeration task detected'\n            },\n            TaskCategory.SYMMETRY: {\n                'primary': 'pattern_solver',\n                'alternatives': ['program_synthesis'],\n                'reasoning': 'Symmetry-based transformations detected'\n            },\n            TaskCategory.OBJECT_MANIPULATION: {\n                'primary': 'program_synthesis',\n                'alternatives': ['pattern_solver', 'csp_solver'],\n                'reasoning': 'Object manipulation patterns detected'\n            },\n            TaskCategory.LOGICAL: {\n                'primary': 'csp_solver',\n                'alternatives': ['program_synthesis'],\n                'reasoning': 'Logical constraints detected'\n            },\n            TaskCategory.SPATIAL_REASONING: {\n                'primary': 'program_synthesis',\n                'alternatives': ['pattern_solver'],\n                'reasoning': 'Spatial reasoning required'\n            },\n            TaskCategory.UNKNOWN: {\n                'primary': 'ensemble',\n                'alternatives': ['program_synthesis', 'pattern_solver', 'csp_solver'],\n                'reasoning': 'Task category unclear, using ensemble approach'\n            }\n        }\n        \n        strategy_info = strategy_map.get(category, strategy_map[TaskCategory.UNKNOWN])\n        \n        # Adjust for complexity\n        if complexity in [ComplexityLevel.COMPLEX, ComplexityLevel.VERY_COMPLEX]:\n            # For complex tasks, prefer ensemble or program synthesis\n            if strategy_info['primary'] != 'ensemble':\n                strategy_info['alternatives'].insert(0, strategy_info['primary'])\n                strategy_info['primary'] = 'ensemble'\n                strategy_info['reasoning'] += ' (high complexity detected)'\n        \n        # Calculate confidence\n        confidence = self._calculate_confidence(category, features, complexity)\n        \n        # Determine timeout\n        timeout_map = {\n            ComplexityLevel.TRIVIAL: 5.0,\n            ComplexityLevel.SIMPLE: 10.0,\n            ComplexityLevel.MODERATE: 15.0,\n            ComplexityLevel.COMPLEX: 30.0,\n            ComplexityLevel.VERY_COMPLEX: 60.0\n        }\n        \n        suggested_timeout = timeout_map[complexity]\n        \n        return StrategyRecommendation(\n            primary_strategy=strategy_info['primary'],\n            alternative_strategies=strategy_info['alternatives'],\n            confidence=confidence,\n            reasoning=strategy_info['reasoning'],\n            expected_difficulty=complexity,\n            suggested_timeout=suggested_timeout\n        )\n    \n    def _calculate_confidence(self,\n                            category: TaskCategory,\n                            features: TaskFeatures,\n                            complexity: ComplexityLevel) -> float:\n        \"\"\"Calculate confidence in strategy selection\"\"\"\n        \n        base_confidence = 0.5\n        \n        # Adjust based on category clarity\n        if category != TaskCategory.UNKNOWN:\n            base_confidence += 0.2\n        \n        # Adjust based on feature consistency\n        if features.transformation_consistency > 0.8:\n            base_confidence += 0.15\n        \n        # Adjust based on complexity\n        if complexity in [ComplexityLevel.TRIVIAL, ComplexityLevel.SIMPLE]:\n            base_confidence += 0.1\n        elif complexity in [ComplexityLevel.COMPLEX, ComplexityLevel.VERY_COMPLEX]:\n            base_confidence -= 0.1\n        \n        # Adjust based on number of examples\n        if features.num_examples >= 3:\n            base_confidence += 0.1\n        \n        return min(1.0, max(0.0, base_confidence))","size_bytes":29670},"src/utils/__init__.py":{"content":"\"\"\"Utility functions and helpers\"\"\"","size_bytes":35},"src/utils/evaluator.py":{"content":"\"\"\"\nSolution Evaluator for ARC Prize 2025\nEvaluates and analyzes solutions for ARC tasks\n\"\"\"\n\nimport numpy as np\nfrom typing import List, Dict, Optional, Tuple, Any\nfrom dataclasses import dataclass, field\nimport time\nfrom collections import defaultdict\n\nfrom ..arc.grid_operations import Grid\nfrom ..arc.pattern_detector import PatternDetector\n\n\n@dataclass\nclass EvaluationMetrics:\n    \"\"\"Metrics for evaluating a solution\"\"\"\n    task_id: str\n    is_correct: bool\n    pixel_accuracy: float\n    structural_similarity: float\n    color_accuracy: float\n    shape_match: bool\n    execution_time: float\n    solver_used: str\n    confidence: float\n    error_analysis: Dict[str, Any] = field(default_factory=dict)\n\n\n@dataclass\nclass BatchEvaluation:\n    \"\"\"Evaluation results for multiple tasks\"\"\"\n    total_tasks: int\n    correct_tasks: int\n    accuracy: float\n    average_pixel_accuracy: float\n    average_time: float\n    solver_performance: Dict[str, Dict[str, float]]\n    difficulty_breakdown: Dict[str, Dict[str, float]]\n    error_patterns: Dict[str, int]\n\n\nclass SolutionEvaluator:\n    \"\"\"Evaluates ARC solutions against ground truth\"\"\"\n    \n    def __init__(self):\n        self.evaluation_history = []\n        self.error_patterns = defaultdict(int)\n    \n    def evaluate_solution(self,\n                         predicted: np.ndarray,\n                         ground_truth: np.ndarray,\n                         task_id: str = \"\",\n                         solver_used: str = \"\",\n                         execution_time: float = 0.0,\n                         confidence: float = 0.0) -> EvaluationMetrics:\n        \"\"\"\n        Evaluate a single solution against ground truth\n        \n        Args:\n            predicted: Predicted output grid\n            ground_truth: True output grid\n            task_id: Task identifier\n            solver_used: Name of solver used\n            execution_time: Time taken to solve\n            confidence: Solver confidence\n            \n        Returns:\n            EvaluationMetrics with detailed analysis\n        \"\"\"\n        \n        # Check if completely correct\n        is_correct = np.array_equal(predicted, ground_truth)\n        \n        # Calculate pixel accuracy\n        pixel_accuracy = self._calculate_pixel_accuracy(predicted, ground_truth)\n        \n        # Calculate structural similarity\n        structural_similarity = self._calculate_structural_similarity(\n            predicted, ground_truth\n        )\n        \n        # Calculate color accuracy\n        color_accuracy = self._calculate_color_accuracy(predicted, ground_truth)\n        \n        # Check shape match\n        shape_match = predicted.shape == ground_truth.shape\n        \n        # Perform error analysis\n        error_analysis = self._analyze_errors(predicted, ground_truth)\n        \n        # Create metrics\n        metrics = EvaluationMetrics(\n            task_id=task_id,\n            is_correct=is_correct,\n            pixel_accuracy=pixel_accuracy,\n            structural_similarity=structural_similarity,\n            color_accuracy=color_accuracy,\n            shape_match=shape_match,\n            execution_time=execution_time,\n            solver_used=solver_used,\n            confidence=confidence,\n            error_analysis=error_analysis\n        )\n        \n        # Track evaluation\n        self.evaluation_history.append(metrics)\n        \n        # Track error patterns\n        if not is_correct:\n            for error_type in error_analysis.get('error_types', []):\n                self.error_patterns[error_type] += 1\n        \n        return metrics\n    \n    def _calculate_pixel_accuracy(self,\n                                 predicted: np.ndarray,\n                                 ground_truth: np.ndarray) -> float:\n        \"\"\"Calculate pixel-level accuracy\"\"\"\n        \n        if predicted.shape != ground_truth.shape:\n            # Different shapes - calculate based on overlap\n            min_h = min(predicted.shape[0], ground_truth.shape[0])\n            min_w = min(predicted.shape[1], ground_truth.shape[1])\n            \n            if min_h == 0 or min_w == 0:\n                return 0.0\n            \n            # Compare overlapping region\n            pred_region = predicted[:min_h, :min_w]\n            truth_region = ground_truth[:min_h, :min_w]\n            \n            correct_pixels = np.sum(pred_region == truth_region)\n            total_pixels = ground_truth.size\n            \n            # Penalize for size mismatch\n            size_penalty = abs(predicted.size - ground_truth.size) / ground_truth.size\n            accuracy = (correct_pixels / total_pixels) * (1 - min(size_penalty, 0.5))\n            \n        else:\n            # Same shape - direct comparison\n            correct_pixels = np.sum(predicted == ground_truth)\n            total_pixels = ground_truth.size\n            accuracy = correct_pixels / total_pixels if total_pixels > 0 else 0.0\n        \n        return accuracy\n    \n    def _calculate_structural_similarity(self,\n                                        predicted: np.ndarray,\n                                        ground_truth: np.ndarray) -> float:\n        \"\"\"Calculate structural similarity between grids\"\"\"\n        \n        score = 0.0\n        num_checks = 0\n        \n        # Convert to Grid objects\n        pred_grid = Grid(predicted)\n        truth_grid = Grid(ground_truth)\n        \n        # Check pattern similarity\n        pred_detector = PatternDetector(pred_grid)\n        truth_detector = PatternDetector(truth_grid)\n        \n        # Compare symmetries\n        pred_symmetries = pred_detector.get_symmetries()\n        truth_symmetries = truth_detector.get_symmetries()\n        \n        for sym_type in pred_symmetries:\n            if pred_symmetries[sym_type] == truth_symmetries[sym_type]:\n                score += 1\n            num_checks += 1\n        \n        # Compare connected components count\n        pred_components = pred_grid.get_connected_components()\n        truth_components = truth_grid.get_connected_components()\n        \n        if len(pred_components) == len(truth_components):\n            score += 1\n        num_checks += 1\n        \n        # Compare color distribution similarity\n        pred_colors = pred_grid.get_color_counts()\n        truth_colors = truth_grid.get_color_counts()\n        \n        color_similarity = self._compare_color_distributions(pred_colors, truth_colors)\n        score += color_similarity\n        num_checks += 1\n        \n        # Calculate final score\n        similarity = score / num_checks if num_checks > 0 else 0.0\n        \n        return similarity\n    \n    def _calculate_color_accuracy(self,\n                                 predicted: np.ndarray,\n                                 ground_truth: np.ndarray) -> float:\n        \"\"\"Calculate color matching accuracy\"\"\"\n        \n        pred_colors = set(np.unique(predicted))\n        truth_colors = set(np.unique(ground_truth))\n        \n        # Check if same colors are present\n        if pred_colors == truth_colors:\n            return 1.0\n        \n        # Calculate Jaccard similarity\n        intersection = len(pred_colors & truth_colors)\n        union = len(pred_colors | truth_colors)\n        \n        return intersection / union if union > 0 else 0.0\n    \n    def _compare_color_distributions(self,\n                                    dist1: Dict[int, int],\n                                    dist2: Dict[int, int]) -> float:\n        \"\"\"Compare two color distributions\"\"\"\n        \n        all_colors = set(dist1.keys()) | set(dist2.keys())\n        \n        if not all_colors:\n            return 1.0\n        \n        total_diff = 0\n        total_count = sum(dist1.values()) + sum(dist2.values())\n        \n        for color in all_colors:\n            count1 = dist1.get(color, 0)\n            count2 = dist2.get(color, 0)\n            total_diff += abs(count1 - count2)\n        \n        similarity = 1 - (total_diff / total_count) if total_count > 0 else 0.0\n        \n        return max(0, similarity)\n    \n    def _analyze_errors(self,\n                       predicted: np.ndarray,\n                       ground_truth: np.ndarray) -> Dict[str, Any]:\n        \"\"\"Analyze errors in the prediction\"\"\"\n        \n        analysis = {\n            'error_types': [],\n            'shape_error': None,\n            'color_errors': {},\n            'pattern_errors': []\n        }\n        \n        # Shape analysis\n        if predicted.shape != ground_truth.shape:\n            analysis['error_types'].append('shape_mismatch')\n            analysis['shape_error'] = {\n                'predicted': predicted.shape,\n                'expected': ground_truth.shape,\n                'difference': (predicted.shape[0] - ground_truth.shape[0],\n                             predicted.shape[1] - ground_truth.shape[1])\n            }\n        \n        # Color analysis\n        pred_colors = set(np.unique(predicted))\n        truth_colors = set(np.unique(ground_truth))\n        \n        missing_colors = truth_colors - pred_colors\n        extra_colors = pred_colors - truth_colors\n        \n        if missing_colors:\n            analysis['error_types'].append('missing_colors')\n            analysis['color_errors']['missing'] = list(missing_colors)\n        \n        if extra_colors:\n            analysis['error_types'].append('extra_colors')\n            analysis['color_errors']['extra'] = list(extra_colors)\n        \n        # Pattern analysis (if same shape)\n        if predicted.shape == ground_truth.shape:\n            # Find regions with errors\n            error_mask = predicted != ground_truth\n            if np.any(error_mask):\n                analysis['error_types'].append('pixel_errors')\n                \n                # Identify error patterns\n                error_positions = np.argwhere(error_mask)\n                \n                # Check if errors are clustered\n                if len(error_positions) > 0:\n                    # Simple clustering check\n                    y_coords = error_positions[:, 0]\n                    x_coords = error_positions[:, 1]\n                    \n                    y_range = np.max(y_coords) - np.min(y_coords)\n                    x_range = np.max(x_coords) - np.min(x_coords)\n                    \n                    if y_range < predicted.shape[0] / 3 and x_range < predicted.shape[1] / 3:\n                        analysis['pattern_errors'].append('localized_errors')\n                    else:\n                        analysis['pattern_errors'].append('distributed_errors')\n        \n        return analysis\n    \n    def evaluate_batch(self,\n                      predictions: List[np.ndarray],\n                      ground_truths: List[np.ndarray],\n                      task_ids: Optional[List[str]] = None,\n                      metadata: Optional[List[Dict[str, Any]]] = None) -> BatchEvaluation:\n        \"\"\"\n        Evaluate multiple solutions\n        \n        Args:\n            predictions: List of predicted grids\n            ground_truths: List of ground truth grids\n            task_ids: Optional list of task IDs\n            metadata: Optional metadata for each solution\n            \n        Returns:\n            BatchEvaluation with aggregate metrics\n        \"\"\"\n        \n        if not task_ids:\n            task_ids = [f\"task_{i}\" for i in range(len(predictions))]\n        \n        if not metadata:\n            metadata = [{} for _ in range(len(predictions))]\n        \n        # Evaluate each solution\n        metrics_list = []\n        for pred, truth, task_id, meta in zip(predictions, ground_truths, task_ids, metadata):\n            metrics = self.evaluate_solution(\n                pred, truth, task_id,\n                solver_used=meta.get('solver_used', ''),\n                execution_time=meta.get('execution_time', 0.0),\n                confidence=meta.get('confidence', 0.0)\n            )\n            metrics_list.append(metrics)\n        \n        # Calculate aggregate metrics\n        correct_tasks = sum(1 for m in metrics_list if m.is_correct)\n        accuracy = correct_tasks / len(metrics_list) if metrics_list else 0.0\n        \n        avg_pixel_accuracy = np.mean([m.pixel_accuracy for m in metrics_list])\n        avg_time = np.mean([m.execution_time for m in metrics_list])\n        \n        # Solver performance breakdown\n        solver_performance = defaultdict(lambda: {'count': 0, 'correct': 0, 'accuracy': 0.0})\n        for m in metrics_list:\n            solver = m.solver_used\n            solver_performance[solver]['count'] += 1\n            if m.is_correct:\n                solver_performance[solver]['correct'] += 1\n        \n        for solver in solver_performance:\n            stats = solver_performance[solver]\n            stats['accuracy'] = stats['correct'] / stats['count'] if stats['count'] > 0 else 0.0\n        \n        # Create batch evaluation\n        batch_eval = BatchEvaluation(\n            total_tasks=len(metrics_list),\n            correct_tasks=correct_tasks,\n            accuracy=accuracy,\n            average_pixel_accuracy=avg_pixel_accuracy,\n            average_time=avg_time,\n            solver_performance=dict(solver_performance),\n            difficulty_breakdown={},  # Could be extended with difficulty analysis\n            error_patterns=dict(self.error_patterns)\n        )\n        \n        return batch_eval\n    \n    def generate_report(self, \n                       evaluation: EvaluationMetrics,\n                       verbose: bool = True) -> str:\n        \"\"\"\n        Generate a text report for an evaluation\n        \n        Args:\n            evaluation: Evaluation metrics\n            verbose: Include detailed error analysis\n            \n        Returns:\n            Formatted report string\n        \"\"\"\n        \n        report = []\n        report.append(f\"{'='*60}\")\n        report.append(f\"Task: {evaluation.task_id}\")\n        report.append(f\"{'='*60}\")\n        report.append(f\"Result: {'âœ“ CORRECT' if evaluation.is_correct else 'âœ— INCORRECT'}\")\n        report.append(f\"Solver: {evaluation.solver_used}\")\n        report.append(f\"Confidence: {evaluation.confidence:.2%}\")\n        report.append(f\"Execution Time: {evaluation.execution_time:.2f}s\")\n        report.append(\"\")\n        report.append(\"Metrics:\")\n        report.append(f\"  Pixel Accuracy: {evaluation.pixel_accuracy:.2%}\")\n        report.append(f\"  Structural Similarity: {evaluation.structural_similarity:.2%}\")\n        report.append(f\"  Color Accuracy: {evaluation.color_accuracy:.2%}\")\n        report.append(f\"  Shape Match: {'Yes' if evaluation.shape_match else 'No'}\")\n        \n        if verbose and evaluation.error_analysis:\n            report.append(\"\")\n            report.append(\"Error Analysis:\")\n            \n            if evaluation.error_analysis.get('error_types'):\n                report.append(f\"  Error Types: {', '.join(evaluation.error_analysis['error_types'])}\")\n            \n            if evaluation.error_analysis.get('shape_error'):\n                shape_err = evaluation.error_analysis['shape_error']\n                report.append(f\"  Shape: {shape_err['predicted']} vs {shape_err['expected']}\")\n            \n            if evaluation.error_analysis.get('color_errors'):\n                color_err = evaluation.error_analysis['color_errors']\n                if color_err.get('missing'):\n                    report.append(f\"  Missing Colors: {color_err['missing']}\")\n                if color_err.get('extra'):\n                    report.append(f\"  Extra Colors: {color_err['extra']}\")\n        \n        report.append(f\"{'='*60}\")\n        \n        return '\\n'.join(report)\n    \n    def get_summary_stats(self) -> Dict[str, Any]:\n        \"\"\"Get summary statistics for all evaluations\"\"\"\n        \n        if not self.evaluation_history:\n            return {}\n        \n        total = len(self.evaluation_history)\n        correct = sum(1 for e in self.evaluation_history if e.is_correct)\n        \n        return {\n            'total_evaluations': total,\n            'correct': correct,\n            'accuracy': correct / total if total > 0 else 0.0,\n            'average_pixel_accuracy': np.mean([e.pixel_accuracy for e in self.evaluation_history]),\n            'average_execution_time': np.mean([e.execution_time for e in self.evaluation_history]),\n            'error_patterns': dict(self.error_patterns)\n        }","size_bytes":16282},"test_arc_system.py":{"content":"#!/usr/bin/env python3\n\"\"\"\nSimple test script to verify ARC solver system functionality\nTests basic operations and checks for method name mismatches\n\"\"\"\n\nimport json\nimport numpy as np\nimport sys\nimport traceback\n\n# Import ARC modules\nfrom src.arc.grid_operations import Grid\nfrom src.arc.pattern_detector import PatternDetector\nfrom src.arc.task_loader import TaskLoader\nfrom src.solvers.pattern_solver import PatternSolver\nfrom src.solvers.csp_solver import CSPSolver\nfrom src.arc_agent import ARCAgent\n\n\ndef test_grid_operations():\n    \"\"\"Test basic grid operations\"\"\"\n    print(\"\\n=== Testing Grid Operations ===\")\n    \n    # Create a simple grid\n    data = np.array([\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1]\n    ])\n    \n    grid = Grid(data)\n    print(f\"Created grid with shape: {grid.shape}\")\n    print(f\"Unique colors: {grid.unique_colors}\")\n    \n    # Test rotation\n    rotated = grid.rotate(90)\n    print(f\"Rotated grid shape: {rotated.shape}\")\n    \n    # Test get_connected_components (this was previously broken)\n    components = grid.get_connected_components()\n    print(f\"Found {len(components)} connected components\")\n    \n    return True\n\n\ndef test_pattern_detector():\n    \"\"\"Test pattern detector functionality\"\"\"\n    print(\"\\n=== Testing Pattern Detector ===\")\n    \n    # Create a symmetric grid\n    data = np.array([\n        [1, 2, 1],\n        [3, 4, 3],\n        [1, 2, 1]\n    ])\n    \n    grid = Grid(data)\n    detector = PatternDetector(grid)\n    \n    # Test symmetry detection\n    symmetries = detector.get_symmetries()\n    print(f\"Symmetries detected: {symmetries}\")\n    \n    # Test get_connected_components (this was the renamed method)\n    components = detector.get_connected_components()\n    print(f\"Found {len(components)} components in pattern detector\")\n    \n    # Test with background inclusion\n    components_with_bg = detector.get_connected_components(include_background=True)\n    print(f\"Found {len(components_with_bg)} components with background\")\n    \n    return True\n\n\ndef test_simple_arc_task():\n    \"\"\"Test solving a simple ARC task\"\"\"\n    print(\"\\n=== Testing Simple ARC Task ===\")\n    \n    # Create a simple tiling task\n    train_inputs = [\n        np.array([[1, 2], [3, 4]]),\n        np.array([[5, 6], [7, 8]])\n    ]\n    \n    train_outputs = [\n        np.array([[1, 2, 1, 2], [3, 4, 3, 4], [1, 2, 1, 2], [3, 4, 3, 4]]),\n        np.array([[5, 6, 5, 6], [7, 8, 7, 8], [5, 6, 5, 6], [7, 8, 7, 8]])\n    ]\n    \n    test_input = np.array([[2, 3], [4, 5]])\n    \n    # Test with pattern solver\n    solver = PatternSolver()\n    print(\"Testing PatternSolver...\")\n    \n    try:\n        result = solver.solve(train_inputs, train_outputs, test_input)\n        if result is not None:\n            print(f\"Pattern solver produced output with shape: {result.shape}\")\n        else:\n            print(\"Pattern solver could not find a solution\")\n    except Exception as e:\n        print(f\"Pattern solver error: {e}\")\n        traceback.print_exc()\n    \n    # Test with CSP solver\n    print(\"\\nTesting CSPSolver...\")\n    csp_solver = CSPSolver()\n    \n    try:\n        result = csp_solver.solve(train_inputs, train_outputs, test_input)\n        if result is not None:\n            print(f\"CSP solver produced output with shape: {result.shape}\")\n        else:\n            print(\"CSP solver could not find a solution\")\n    except Exception as e:\n        print(f\"CSP solver error: {e}\")\n        traceback.print_exc()\n    \n    return True\n\n\ndef test_arc_agent():\n    \"\"\"Test the main ARC agent\"\"\"\n    print(\"\\n=== Testing ARC Agent ===\")\n    \n    # Create a simple color mapping task\n    from src.arc.task_loader import ARCTask, ARCExample\n    \n    train_examples = [\n        ARCExample(\n            input=np.array([[1, 0], [0, 1]]),\n            output=np.array([[3, 0], [0, 3]])\n        ),\n        ARCExample(\n            input=np.array([[2, 0], [0, 2]]),\n            output=np.array([[4, 0], [0, 4]])\n        )\n    ]\n    \n    test_examples = [\n        ARCExample(\n            input=np.array([[5, 0], [0, 5]]),\n            output=None  # We don't know the output yet\n        )\n    ]\n    \n    # Create ARCTask object\n    task = ARCTask(\n        task_id=\"test_simple\",\n        train_examples=train_examples,\n        test_examples=test_examples\n    )\n    \n    # Create agent with short timeout for testing\n    agent_config = {\n        'timeout': 5.0,\n        'max_attempts': 3\n    }\n    \n    agent = ARCAgent(config=agent_config)\n    \n    try:\n        result = agent.solve_task(task)\n        if result and result.test_solutions:\n            print(f\"Agent produced {len(result.test_solutions)} solutions\")\n            if result.test_solutions[0] is not None:\n                print(f\"First solution shape: {result.test_solutions[0].shape}\")\n                print(f\"Output:\\n{result.test_solutions[0]}\")\n                print(f\"Confidence: {result.confidence}\")\n                print(f\"Strategy used: {result.strategy_used}\")\n        else:\n            print(\"Agent could not find a solution\")\n    except Exception as e:\n        print(f\"Agent error: {e}\")\n        traceback.print_exc()\n    \n    return True\n\n\ndef main():\n    \"\"\"Run all tests\"\"\"\n    print(\"=\" * 50)\n    print(\"ARC Solver System Test Suite\")\n    print(\"=\" * 50)\n    \n    all_passed = True\n    \n    # Run tests\n    tests = [\n        (\"Grid Operations\", test_grid_operations),\n        (\"Pattern Detector\", test_pattern_detector),\n        (\"Simple ARC Task\", test_simple_arc_task),\n        (\"ARC Agent\", test_arc_agent)\n    ]\n    \n    for test_name, test_func in tests:\n        try:\n            result = test_func()\n            if result:\n                print(f\"âœ… {test_name} - PASSED\")\n            else:\n                print(f\"âŒ {test_name} - FAILED\")\n                all_passed = False\n        except Exception as e:\n            print(f\"âŒ {test_name} - ERROR: {e}\")\n            traceback.print_exc()\n            all_passed = False\n    \n    print(\"\\n\" + \"=\" * 50)\n    if all_passed:\n        print(\"âœ… All tests passed! The ARC solver system is functional.\")\n    else:\n        print(\"âŒ Some tests failed. Please check the errors above.\")\n    print(\"=\" * 50)\n    \n    return 0 if all_passed else 1\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())","size_bytes":6238}},"version":1}